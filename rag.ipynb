{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install groq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain faiss-cpu transformers pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv -q\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ;oad environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# set environment variables\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_API_KEY\"] = os.getenv(\"HF_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"revised.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Students at Cambridge University, who come from less af\u0003uent backgrounds, are being offered up to\n",
      "1,000 a year under a bursary scheme.\n",
      "This sentence contains a non-restrictive relative clause: who come from less af\u0003uent backgrounds. This is a form\n",
      "of parenthetical comment. The sentence implies that most/all students at Cambridge come from less af\u0003uent back-\n",
      "grounds. What the reporter probably meant was a restrictive relative, which should not have commas round it:\n",
      "Students at Cambridge University who come from less af\u0003uent backgrounds are being offered up to 1,000\n",
      "a year under a bursary scheme.\n",
      "A restrictive relative is a type of modi\u0002er: that is, it further speci\u0002es the entities under discussion. Thus it refers to a\n",
      "subset of the students.\n",
      "1.6 Information retrieval, information extraction and question answering\n",
      "Information retrieval involves returning a set of documents in response to a user query: Internet search engines are a\n",
      "form of IR. However, one change from classical IR is that Internet search now uses techniques that rank documents\n",
      "according to how many links there are to them (e.g., Google's PageRank) as well as the presence of search terms.\n",
      "Information extraction involves trying to discover speci\u0002c information from a set of documents. The information\n",
      "required can be described as a template. For instance, for company joint ventures, the template might have slots for\n",
      "the companies, the dates, the products, the amount of money involved. The slot \u0002llers are generally strings.\n",
      "Question answering attempts to \u0002nd a speci\u0002c answer to a speci\u0002c question from a set of documents, or at least a short\n",
      "piece of text that contains the answer.\n",
      "(22) What is the capital of France?\n",
      "Paris has been the French capital for many centuries.\n",
      "There are some question-answering systems on the Web, but most use very basic techniques. For instance, Ask Jeeves\n",
      "relies on a fairly large staff of people who search the web to \u0002nd pages which are answers to potential questions. The\n",
      "system performs very limited manipulation on the input to map to a known question. The same basic technique is used\n",
      "in many online help systems.\n",
      "1.7 Machine translation\n",
      "MT work started in the US in the early \u0002fties, concentrating on Russian to English. A prototype system was publicly\n",
      "demonstrated in 1954 (remember that the \u0002rst electronic computer had only been built a few years before that). MT\n",
      "funding got drastically cut in the US in the mid-60s and ceased to be academically respectable in some places, but\n",
      "Systran was providing useful translations by the late 60s. Systran is still going (updating it over the years is an amazing\n",
      "feat of software engineering): Systran now powers AltaVista's BabelFish\n",
      "http://world.altavista.com/\n",
      "andmany other translation services on the web.\n",
      "Until the 80s, the utility of general purpose MT systems was severely limited by the fact that text was not available in\n",
      "electronic form: Systran used teams of skilled typists to input Russian documents.\n",
      "Systran and similar systems are not a substitute for human translation: they are useful because they allow people to\n",
      "get an idea of what a document is about, and maybe decide whether it is interesting enough to get translated properly.\n",
      "This is much more relevant now that documents etc are available on the Web. Bad translation is also, apparently, good\n",
      "enough for chatrooms.\n",
      "Spoken language translation is viable for limited domains: research systems include Verbmobil, SLT and CSTAR.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(documents[7].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the document into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'revised.pdf', 'page': 0}, page_content='Natural Language Processing\\n2004, 8 Lectures\\nAnn Copestake (aac@cl.cam.ac.uk)\\nhttp://www.cl.cam.ac.uk/users/aac/\\nCopyright c\\rAnn Copestake, 2003\\x962004\\nLecture Synopsis\\nAims\\nThiscourse aims to introduce the fundamental techniques of natural language processing and to develop an under-\\nstanding of the limits of those techniques. It aims to introduce some current research issues, and to evaluate some\\ncurrent and potential applications.\\n\\x0f Introduction. Brief history of NLP research, current applications, generic NLP system architecture, knowledge-\\nbased versus probabilistic approaches.\\n\\x0f Finite-state techniques. In\\x03ectional and derivational morphology, \\x02nite-state automata in NLP, \\x02nite-state\\ntransducers.\\n\\x0f Prediction and part-of-speech tagging. Corpora, simple N-grams, word prediction, stochastic tagging, evalu-\\nating system performance.\\n\\x0f Parsing and generation. Generative grammar, context-free grammars, parsing and generation with context-free\\ngrammars, weights and probabilities.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 0}, page_content='ating system performance.\\n\\x0f Parsing and generation. Generative grammar, context-free grammars, parsing and generation with context-free\\ngrammars, weights and probabilities.\\n\\x0f Parsing with constraint-based grammars. Constraint-based grammar, uni\\x02cation.\\n\\x0f Compositional and lexical semantics. Simple compositional semantics in constraint-based grammar. Semantic\\nrelations, WordNet, word senses, word sense disambiguation.\\n\\x0f Discourse and dialogue. Anaphora resolution, discourse relations.\\n\\x0f Applications. Machine translation, email response, spoken dialogue systems.\\nObjectives\\nAt the end of the course students should\\n\\x0f be able to describe the architecture of and basic design for a generic NLP system \\x93shell\\x94\\n\\x0f be able to discuss the current and likely future performance of several NLP applications, such as machine\\ntranslation and email response\\n\\x0f be able to describe brie\\x03y a fundamental technique for processing language for several subtasks, such as mor-'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 0}, page_content='translation and email response\\n\\x0f be able to describe brie\\x03y a fundamental technique for processing language for several subtasks, such as mor-\\nphological analysis, parsing, word sense disambiguation etc.\\n\\x0f understand how these techniques draw on and relate to other areas of (theoretical) computer science, such as\\nformal language theory, formal semantics of programming languages, or theorem proving\\n1'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 1}, page_content=\"Overview\\nNLP is a large and multidisciplinary \\x02eld, so this course can only provide a very general introduction. The \\x02rst\\nlecture is designed to give an overview of the main subareas and a very brief idea of the main applications and\\nthe methodologies which have been employed. The history of NLP is brie\\x03y discussed as a way of putting this into\\nperspective. The next six lectures describe some of the main subareas in more detail. The organisation is roughly based\\non increased `depth' of processing, starting with relatively surface-oriented techniques and progressing to considering\\nmeaning of sentences and meaning of utterances in context. Most lectures will start off by considering the subarea as\\na whole and then go on to describe one or more sample algorithms which tackle particular problems. The algorithms\\nhave been chosen because they are relatively straightforward to describe and because they illustrate a speci\\x02c technique\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 1}, page_content='have been chosen because they are relatively straightforward to describe and because they illustrate a speci\\x02c technique\\nwhich has been shown to be useful, but the idea is to exemplify an approach, not to give a detailed survey (which would\\nbe impossible in the time available). (Lecture 5 is a bit different in that it concentrates on a data structure instead of\\nan algorithm.) The \\x02nal lecture brings the preceding material together in order to describe the state of the art in three\\nsample applications.\\nThere are various themes running throughout the lectures. One theme is the connection to linguistics and the tension\\nthat sometimes exists between the predominant view in theoretical linguistics and the approaches adopted within NLP.\\nA somewhat related theme is the distinction between knowledge-based and probabilistic approaches. Evaluation will\\nbe discussed in the context of the different algorithms.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 1}, page_content=\"A somewhat related theme is the distinction between knowledge-based and probabilistic approaches. Evaluation will\\nbe discussed in the context of the different algorithms.\\nBecause NLP is such a large area, there are many topics that aren't touched on at all in these lectures. Speech\\nrecognition and speech synthesis is almost totally ignored. Information retrieval and information extraction are the\\ntopic of a separate course given by Simone Teufel, for which this course is a prerequisite.\\nFeedback on the handout, lists of typos etc, would be greatly appreciated.\\nRecommended Reading\\nRecommended Book:\\nJurafsky, Daniel and James Martin, Speech and Language Processing, Prentice-Hall, 2000 (referenced as J&M through-\\nout this handout).\\nBackground:\\nThesebooks are about linguistics rather that NLP/computational linguistics. They are not necessary to understand the\\ncourse, but should give readers an idea about some of the properties of human languages that make NLP interesting\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 1}, page_content=\"course, but should give readers an idea about some of the properties of human languages that make NLP interesting\\nand challenging, without being technical.\\nPinker, S., The Language Instinct, Penguin, 1994.\\nThis is a thought-provoking and sometimes controversial `popular' introduction to linguistics.\\nMatthews, Peter, Linguistics: a very short introduction, OUP, 2003.\\nThe title is accurate . . .\\nBackground/reference:\\nTheInternet Grammar of English, http://www.ucl.ac.uk/internet-grammar/home.htm\\nSyntactic concepts and terminology.\\nStudy and Supervision Guide\\nThe handouts and lectures should contain enough information to enable students to adequately answer the exam\\nquestions, but the handout is not intended to substitute for a textbook. In most cases, J&M go into a considerable\\namount of further detail: rather than put lots of suggestions for further reading in the handout, in general I have\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 1}, page_content='amount of further detail: rather than put lots of suggestions for further reading in the handout, in general I have\\nassumed that students will look at J&M, and then follow up the references in there if they are interested. The notes at\\nthe end of each lecture give details of the sections of J&M that are relevant and details of any discrepancies with these\\nnotes.\\nSupervisorsought to familiarise themselves with the relevant parts of Jurafsky and Martin (see notes at the end of each\\nlecture). However, good students should \\x02nd it quite easy to come up with questions that the supervisors (and the\\n2'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 2}, page_content=\"lecturer) can't answer! Language is like that . . .\\nGenerally I'm taking a rather informal/example-based approach to concepts such as \\x02nite-state automata, context-free\\ngrammars etc. Part II students should have already got the formal background that enables them to understand the\\napplication to NLP. Diploma and Part II (General) students may not have covered all these concepts before, but the\\nexpectation is that the examples are straightforward enough so that this won't matter too much.\\nThis course inevitably assumes some very basic linguistic knowledge, such as the distinction between the major parts\\nof speech. It introduces some linguistic concepts that won't be familiar to all students: since I'll have to go through\\nthese quickly, reading the \\x02rst few chapters of an introductory linguistics textbook may help students understand the\\nmaterial. The idea is to introduce just enough linguistics to motivate the approaches used within NLP rather than\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 2}, page_content=\"material. The idea is to introduce just enough linguistics to motivate the approaches used within NLP rather than\\nto teach the linguistics for its own sake. At the end of this handout, there are some mini-exercises to help students\\nunderstand the concepts: it would be very useful if these were attempted before the lectures as indicated. There are\\nalso some suggested post-lecture exercises.\\nExam questions won't rely on students remembering the details of any speci\\x02c linguistic phenomenon. As far as\\npossible, exam questions will be suitable for people who speak English as a second language. For instance, if a\\nquestion relied on knowledge of the ambiguity of a particular English word, a gloss of the relevant senses would be\\ngiven.\\nOf course, I'll be happy to try and answer questions about the course or more general NLP questions, preferably by\\nemail.\\n3\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 3}, page_content=\"1 Lecture 1: Introduction to NLP\\nThe aim of this lecture is to give students some idea of the objectives of NLP. The main subareas of NLP will be\\nintroduced, especially those which will be discussed in more detail in the rest of the course. There will be a preliminary\\ndiscussion of the main problems involved in language processing by means of examples taken from NLP applications.\\nThis lecture also introduces some methodological distinctions and puts the applications and methodology into some\\nhistorical context.\\n1.1 What is NLP?\\nNatural language processing (NLP) can be de\\x02ned as the automatic (or semi-automatic) processing of human language.\\nThe term `NLP' is sometimes used rather more narrowly than that, often excluding information retrieval and sometimes\\neven excluding machine translation. NLP is sometimes contrasted with `computational linguistics', with NLP being\\nthought of as more applied. Nowadays, alternative terms are often preferred, like `Language Technology' or `Language\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 3}, page_content=\"thought of as more applied. Nowadays, alternative terms are often preferred, like `Language Technology' or `Language\\nEngineering'. Language is often used in contrast with speech (e.g., Speech and Language Technology). But I'm going\\nto simply refer to NLP and use the term broadly.\\nNLP is essentially multidisciplinary: it is closely related to linguistics (although the extent to which NLP overtly draws\\non linguistic theory varies considerably). It also has links to research in cognitive science, psychology, philosophy and\\nmaths (especially logic). Within CS, it relates to formal language theory, compiler techniques, theorem proving, ma-\\nchine learning and human-computer interaction. Of course it is also related to AI, though nowadays it's not generally\\nthought of as part of AI.\\n1.2 Some linguistic terminology\\nThe course is organised so that there are six lectures corresponding to different NLP subareas, moving from relatively\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 3}, page_content=\"thought of as part of AI.\\n1.2 Some linguistic terminology\\nThe course is organised so that there are six lectures corresponding to different NLP subareas, moving from relatively\\n`shallow' processing to areas which involve meaning and connections with the real world. These subareas loosely\\ncorrespond to some of the standard subdivisions of linguistics:\\n1. Morphology: the structure of words. For instance, unusually can be thought of as composed of a pre\\x02x un-, a\\nstem usual, and an af\\x02x -ly. composed is compose plus the in\\x03ectional af\\x02x -ed: a spelling rule means we end\\nup with composed rather than composeed. Morphology will be discussed in lecture 2.\\n2. Syntax: the way words are used to form phrases. e.g., it is part of English syntax that a determiner such as\\nthe will come before a noun, and also that determiners are obligatory with certain singular nouns. Formal and\\ncomputational aspects of syntax will be discussed in lectures 3, 4 and 5.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 3}, page_content='the will come before a noun, and also that determiners are obligatory with certain singular nouns. Formal and\\ncomputational aspects of syntax will be discussed in lectures 3, 4 and 5.\\n3. Semantics. Compositional semantics is the construction of meaning (generally expressed as logic) based on\\nsyntax. This is contrasted to lexical semantics, i.e., the meaning of individual words. Compositional and lexical\\nsemantics is discussed in lecture 6.\\n4. Pragmatics: meaning in context. This will come into lecture 7, although linguistics and NLP generally have\\nvery different perspectives here.\\n1.3 Why is language processing dif\\x02cult?\\nConsider trying to build a system that would answer email sent by customers to a retailer selling laptops and accessories\\nvia the Internet. This might be expected to handle queries such as the following:\\n\\x0f Has my order number 4291 been shipped yet?\\n\\x0f Is FD5 compatible with a 505G?\\n\\x0f What is the speed of the 505G?\\n4'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 4}, page_content='Assume the query is to be evaluated against a database containing product and order information, with relations such\\nas the following:\\nORDER\\nOrdernumber Date ordered Date shipped\\n4290 2/2/02 2/2/02\\n4291 2/2/02 2/2/02\\n4292 2/2/02\\nUSER: Has my order number 4291 been shipped yet?\\nDB QUERY: order(number=4291,date\\nshipped=?)\\nRESPONSE TO USER: Order number 4291 was shipped on 2/2/02\\nIt might look quite easy to write patterns for these queries, but very similar strings can mean very different things,\\nwhile very different strings can mean much the same thing. 1 and 2 below look very similar but mean something\\ncompletely different, while 2 and 3 look very different but mean much the same thing.\\n1. How fast is the 505G?\\n2. How fast will my 505G arrive?\\n3. Please tell me when I can expect the 505G I ordered.\\nWhile some tasks in NLP can be done adequately without having any sort of account of meaning, others require that'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 4}, page_content=\"3. Please tell me when I can expect the 505G I ordered.\\nWhile some tasks in NLP can be done adequately without having any sort of account of meaning, others require that\\nwe can construct detailed representations which will re\\x03ect the underlying meaning rather than the super\\x02cial string.\\nIn fact, in natural languages (as opposed to programming languages), ambiguity is ubiquitous, so exactly the same\\nstring might mean different things. For instance in the query:\\nDo you sell Sony laptops and disk drives?\\nthe user may or may not be asking about Sony disk drives. This particular ambiguity may be represented by different\\nbracketings:\\nDo you sell (Sony laptops) and (disk drives)?\\nDo you sell (Sony (laptops and disk drives))?\\nWe'll see lots of examples of different types of ambiguity in these lectures.\\nOften humans have knowledge of the world which resolves a possible ambiguity, probably without the speaker or\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 4}, page_content=\"We'll see lots of examples of different types of ambiguity in these lectures.\\nOften humans have knowledge of the world which resolves a possible ambiguity, probably without the speaker or\\nhearer even being aware that there is a potential ambiguity.1 But hand-coding such knowledge in NLP applications\\nhas turned out to be impossibly hard to do for more than very limited domains: the term AI-complete is sometimes\\nused (by analogy to NP-complete), meaning that we'd have to solve the entire problem of representing the world\\nand acquiring world knowledge.2 The term AI-complete is intended jokingly, but conveys what's probably the most\\nimportant guiding principle in current NLP: we're looking for applications which don't require AI-complete solutions:\\ni.e., ones where we can work with very limited domains or approximate full world knowledge by relatively simple\\ntechniques.\\n1.4Some NLP applications\\nThe following list is not complete, but useful systems have been built for:\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 4}, page_content=\"techniques.\\n1.4Some NLP applications\\nThe following list is not complete, but useful systems have been built for:\\n1I'll use hearer generally to mean the person who is on the receiving end, regardless of the modality of the language transmission: i.e., regardless\\nof whether it's spoken, signed or written. Similarly, I'll use speaker for the person generating the speech, text etc and utterance to mean the speech\\nor text itself. This is the standard linguistic terminology, which recognises that spoken language is primary and text is a later development.\\n2In this course, I will use domain to mean some circumscribed body of knowledge: for instance, information about laptop orders constitutes a\\nlimited domain.\\n5\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 5}, page_content=\"\\x0f spelling and grammar checking\\n\\x0f optical character recognition (OCR)\\n\\x0f screen readers for blind and partially sighted users\\n\\x0f augmentative and alternative communication (i.e., systems to aid people who have dif\\x02culty communicating\\nbecause of disability)\\n\\x0f machine aided translation (i.e., systems which help a human translator, e.g., by storing translations of phrases\\nand providing online dictionaries integrated with word processors, etc)\\n\\x0f lexicographers' tools\\n\\x0f information retrieval\\n\\x0f document classi\\x02cation (\\x02ltering, routing)\\n\\x0f document clustering\\n\\x0f information extraction\\n\\x0f question answering\\n\\x0f summarization\\n\\x0f text segmentation\\n\\x0f exam marking\\n\\x0f report generation (possibly multilingual)\\n\\x0f machine translation\\n\\x0f natural language interfaces to databases\\n\\x0f email understanding\\n\\x0f dialogue systems\\nSeveral of these applications are discussed brie\\x03y below. Roughly speaking, they are ordered according to the com-\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 5}, page_content=\"\\x0f natural language interfaces to databases\\n\\x0f email understanding\\n\\x0f dialogue systems\\nSeveral of these applications are discussed brie\\x03y below. Roughly speaking, they are ordered according to the com-\\nplexity of the language technology required. The applications towards the top of the list can be seen simply as aids to\\nhuman users, while those at the bottom are perceived as agents in their own right. Perfect performance on any of these\\napplications would be AI-complete, but perfection isn't necessary for utility: in many cases, useful versions of these\\napplications had been built by the late 70s. Commercial success has often been harder to achieve, however.\\n1.5 Spelling and grammar checking\\nAll spelling checkers can \\x03ag words which aren't in a dictionary.\\n(1) * The neccesary steps are obvious.\\n(2) The necessary steps are obvious.\\nIf the user can expand the dictionary, or if the language has complex productive morphology (see x2.1), then a simple\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 5}, page_content=\"(1) * The neccesary steps are obvious.\\n(2) The necessary steps are obvious.\\nIf the user can expand the dictionary, or if the language has complex productive morphology (see x2.1), then a simple\\nlist of words isn't enough to do this and some morphological processing is needed.3\\nMore subtle cases involve words which are correct in isolation, but not in context. Syntax could sort some of these\\ncases out. For instance, possessive its generally has to be immediately followed by a noun or by one or more adjectives\\nwhich are immediately in front of a noun:\\n3Note the use of * (`star') above: this notation is used in linguistics to indicate a sentence which is judged (by the author, at least) to be incorrect.\\n? is generally used for a sentence which is questionable, or at least doesn't have the intended interpretation. # is used for a pragmatically anomalous\\nsentence.\\n6\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 6}, page_content=\"(3) * Its a fair exchange.\\n(4) It's a fair exchange.\\n(5) * The dog came into the room, it's tail wagging.\\n(6) The dog came into the room, its tail wagging.\\nBut, it sometimes isn't locally clear what the context is: e.g. fair is ambiguous between a noun and an adjective.\\n(7) * `Its fair', was all Kim said.\\n(8) `It's fair', was all Kim said.\\n(9) * Every village has an annual fair, except Kimbolton: it's fair is held twice a year.\\n(10) Every village has an annual fair, except Kimbolton: its fair is held twice a year.\\nThe most elaborate spelling/grammar checkers can get some of these cases right, but none are anywhere near perfect.\\nSpelling correction can require a form of word sense disambiguation:\\n(11) # The tree's bows were heavy with snow.\\n(12) The tree's boughs were heavy with snow.\\nGetting this right requires an association between tree and bough. In the past, attempts might have been made to\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 6}, page_content=\"(12) The tree's boughs were heavy with snow.\\nGetting this right requires an association between tree and bough. In the past, attempts might have been made to\\nhand-code this in terms of general knowledge of trees and their parts. However this sort of hand-coding is not suitable\\nfor applications that work on unbounded domains. These days machine learning techniques are generally used to\\nderive word associations from corpora:4 this can be seen as a substitute for the fully detailed world knowledge, but\\nmay actually be a more realistic model of how humans do word sense disambiguation. However, commercial systems\\ndon't (yet) do this systematically.\\nSimple subject verb agreement can be checked automatically:5\\n(13) My friends like pizza.\\n(14) My friend likes pizza.\\n(15) * My friends likes pizza.\\n(16) * My friend like pizza.\\n(17) My friends were unhappy.\\n(18) * My friend were unhappy.\\nBut this isn't as straightforward as it may seem:\\n(19) A number of my friends were unhappy.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 6}, page_content=\"(16) * My friend like pizza.\\n(17) My friends were unhappy.\\n(18) * My friend were unhappy.\\nBut this isn't as straightforward as it may seem:\\n(19) A number of my friends were unhappy.\\n(20) The number of my friends who were unhappy was amazing.\\n(21) My family were unhappy.\\nWhether the last example is grammatical or not depends on your dialect of English: it is grammatical for most British\\nEnglish speakers, but not for many Americans.\\nChecking punctuation can be hard (even AI-complete):\\nBBC News Online, 3 October, 2001\\n4A corpus is a body of text that has been collected for some purpose, see x3.1.\\n5In English, the subject of a sentence is generally a noun phrase which comes before the verb, in contrast to the object, which follows the verb.\\n7\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 7}, page_content='Students at Cambridge University, who come from less af\\x03uent backgrounds, are being offered up to\\n1,000 a year under a bursary scheme.\\nThis sentence contains a non-restrictive relative clause: who come from less af\\x03uent backgrounds. This is a form\\nof parenthetical comment. The sentence implies that most/all students at Cambridge come from less af\\x03uent back-\\ngrounds. What the reporter probably meant was a restrictive relative, which should not have commas round it:\\nStudents at Cambridge University who come from less af\\x03uent backgrounds are being offered up to 1,000\\na year under a bursary scheme.\\nA restrictive relative is a type of modi\\x02er: that is, it further speci\\x02es the entities under discussion. Thus it refers to a\\nsubset of the students.\\n1.6 Information retrieval, information extraction and question answering\\nInformation retrieval involves returning a set of documents in response to a user query: Internet search engines are a'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 7}, page_content=\"1.6 Information retrieval, information extraction and question answering\\nInformation retrieval involves returning a set of documents in response to a user query: Internet search engines are a\\nform of IR. However, one change from classical IR is that Internet search now uses techniques that rank documents\\naccording to how many links there are to them (e.g., Google's PageRank) as well as the presence of search terms.\\nInformation extraction involves trying to discover speci\\x02c information from a set of documents. The information\\nrequired can be described as a template. For instance, for company joint ventures, the template might have slots for\\nthe companies, the dates, the products, the amount of money involved. The slot \\x02llers are generally strings.\\nQuestion answering attempts to \\x02nd a speci\\x02c answer to a speci\\x02c question from a set of documents, or at least a short\\npiece of text that contains the answer.\\n(22) What is the capital of France?\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 7}, page_content='Question answering attempts to \\x02nd a speci\\x02c answer to a speci\\x02c question from a set of documents, or at least a short\\npiece of text that contains the answer.\\n(22) What is the capital of France?\\nParis has been the French capital for many centuries.\\nThere are some question-answering systems on the Web, but most use very basic techniques. For instance, Ask Jeeves\\nrelies on a fairly large staff of people who search the web to \\x02nd pages which are answers to potential questions. The\\nsystem performs very limited manipulation on the input to map to a known question. The same basic technique is used\\nin many online help systems.\\n1.7 Machine translation\\nMT work started in the US in the early \\x02fties, concentrating on Russian to English. A prototype system was publicly\\ndemonstrated in 1954 (remember that the \\x02rst electronic computer had only been built a few years before that). MT\\nfunding got drastically cut in the US in the mid-60s and ceased to be academically respectable in some places, but'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 7}, page_content=\"funding got drastically cut in the US in the mid-60s and ceased to be academically respectable in some places, but\\nSystran was providing useful translations by the late 60s. Systran is still going (updating it over the years is an amazing\\nfeat of software engineering): Systran now powers AltaVista's BabelFish\\nhttp://world.altavista.com/\\nandmany other translation services on the web.\\nUntil the 80s, the utility of general purpose MT systems was severely limited by the fact that text was not available in\\nelectronic form: Systran used teams of skilled typists to input Russian documents.\\nSystran and similar systems are not a substitute for human translation: they are useful because they allow people to\\nget an idea of what a document is about, and maybe decide whether it is interesting enough to get translated properly.\\nThis is much more relevant now that documents etc are available on the Web. Bad translation is also, apparently, good\\nenough for chatrooms.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 7}, page_content='This is much more relevant now that documents etc are available on the Web. Bad translation is also, apparently, good\\nenough for chatrooms.\\nSpoken language translation is viable for limited domains: research systems include Verbmobil, SLT and CSTAR.\\n8'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content=\"1.8 Natural language interfaces and dialogue systems\\nNatural language interfaces were the `classic' NLP problem in the 70s and 80s. LUNAR is the classic example of\\na natural language interface to a database (NLID): its database concerned lunar rock samples brought back from the\\nApollo missions. LUNAR is described by Woods (1978) (but note most of the work was done several years earlier): it\\nwas capable of translating elaborate natural language expressions into database queries.\\nSHRDLU (Winograd, 1973) was a system capable of participating in a dialogue about a microworld (the blocks world)\\nand manipulating this world according to commands issued in English by the user. SHRDLU had a big impact on the\\nperception of NLP at the time since it seemed to show that computers could actually `understand' language: the\\nimpossibility of scaling up from the microworld was not realised.\\nLUNAR and SHRDLU both exploited the limitations of one particular domain to make the natural language under-\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content=\"impossibility of scaling up from the microworld was not realised.\\nLUNAR and SHRDLU both exploited the limitations of one particular domain to make the natural language under-\\nstanding problem tractable, particularly with respect to ambiguity. To take a trivial example, if you know your database\\nis about lunar rock, you don't need to consider the music or movement senses of rock when you're analysing a query.\\nThere have been many advances in NLP since these systems were built: systems have become much easier to build,\\nand somewhat easier to use, but they still haven't become ubiquitous. Natural Language interfaces to databases were\\ncommercially available in the late 1970s, but largely died out by the 1990s: porting to new databases and especially to\\nnew domains requires very specialist skills and is essentially too expensive (automatic porting was attempted but never\\nsuccessfully developed). Users generally preferred graphical interfaces when these became available. Speech input\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content=\"successfully developed). Users generally preferred graphical interfaces when these became available. Speech input\\nwould make natural language interfaces much more useful: unfortunately, speaker-independent speech recognition\\nstill isn't good enough for even 1970s scale NLP to work well. Techniques for dealing with misrecognised data\\nhave proved hard to develop. In many ways, current commercially-deployed spoken dialogue systems are using pre-\\nSHRDLU technology.\\n1.9 Some more history\\nBefore the 1970s, most NLP researchers were concentrating on MT as an application (see above). NLP was a very\\nearly application of CS and started about the same time as Chomsky was publishing his \\x02rst major works in formal\\nlinguistics (Chomskyan linguistics quickly became dominant, especially in the US). In the 1950s and early 1960s,\\nideas about formal grammar were being worked out in linguistics and algorithms for parsing natural language were\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content=\"ideas about formal grammar were being worked out in linguistics and algorithms for parsing natural language were\\nbeing developed at the same time as algorithms for parsing programming languages. However, most linguists were\\nuninterested in NLP and the approach that Chomsky developed turned out to be only somewhat indirectly useful for\\nNLP.\\nNLP in the 1970s and \\x02rst half of the 1980s was predominantly based on a paradigm where extensive linguistic and\\nreal-world knowledge was hand-coded. There was controversy about how much linguistic knowledge was necessary\\nfor processing, with some researchers downplaying syntax, in particular, in favour of world knowledge. NLP re-\\nsearchers were very much part of the AI community (especially in the US and the UK), and the debate that went on in\\nAI about the use of logic vs other meaning representations (`neat' vs `scruffy') also affected NLP. By the 1980s, several\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content=\"AI about the use of logic vs other meaning representations (`neat' vs `scruffy') also affected NLP. By the 1980s, several\\nlinguistic formalisms had appeared which were fully formally grounded and reasonably computationally tractable, and\\nthe linguistic/logical paradigm in NLP was \\x02rmly established. Unfortunately, this didn't lead to many useful systems,\\npartly because many of the dif\\x02cult problems (disambiguation etc) were seen as somebody else's job (and mainstream\\nAI was not developing adequate knowledge representation techniques) and partly because most researchers were con-\\ncentrating on the `agent-like' applications and neglecting the user aids. Although the symbolic, linguistically-based\\nsystems sometimes worked quite well as NLIDs, they proved to be of little use when it came to processing less re-\\nstricted text, for applications such as IE. It also became apparent that lexical acquisition was a serious bottleneck for\\nserious development of such systems.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content='stricted text, for applications such as IE. It also became apparent that lexical acquisition was a serious bottleneck for\\nserious development of such systems.\\nStatistical NLP became the most common paradigm in the 1990s, at least in the research community. Speech recog-\\nnition had demonstrated that simple statistical techniques worked, given enough training data. NLP systems were\\nbuilt which required very limited hand-coded knowledge, apart from initial training material. Most applications were\\nmuch shallower than the earlier NLIDs, but the switch to statistical NLP coincided with a change in US funding,\\nwhich started to emphasise speech-based interfaces and IE. There was also a general realization of the importance\\nof serious evaluation and of reporting results in a way that could be reproduced by other researchers. US funding\\nemphasised competitions with speci\\x02c tasks and supplied test material, which encouraged this, although there was a'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 8}, page_content='emphasised competitions with speci\\x02c tasks and supplied test material, which encouraged this, although there was a\\ndownside in that some of the techniques developed were very task-speci\\x02c. It should be emphasised that there had\\n9'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 9}, page_content='been computational work on corpora for many years (much of it by linguists): it became much easier to do corpus\\nwork by the late 1980s as disk space became cheap and machine-readable text became ubiquitous. Despite the shift\\nin research emphasis to statistical approaches, most commercial systems remained primarily based on hand-coded\\nlinguistic information.\\nMore recently the symbolic/statistical split has become less pronounced, since most researchers are interested in both.6\\nThere is considerable emphasis on machine learning in general, including machine learning for symbolic processing.\\nLinguistically-based NLP has made something of a comeback, with increasing availability of open source resources,\\nand the realisation that at least some of the classic statistical techniques seem to be reaching limits on performance,\\nespecially because of dif\\x02culties in adapting to new types of text. However, the modern linguistically-based approaches'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 9}, page_content=\"especially because of dif\\x02culties in adapting to new types of text. However, the modern linguistically-based approaches\\nare making use of machine learning and statistical processing. The dotcom boom and bust has considerably affected\\nNLP, but it's too early to say what the long-term implications are. The ubiquity of the Internet has certainly changed\\nthe space of interesting NLP applications, and the vast amount of text available can potentially be exploited, especially\\nfor statistical techniques.\\n1.10 Generic `deep' NLP application architecture\\nMany NLP applications can be adequately implemented with relatively shallow processing. For instance, spelling\\nchecking only requires a word list and simple morphology to be useful. I'll use the term `deep' NLP for systems that\\nbuild a meaning representation (or an elaborate syntactic representation), which is generally agreed to be required for\\napplications such as NLIDs, email question answering and good MT.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 9}, page_content=\"build a meaning representation (or an elaborate syntactic representation), which is generally agreed to be required for\\napplications such as NLIDs, email question answering and good MT.\\nThe most important principle in building a successful NLP system is modularity. NLP systems are often big software\\nengineering projects \\x97 success requires that systems can be improved incrementally.\\nThe input to an NLP system could be speech or text. It could also be gesture (multimodal input or perhaps a Sign\\nLanguage). The output might be non-linguistic, but most systems need to give some sort of feedback to the user, even\\nif they are simply performing some action (issuing a ticket, paying a bill, etc). However, often the feedback can be\\nvery formulaic.\\nThere's general agreement that the following system components can be described semi-independently, although as-\\nsumptions about the detailed nature of the interfaces between them differ. Not all systems have all of these components:\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 9}, page_content=\"sumptions about the detailed nature of the interfaces between them differ. Not all systems have all of these components:\\n\\x0f input preprocessing: speech recogniser or text preprocessor (non-trivial in languages like Chinese or for highly\\nstructured text for any language) or gesture recogniser. Such system might themselves be very complex, but I\\nwon't discuss them in this course \\x97 we'll assume that the input to the main NLP component is segmented text.\\n\\x0f morphological analysis: this is relatively well-understood for the most common languages that NLP has consid-\\nered, but is complicated for many languages (e.g., Turkish, Basque).\\n\\x0f part of speech tagging: not an essential part of most deep processing systems, but sometimes used as a way of\\ncutting down parser search space.\\n\\x0f parsing: this includes syntax and compositional semantics, which are sometimes treated as separate components\\n\\x0f disambiguation: this can be done as part of parsing, or (partially) left to a later phase\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 9}, page_content=\"\\x0f disambiguation: this can be done as part of parsing, or (partially) left to a later phase\\n\\x0f context module: this maintains information about the context, for anaphora resolution, for instance.\\n\\x0f text planning: the part of language generation that's concerned with deciding what meaning to convey (I won't\\ndiscuss this in this course)\\n\\x0f tactical generation: converts meaning representations to strings. This may use the same grammar and lexicon7\\nas the parser.\\n\\x0f morphological generation: as with morphological analysis, this is relatively straightforward for English.\\n6At least, there are only a few researchers who avoid statistical techniques as a matter of principle and all statistical systems have a symbolic\\ncomponent!\\n7The term lexicon is generally used for the part of the NLP system that contains dictionary-like information \\x97 i.e. information about individual\\nwords.\\n10\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 10}, page_content=\"\\x0f output processing: text-to-speech, text formatter, etc. As with input processing, this may be complex, but for\\nnow we'll assume that we're outputting simple text.\\nApplication speci\\x02c components, for instance:\\n1. For NL interfaces, email answering and so on, we need an interface between semantic representation (expressed\\nas some form of logic, for instance) and the underlying knowledge base.\\n2. For MT based on transfer, we need a component that maps between semantic representations.\\nIt is also very important to distinguish between the knowledge sources and the programs that use them. For instance,\\na morphological analyser has access to a lexicon and a set of morphological rules: the morphological generator might\\nshare these knowledge sources. The lexicon for the morphology system may be the same as the lexicon for the parser\\nand generator.\\nOther things might be required in order to construct the standard components and knowledge sources:\\n\\x0f lexicon acquisition\\n\\x0f grammar acquisition\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 10}, page_content=\"and generator.\\nOther things might be required in order to construct the standard components and knowledge sources:\\n\\x0f lexicon acquisition\\n\\x0f grammar acquisition\\n\\x0f acquisition of statistical information\\nFor a component to be a true module, it obviously needs a well-de\\x02ned set of interfaces. What's less obvious is that it\\nneeds its own evaluation strategy and test suites: developers need to be able to work somewhat independently.\\nIn principle, at least, components are reusable in various ways: for instance, a parser could be used with multiple\\ngrammars, the same grammar can be processed by different parsers and generators, a parser/grammar combination\\ncould be used in MT or in a natural language interface. However, for a variety of reasons, it is not easy to reuse\\ncomponents like this, and generally a lot of work is required for each new application, even if it's based on an existing\\ngrammar or the grammar is automatically acquired.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 10}, page_content=\"components like this, and generally a lot of work is required for each new application, even if it's based on an existing\\ngrammar or the grammar is automatically acquired.\\nWe can draw schematic diagrams for applications showing how the modules \\x02t together.\\n1.11 Natural language interface to a knowledge base\\nKB\\n*\\nKB INTERFACE/CONTEXT\\n6\\nPARSING\\n6\\nMORPHOLOGY\\n6\\nINPUT PROCESSING\\n6\\nuser input\\nj\\nKB OUTPUT/TEXT PLANNING\\n?\\nTACTICAL GENERATION\\n?\\nMORPHOLOGY GENERATION\\n?\\nOUTPUT PROCESSING\\n?\\noutput\\n11\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 11}, page_content=\"In such systems, the context module generally gets included as part of the KB interface because the discourse state is\\nquite simple, and contextual resolution is domain speci\\x02c. Similarly, there's often no elaborate text planning require-\\nment, though this depends very much on the KB and type of queries involved.\\nIn lectures 2\\x967, various algorithms will be discussed which could be parts of modules in this generic architecture,\\nalthough most are also useful in less elaborate contexts. Lecture 8 will discuss the architecture and requirements of a\\nfew applications in a bit more detail.\\n1.12 General comments\\n\\x0f Even `simple' NLP applications, such as spelling checkers, need complex knowledge sources for some prob-\\nlems.\\n\\x0f Applications cannot be 100% perfect, because full real world knowledge is not possible.\\n\\x0f Applications that are less than 100% perfect can be useful (humans aren't 100% perfect anyway).\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 11}, page_content=\"lems.\\n\\x0f Applications cannot be 100% perfect, because full real world knowledge is not possible.\\n\\x0f Applications that are less than 100% perfect can be useful (humans aren't 100% perfect anyway).\\n\\x0f Applications that aid humans are much easier to construct than applications which replace humans. It is dif\\x02cult\\nto make the limitations of systems which accept speech or language obvious to naive human users.\\n\\x0f NLP interfaces are nearly always competing with a non-language based approach.\\n\\x0f Currently nearly all applications either do relatively shallow processing on arbitrary input or deep processing on\\nnarrow domains. MT can be domain-speci\\x02c to varying extents: MT on arbitrary text isn't very good, but has\\nsome applications.\\n\\x0f Limited domain systems require extensive and expensive expertise to port. Research that relies on extensive\\nhand-coding of knowledge for small domains is now generally regarded as a dead-end, though reusable hand-\\ncoding is a different matter.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 11}, page_content='hand-coding of knowledge for small domains is now generally regarded as a dead-end, though reusable hand-\\ncoding is a different matter.\\n\\x0f The development of NLP has mainly been driven by hardware and software advances, and societal and infras-\\ntructure changes, not by great new ideas. Improvements in NLP techniques are generally incremental rather\\nthan revolutionary.\\n12'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 12}, page_content='2 Lecture 2: Morphology and \\x02nite-state techniques\\nThis lecture starts with a brief discussion of morphology, concentrating mainly on English morphology. The concept\\nof a lexicon in an NLP system is discussed with respect to morphological processing. Spelling rules are introduced\\nand the use of \\x02nite state transducers to implement spelling rules is explained. The lecture concludes with a brief\\noverview of some other uses of \\x02nite state techniques in NLP.\\n2.1 A very brief and simpli\\x02ed introduction to morphology\\nMorphology concerns the structure of words. Words are assumed to be made up of morphemes, which are the minimal\\ninformation carrying unit. Morphemes which can only occur in conjunction with other morphemes are af\\x02xes: words\\nare made up of a stem (more than one in the case of compounds) and zero or more af\\x02xes. For instance, dog is a stem\\nwhich may occur with the plural suf\\x02x +s i.e., dogs. English only has suf\\x02xes (af\\x02xes which come after a stem) and'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 12}, page_content=\"which may occur with the plural suf\\x02x +s i.e., dogs. English only has suf\\x02xes (af\\x02xes which come after a stem) and\\npre\\x02xes (which come before the stem \\x97 in English these are limited to derivational morphology), but other languages\\nhave in\\x02xes (af\\x02xes which occur inside the stem) and circum\\x02xes (af\\x02xes which go around a stem). For instance,\\nArabic has stems (root forms) such as k t b, which are combined with in\\x02xes to form words (e.g., kataba, he wrote;\\nkotob, books). Some English irregular verbs show a relic of in\\x03ection by in\\x02xation (e.g. sing, sang, sung) but this\\nprocess is no longer productive (i.e., it won't apply to any new words, such as ping).8\\n2.2 In\\x03ectional vs derivational morphology\\nIn\\x03ectional and derivational morphology can be distinguished, although the dividing line isn't always sharp. The\\ndistinction is of some importance in NLP, since it means different representation techniques may be appropriate.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 12}, page_content=\"distinction is of some importance in NLP, since it means different representation techniques may be appropriate.\\nIn\\x03ectional morphology can be thought of as setting values of slots in some paradigm. In\\x03ectional morphology\\nconcerns properties such as tense, aspect, number, person, gender, and case, although not all languages code all of\\nthese: English, for instance, has very little morphological marking of case and gender. Derivational af\\x02xes, such as\\nun-, re-, anti- etc, have a broader range of semantic possibilities and don't \\x02t into neat paradigms. In\\x03ectional af\\x02xes\\nmay be combined (though not in English). However, there are always obvious limits to this, since once all the possible\\nslot values are `set', nothing else can happen. In contrast, there are no obvious limitations on the number of derivational\\naf\\x02xes (antidisestablishmentarianism, antidisestablishmentarianismization) and they may even be applied recursively\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 12}, page_content=\"af\\x02xes (antidisestablishmentarianism, antidisestablishmentarianismization) and they may even be applied recursively\\n(antiantimissile). In some languages, such as Inuit, derivational morphology is often used where English would use\\nadjectival modi\\x02cation or other syntactic means. This leads to very long `words' occurring naturally and is presumably\\nresponsible for the claim that `Eskimo' has hundreds of words for snow.\\nIn\\x03ectional morphology is generally close to fully productive, in the sense that a word of a particular class will\\ngenerally show all the possible in\\x03ections although the actual af\\x02x used may vary. For instance, an English verb will\\nhave a present tense form, a 3rd person singular present tense form, a past participle and a passive participle (the latter\\ntwo being the same for regular verbs). This will also apply to any new words which enter the language: e.g., text as\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 12}, page_content=\"two being the same for regular verbs). This will also apply to any new words which enter the language: e.g., text as\\na verb \\x97 texts, texted. Derivational morphology is less productive and the classes of words to which an af\\x02x applies\\nis less clearcut. For instance, the suf\\x02x -ee is relatively productive (textee sounds plausible, meaning the recipient\\nof a text message, for instance), but doesn't apply to all verbs (?snoree, ?jogee, ?dropee). Derivational af\\x02xes may\\nchange the part of speech of a word (e.g., -ise/-ize converts nouns into verbs: plural, pluralise). However, there are\\nalso examples of what is sometimes called zero derivation, where a similar effect is observed without an af\\x02x: e.g.\\ntango, waltz etc are words which are basically nouns but can be used as verbs.\\nStems and af\\x02xes can be individually ambiguous. There is also potential for ambiguity in how a word form is split into\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 12}, page_content=\"tango, waltz etc are words which are basically nouns but can be used as verbs.\\nStems and af\\x02xes can be individually ambiguous. There is also potential for ambiguity in how a word form is split into\\nmorphemes. For instance, unionised could be union -ise -ed or (in chemistry) un- ion -ise -ed. This sort of structural\\nambiguity isn't nearly as common in English morphology as in syntax, however. Note that un- ion is not a possible\\nform (because un- can't attach to a noun). Furthermore, although there is a pre\\x02x un- that can attach to verbs, it nearly\\nalways denotes a reversal of a process (e.g., untie), whereas the un- that attaches to adjectives means `not', which is\\nthe meaning in the case of un- ion -ise -ed. Hence the internal structure of un- ion -ise -ed has to be (un- ((ion -ise)\\n-ed)).\\n8Arguably, though, spoken English has one productive in\\x02xation process, exempli\\x02ed by absobloodylutely.\\n13\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 13}, page_content='2.3 Spelling rules\\nEnglish morphology is essentially concatenative: i.e., we can think of words as a sequence of pre\\x02xes, stems and\\nsuf\\x02xes. Some words have irregular morphology and their in\\x03ectional forms simply have to be listed. However, in\\nother cases, there are regular phonological or spelling changes associated with af\\x02xation. For instance, the suf\\x02x -s is\\npronounced differently when it is added to a stem which ends in s, x or z and the spelling re\\x03ects this with the addition\\nof an e (boxes etc). For the purposes of this course, we\\'ll just talk about spelling effects rather than phonological\\neffects: these effects can be captured by spelling rules (also known as orthographic rules).\\nEnglish spelling rules can be described independently of the particular stems and af\\x02xes involved, simply in terms of\\nthe af\\x02x boundary. The `e-insertion\\' rule can be described as follows:\\n\"!e=\\n8\\n<\\n:\\ns\\nx\\nz\\n9\\n=\\n;^\\ns'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 13}, page_content='the af\\x02x boundary. The `e-insertion\\' rule can be described as follows:\\n\"!e=\\n8\\n<\\n:\\ns\\nx\\nz\\n9\\n=\\n;^\\ns\\nIn such rules, the mapping is always given from the `underlying\\' form to the surface form, the mapping is shown to\\nthe left of the slash and the context to the right, with the indicating the position in question. \"is used for the empty\\nstring and ^ for the af\\x02x boundary. This particular rule is read as saying that the empty string maps to `e\\' in the context\\nwhere it is preceded by an s,x, or z and an af\\x02x boundary and followed by an s. For instance, this maps box^s to boxes.\\nThis rule might look as though it is written in a context sensitive grammar formalism, but actually we\\'ll see in x2.7\\nthat it corresponds to a \\x02nite state transducer. Because the rule is independent of the particular af\\x02x, it applies equally\\nto the plural form of nouns and the 3rd person singular present form of verbs. Other spelling rules in English include'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 13}, page_content='to the plural form of nouns and the 3rd person singular present form of verbs. Other spelling rules in English include\\nconsonant doubling (e.g., rat, ratted, though note, not *auditted) and y/ie conversion (party, parties).\\n2.4 Applications of morphological processing\\nIt is possible to use a full-form lexicon for English NLP: i.e., to list all the in\\x03ected forms and to treat derivational\\nmorphology as non-productive. However, when a new word has to be treated (generally because the application is\\nexpanded but in principle because a new word has entered the language) it is redundant to have to specify (or learn)\\nthe in\\x03ected forms as well as the stem, since the vast majority of words in English have regular morphology. So a\\nfull-form lexicon is best regarded as a form of compilation. Many other languages have many more in\\x03ectional forms,\\nwhich increases the need to do morphological analysis rather than full-form listing.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 13}, page_content='which increases the need to do morphological analysis rather than full-form listing.\\nIR systems use stemming rather than full morphological analysis. For IR, what is required is to relate forms, not to\\nanalyse them compositionally, and this can most easily be achieved by reducing all morphologically complex forms\\nto a canonical form. Although this is referred to as stemming, the canonical form may not be the linguistic stem.\\nThe most commonly used algorithm is the Porter stemmer, which uses a series of simple rules to strip endings (see\\nJ&M, section 3.4) without the need for a lexicon. However, stemming does not necessarily help IR. Search engines\\nsometimes do in\\x03ectional morphology, but this can be dangerous. For instance, one search engine searches for corpus\\nas well as corpora when given the latter as input, resulting in a large number of spurious results involving Corpus\\nChristi and similar terms.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 13}, page_content=\"as well as corpora when given the latter as input, resulting in a large number of spurious results involving Corpus\\nChristi and similar terms.\\nIn most NLP applications, however, morphological analysis is a precursor to some form of parsing. In this case, the\\nrequirement is to analyse the form into a stem and af\\x02xes so that the necessary syntactic (and possibly semantic)\\ninformation can be associated with it. Morphological analysis is often called lemmatization. For instance, for the part\\nof speech tagging application which we will discuss in the next lecture, mugged would be assigned a part of speech\\ntag which indicates it is a verb, though mug is ambiguous between verb and noun. For full parsing, as discussed\\nin lectures 4 and 5, we'll need more detailed syntactic and semantic information. Morphological generation takes a\\nstem and some syntactic information and returns the correct form. For some applications, there is a requirement that\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 13}, page_content='stem and some syntactic information and returns the correct form. For some applications, there is a requirement that\\nmorphological processing is bidirectional: that is, can be used for analysis and generation. The \\x02nite state transducers\\nwe will look at below have this property.\\n2.5 Lexical requirements for morphological processing\\nThere are three sorts of lexical information that are needed for full, high precision morphological processing:\\n14'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 14}, page_content=\"\\x0f af\\x02xes, plus the associated information conveyed by the af\\x02x\\n\\x0f irregular forms, with associated information similar to that for af\\x02xes\\n\\x0f stems with syntactic categories (plus more detailed information if derivational morphology is to be treated as\\nproductive)\\nOne approach to an af\\x02x lexicon is for it to consist of a pairing of af\\x02x and some encoding of the syntactic/semantic\\neffect of the af\\x02x.9 For instance, consider the following fragment of a suf\\x02x lexicon (we can assume there is a separate\\nlexicon for pre\\x02xes):\\ned PAST_VERB\\ned PSP_VERB\\ns PLURAL_NOUN\\nHere PAST_VERB, PSP_VERBand PLURAL_NOUNare abbreviations for some bundle of syntactic/semantic infor-\\nmation: we'll discuss this brie\\x03y in x5.7.\\nA lexicon of irregular forms is also needed. One approach is for this to just be a triple consisting of in\\x03ected form,\\n`af\\x02x information' and stem, where `af\\x02x information' corresponds to whatever encoding is used for the regular af\\x02x.\\nFor instance:\\nbegan PAST_VERB begin\\nbegun PSP_VERB begin\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 14}, page_content=\"`af\\x02x information' and stem, where `af\\x02x information' corresponds to whatever encoding is used for the regular af\\x02x.\\nFor instance:\\nbegan PAST_VERB begin\\nbegun PSP_VERB begin\\nNote that this information can be used for generation as well as analysis, as can the af\\x02x lexicon.\\nIn most cases, English irregular forms are the same for all senses of a word. For instance, ran is the past of run\\nwhether we are talking about athletes, politicians or noses. This argues for associating irregularity with particular\\nword forms rather than particular senses, especially since compounds also tend to follow the irregular spelling, even\\nnon-productively formed ones (e.g., the plural of dormouse is dormice). However, there are exceptions: e.g., The\\nwashing was hung/*hanged out to dry vs the murderer was hanged.\\nMorphological analysers also generally have access to a lexicon of regular stems. This is needed for high precision:\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 14}, page_content=\"washing was hung/*hanged out to dry vs the murderer was hanged.\\nMorphological analysers also generally have access to a lexicon of regular stems. This is needed for high precision:\\ne.g. to avoid analysing corpus as corpu -s we need to know that there isn't a word corpu. There are also cases where\\nhistorically a word was derived, but where the base form is no longer found in the language: we can avoid analysing\\nunkempt as un- kempt, for instance, simply by not having kempt in the stem lexicon. Ideally this lexicon should have\\nsyntactic information: for instance, feed could be fee -ed, but since fee is a noun rather than a verb, this isn't a possible\\nanalysis. However, in the approach we'll assume, the morphological analyser is split into two stages. The \\x02rst of\\nthese only concerns morpheme forms and returns both fee -ed and feed given the input feed. A second stage which is\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 14}, page_content=\"these only concerns morpheme forms and returns both fee -ed and feed given the input feed. A second stage which is\\nclosely coupled to the syntactic analysis then rules out fee -ed because the af\\x02x and stem syntactic information are not\\ncompatible (see x5.7 for one approach to this).\\nIf morphology was purely concatenative, it would be very simple to write an algorithm to split off af\\x02xes. Spelling\\nrules complicate this somewhat: in fact, it's still possible to do a reasonable job for English with ad hoc code, but a\\ncleaner and more general approach is to use \\x02nite state techniques.\\n2.6 Finite state automata for recognition\\nThe approach to spelling rules that we'll describe involves the use of \\x02nite state transducers (FSTs). Rather than\\njumping straight into this, we'll brie\\x03y consider the simpler \\x02nite state automata and how they can be used in a simple\\nrecogniser. Suppose we want to recognise dates (just day and month pairs) written in the format day/month. The day\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 14}, page_content='recogniser. Suppose we want to recognise dates (just day and month pairs) written in the format day/month. The day\\nand the month may be expressed as one or two digits (e.g. 11/2, 1/12 etc). This format corresponds to the following\\nsimple FSA, where each character corresponds to one transition:\\n9J&M describe an alternative approach which is to make the syntactic information correspond to a level in a \\x02nite state transducer. However, at\\nleast for English, this considerably complicates the transducers.\\n15'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 15}, page_content=\"0,1,2,3 digit / 0,1 0,1,2\\ndigit digit\\n1 2 3 4 5 6\\nAccept states are shown with a double circle. This is a non-deterministic FSA: for instance, an input starting with the\\ndigit 3 will move the FSA to both state 2 and state 3. This corresponds to a local ambiguity: i.e., one that will be\\nresolved by subsequent context. By convention, there must be no `left over' characters when the system is in the \\x02nal\\nstate.\\nTo make this a bit more interesting, suppose we want to recognise a comma-separated list of such dates. The FSA,\\nshown below, now has a cycle and can accept a sequence of inde\\x02nite length (note that this is iteration and not full\\nrecursion, however).\\n0,1,2,3 digit / 0,1 0,1,2\\ndigit digit\\n,\\n1 2 3 4 5 6\\nBoth these FSAs will accept sequences which are not valid dates, such as 37/00. Conversely, if we use them to generate\\n(random) dates, we will get some invalid output. In general, a system which generates output which is invalid is said\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 15}, page_content=\"(random) dates, we will get some invalid output. In general, a system which generates output which is invalid is said\\nto overgenerate. In fact, in many language applications, some amount of overgeneration can be tolerated, especially if\\nwe are only concerned with analysis.\\n2.7 Finite state transducers\\nFSAs can be used to recognise particular patterns, but don't, by themselves, allow for any analysis of word forms.\\nHence for morphology, we use \\x02nite state transducers (FSTs) which allow the surface structure to be mapped into the\\nlist of morphemes. FSTs are useful for both analysis and generation, since the mapping is bidirectional. This approach\\nis known as two-level morphology.\\nTo illustrate two-level morphology, consider the following FST, which recognises the af\\x02x -s allowing for environ-\\nments corresponding to the e-insertion spelling rule shown in x2.3.10\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 15}, page_content=\"To illustrate two-level morphology, consider the following FST, which recognises the af\\x02x -s allowing for environ-\\nments corresponding to the e-insertion spelling rule shown in x2.3.10\\n10Actually, I've simpli\\x02ed this slightly so the correspondence to the spelling rule is not exact: J&M give a more complex transducer which is an\\naccurate re\\x03ection of the spelling rule.\\n16\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 16}, page_content='1\\nother : other\\n\": ^\\n2\\ns : s\\n3\\n4\\nother : other\\ns : sx : xz : z e : ^\\ns : sx : xz : z\\nTransducers map between two representations, so each transition corresponds to a pair of characters. As with the\\nspelling rule, we use the special character `\"\\' to correspond to the empty character and `^\\' to correspond to an af\\x02x\\nboundary. The abbreviation `other : other\\' means that any character not mentioned speci\\x02cally in the FST maps to\\nitself. As with the FSA example, we assume that the FST only accepts an input if the end of the input corresponds to\\nan accept state (i.e., no `left-over\\' characters are allowed).\\nFor instance, with this FST, `d o g s\\' maps to `d o g \\x88s\\', `f o x e s\\' maps to `f o x \\x88s\\' and `b u z z e s\\' maps to `b u\\nz z \\x88s\\'. When the transducer is run in analysis mode, this means the system can detect an af\\x02x boundary (and hence\\nlook up the stem and the af\\x02x in the appropriate lexicons). In generation mode, it can construct the correct string. This\\nFST is non-deterministic.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 16}, page_content=\"look up the stem and the af\\x02x in the appropriate lexicons). In generation mode, it can construct the correct string. This\\nFST is non-deterministic.\\nSimilar FSTs can be written for the other spelling rules for English (although to do consonant doubling correctly, in-\\nformation about stress and syllable boundaries is required and there are also differences between British and American\\nspelling conventions which complicate matters). Morphology systems are usually implemented so that there is one\\nFST per spelling rule and these operate in parallel.\\nOne issue with this use of FSTs is that they do not allow for any internal structure of the word form. For instance, we\\ncan produce a set of FSTs which will result in unionised being mapped into un^ion^ise^ed, but as we've seen, the\\naf\\x02xes actually have to be applied in the right order and this isn't modelled by the FSTs.\\n2.8 Some other uses of \\x02nite state techniques in NLP\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 16}, page_content=\"af\\x02xes actually have to be applied in the right order and this isn't modelled by the FSTs.\\n2.8 Some other uses of \\x02nite state techniques in NLP\\n\\x0f Grammars for simple spoken dialogue systems. Finite state techniques are not adequate to model grammars\\nof natural languages: we'll discuss this a little in x4.12. However, for very simple spoken dialogue systems,\\na \\x02nite-state grammar may be adequate. More complex grammars can be written as CFGs and compiled into\\n\\x02nite state approximations.\\n\\x0f Partial grammars for named entity recognition (brie\\x03y discussed in x4.12).\\n\\x0f Dialogue models for spoken dialogue systems (SDS). SDS use dialogue models for a variety of purposes: in-\\ncluding controlling the way that the information acquired from the user is instantiated (e.g., the slots that are\\n\\x02lled in an underlying database) and limiting the vocabulary to achieve higher recognition rates. FSAs can be\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 16}, page_content='\\x02lled in an underlying database) and limiting the vocabulary to achieve higher recognition rates. FSAs can be\\nused to record possible transitions between states in a simple dialogue. For instance, consider the problem of\\n17'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 17}, page_content=\"obtaining a date expressed as a day and a month from a user. There are four possible states, corresponding to\\nthe user input recognised so far:\\n1. No information. System prompts for month and day.\\n2. Month only is known. System prompts for day.\\n3. Day only is known. System prompts for month.\\n4. Month and day known.\\nThe FSA is shown below. The loops that stay in a single state correspond to user responses that aren't recognised\\nas containing the required information (mumble is the term generally used for an unrecognised input).\\n1\\nmumble\\nmonth day\\nday &month2\\nmumble\\nday\\n3\\nmumble\\nmonth\\n4\\n2.9 Probabilistic FSAs\\nIn many cases, it is useful to augment the FSA with information about transition probabilities. For instance, in the\\nSDS system described above, it is more likely that a user will specify a month alone than a day alone. A probabilistic\\nFSA for the SDS is shown below. Note that the probabilities on the outgoing arcs from each state must sum to 1.\\n1\\n0.1\\n0.5 0.1\\n0.32\\n0.1\\n0.9\\n3\\n0.2\\n0.8\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 17}, page_content=\"FSA for the SDS is shown below. Note that the probabilities on the outgoing arcs from each state must sum to 1.\\n1\\n0.1\\n0.5 0.1\\n0.32\\n0.1\\n0.9\\n3\\n0.2\\n0.8\\n4\\n2.10 Further reading\\nChapters 2 and 3 of J&M. Much of Chapter 2 should be familiar from other courses in the CST (at least to Part II\\nstudents). Chapter 3 uses more elaborate transducers than I've discussed.\\n18\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 18}, page_content='3 Lecture 3: Prediction and part-of-speech tagging\\nThis lecture introduces some simple statistical techniques and illustrates their use in NLP for prediction of words and\\npart-of-speech categories. It starts with a discussion of corpora, then introduces word prediction. Word prediction can\\nbe seen as a way of (crudely) modelling some syntactic information (i.e., word order). Similar statistical techniques\\ncan also be used to discover parts of speech for uses of words in a corpus. The lecture concludes with some discussion\\nof evaluation.\\n3.1 Corpora\\nA corpus (corpora is the plural) is simply a body of text that has been collected for some purpose. A balanced\\ncorpus contains texts which represent different genres (newspapers, \\x02ction, textbooks, parliamentary reports, cooking\\nrecipes, scienti\\x02c papers etc etc): early examples were the Brown corpus (US English) and the Lancaster-Oslo-Bergen'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 18}, page_content=\"recipes, scienti\\x02c papers etc etc): early examples were the Brown corpus (US English) and the Lancaster-Oslo-Bergen\\n(LOB) corpus (British English) which are each about 1 million words: the more recent British National Corpus (BNC)\\ncontains approx 100 million words and includes 20 million words of spoken English. Corpora are important for many\\ntypes of linguistic research, although mainstream linguists have tended to dismiss their use in favour of reliance on\\nintuitive judgements. Corpora are essential for much modern NLP research, though NLP researchers have often used\\nnewspaper text (particularly the Wall Street Journal) rather than balanced corpora.\\nDistributed corpora are often annotated in some way: the most important type of annotation for NLP is part-of-speech\\ntagging (POS tagging), which we'll discuss further below.\\nCorpora may also be collected for a speci\\x02c task. For instance, when implementing an email answering application,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 18}, page_content=\"tagging (POS tagging), which we'll discuss further below.\\nCorpora may also be collected for a speci\\x02c task. For instance, when implementing an email answering application,\\nit is essential to collect samples of representative emails. For interface applications in particular, collecting a corpus\\nrequires a simulation of the actual application: generally this is done by a Wizard of Oz experiment, where a human\\npretends to be a computer.\\nCorpora are needed in NLP for two reasons. Firstly, we have to evaluate algorithms on real language: corpora are\\nrequired for this purpose for any style of NLP. Secondly, corpora provide the data source for many machine-learning\\napproaches.\\n3.2Prediction\\nThe essential idea of prediction is that, given a sequence of words, we want to determine what's most likely to come\\nnext. There are a number of reasons to want to do this: the most important is as a form of language modelling for\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 18}, page_content=\"next. There are a number of reasons to want to do this: the most important is as a form of language modelling for\\nautomatic speech recognition. Speech recognisers cannot accurately determine a word from the sound signal for that\\nword alone, and they cannot reliably tell where each word starts and \\x02nishes.11 So the most probable word is chosen\\non the basis of the language model, which predicts the most likely word, given the prior context. The language models\\nwhich are currently most effective work on the basis of N-grams (a type of Markov chain), where the sequence of the\\nprior n\\x001 words is used to predict the next. Trigram models use the preceding 2 words, bigram models the preceding\\nword and unigram models use no context at all, but simply work on the basis of individual word probabilities. Bigrams\\nare discussed below, though I won't go into details of exactly how they are used in speech recognition.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 18}, page_content=\"are discussed below, though I won't go into details of exactly how they are used in speech recognition.\\nWord prediction is also useful in communication aids: i.e., systems for people who can't speak because of some form\\nof disability. People who use text-to-speech systems to talk because of a non-linguistic disability usually have some\\nform of general motor impairment which also restricts their ability to type at normal rates (stroke, ALS, cerebral\\npalsy etc). Often they use alternative input devices, such as adapted keyboards, puffer switches, mouth sticks or\\neye trackers. Generally such users can only construct text at a few words a minute, which is too slow for anything\\nlike normal communication to be possible (normal speech is around 150 words per minute). As a partial aid, a word\\nprediction system is sometimes helpful: this gives a list of candidate words that changes as the initial letters are entered\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 18}, page_content=\"prediction system is sometimes helpful: this gives a list of candidate words that changes as the initial letters are entered\\nby the user. The user chooses the desired word from a menu when it appears. The main dif\\x02culty with using statistical\\nprediction models in such applications is in \\x02nding enough data: to be useful, the model really has to be trained on an\\nindividual speaker's output, but of course very little of this is likely to be available.\\n11In fact, although humans are better at doing this than speech recognisers, we also need context to recognise words, especially words like the\\nand a.\\n19\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 19}, page_content=\"Prediction is important in estimation of entropy, including estimations of the entropy of English. The notion of entropy\\nis important in language modelling because it gives a metric for the dif\\x02culty of the prediction problem. For instance,\\nspeech recognition is much easier in situations where the speaker is only saying two easily distinguishable words than\\nwhen the vocabulary is unlimited: measurements of entropy can quantify this, but won't be discussed further in this\\ncourse.\\nOtherapplications for prediction include optical character recognition (OCR), spelling correction and text segmen-\\ntation for languages such as Chinese, which are conventionally written without explicit word boundaries. Some ap-\\nproaches to word sense disambiguation, to be discussed in lecture 6, can also be treated as a form of prediction.\\n3.3 bigrams\\nA bigram model assigns a probability to a word based on the previous word: i.e. P(wnjwn\\x001) where wn is the nth\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 19}, page_content=\"3.3 bigrams\\nA bigram model assigns a probability to a word based on the previous word: i.e. P(wnjwn\\x001) where wn is the nth\\nword in some string. The probability of some string of words P(Wn\\n1 ) is thus approximated by the product of these\\nconditional probabilities:\\nP(Wn\\n1 ) \\x19\\nnY\\nk=1\\nP(wkjwk\\x001)\\nFor example, suppose we have the following tiny corpus of utterances:\\ngood morning\\ngood afternoon\\ngood afternoon\\nit is very good\\nit is good\\nWe'll use the symbol hsito indicate the start of an utterance, so the corpus really looks like:\\nhsigood morning hsigood afternoon hsigood afternoon hsiit is very good hsiit is good hsi\\nThe bigram probabilities are given as\\nC(wn\\x001wn)\\nP\\nwC(wn\\x001w)\\ni.e. the count of a particular bigram, normalised by dividing by the total number of bigrams starting with the same\\nword (which is equivalent to the total number of occurrences of that word, except in the case of the last token, a\\ncomplication which can be ignored for a reasonable size of corpus).\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 19}, page_content='word (which is equivalent to the total number of occurrences of that word, except in the case of the last token, a\\ncomplication which can be ignored for a reasonable size of corpus).\\nsequence count bigram probability\\n<s> 5\\n<s> good 3 .6\\n<s> it 2 .4\\ngood 5\\ngood morning 1 .2\\ngood afternoon 2 .4\\ngood <s> 2 .4\\nmorning 1\\nmorning <s> 1 1\\nafternoon 2\\nafternoon <s> 2 1\\nit is 2 1\\nis very 1 .5\\nis good 1 .5\\nvery good 1 1\\n20'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 20}, page_content=\"This yields a probability of 0.24 for the string `hsigood hsi' which is the highest probability utterance that we can\\nconstruct on the basis of the bigrams from this corpus, if we impose the constraint that an utterance must begin with\\nhsiand end with hsi.\\nFor application to communication aids, we are simply concerned with predicting the next word: once the user has\\nmade their choice, the word can't be changed. For speech recognition, the N-gram approach is applied to maximise\\nthe likelihood of a sequence of words, hence we're looking to \\x02nd the most likely sequence overall. Notice that we\\ncan regard bigrams as comprising a simple deterministic weighted FSA. The Viterbi algorithm, an ef\\x02cient method of\\napplying N-grams in speech recognition and other applications, is usually described in terms of an FSA.\\nThe probability of `hsivery good' based on this corpus is 0, since the conditional probability of `very' given `hsi' is 0\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 20}, page_content=\"The probability of `hsivery good' based on this corpus is 0, since the conditional probability of `very' given `hsi' is 0\\nsince we haven't found any examples of this in the training data. In general, this is problematic because we will never\\nhave enough data to ensure that we will see all possible events and so we don't want to rule out unseen events entirely.\\nTo allow for sparse data we have to use smoothing, which simply means that we make some assumption about the\\n`real' probability of unseen or very infrequently seen events and distribute that probability appropriately. A common\\napproach is simply to add one to all counts: this is add-one smoothing which is not sound theoretically, but is simple\\nto implement. A better approach in the case of bigrams is to backoff to the unigram probabilities: i.e., to distribute\\nthe unseen probability mass so that it is proportional to the unigram probabilities. This sort of estimation is extremely\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 20}, page_content=\"the unseen probability mass so that it is proportional to the unigram probabilities. This sort of estimation is extremely\\nimportant to get good results from N-gram techniques, but we won't discuss the details in this course.\\n3.4 Part of speech tagging\\nPrediction techniques can be used for word classes, rather than just individual words. One important application is to\\npart-of-speech tagging (POS tagging), where the words in a corpus are associated with a tag indicating some syntactic\\ninformation that applies to that particular use of the word. For instance, consider the example sentence below:\\nThey can \\x02sh.\\nThis has two readings: one (the most likely) about ability to \\x02sh and other about putting \\x02sh in cans. \\x02sh is ambiguous\\nbetween a singular noun, plural noun and a verb, while can is ambiguous between singular noun, verb (the `put in\\ncans' use) and modal verb. However, they is unambiguously a pronoun. (I am ignoring some less likely possibilities,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 20}, page_content=\"cans' use) and modal verb. However, they is unambiguously a pronoun. (I am ignoring some less likely possibilities,\\nsuch as proper names.) These distinctions can be indicated by POS tags:\\nthey PNP\\ncan VM0 VVB VVI NN1\\nfish NN1 NN2 VVB VVI\\nThere are several standard tagsets used in corpora and in POS tagging experiments. The one I'm using for the examples\\nin this lecture is CLAWS 5 (C5) which is given in full in appendix C in J&M. The meaning of the tags above is:\\nNN1 singular noun\\nNN2 plural noun\\nPNP personal pronoun\\nVM0 modal auxiliary verb\\nVVB base form of verb (except infinitive)\\nVVI infinitive form of verb (i.e. occurs with `to')\\nA POS tagger resolves the lexical ambiguities to give the most likely set of tags for the sentence. In this case, the right\\ntagging is likely to be:\\nThey PNP can VM0 \\x02sh VVB . PUN\\nNote the tag for the full stop: punctuation is treated as unambiguous. POS tagging can be regarded as a form of very\\nbasic word sense disambiguation.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 20}, page_content='They PNP can VM0 \\x02sh VVB . PUN\\nNote the tag for the full stop: punctuation is treated as unambiguous. POS tagging can be regarded as a form of very\\nbasic word sense disambiguation.\\nThe other syntactically possible reading is:\\nThey PNP can VVB \\x02sh NN2 . PUN\\n21'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 21}, page_content=\"However, POS taggers (unlike full parsers) don't attempt to produce globally coherent analyses. Thus a POS tagger\\nmight return:\\nThey PNP can VM0 \\x02sh NN2 . PUN\\ndespite the fact that this doesn't correspond to a possible reading of the sentence.\\nPOS tagging is useful as a way of annotating a corpus because it makes it easier to extract some types of information\\n(for linguistic research or NLP experiments). It also acts as a basis for more complex forms of annotation. Named\\nentity recognisers (discussed in lecture 4) are generally run on POS-tagged data. POS taggers are sometimes run as\\npreprocessors to full parsing, since this can cut down the search space to be considered by the parser.\\n3.5 Stochastic POS tagging\\nOne form of POS tagging applies the N-gram technique that we saw above, but in this case it applies to the POS\\ntags rather than the individual words. The most common approaches depend on a small amount of manually tagged\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 21}, page_content=\"tags rather than the individual words. The most common approaches depend on a small amount of manually tagged\\ntraining data from which POS N-grams can be extracted.12 I'll illustrate this with respect to another trivial corpus:\\nThey used to can \\x02sh in those towns. But now few people \\x02sh there.\\nThis might be tagged as follows:\\nThey_PNP used_VVD to_TO0 can_VVI fish_NN2 in_PRP those_DT0 towns_NN2 ._PUN\\nBut_CJC now_AV0 few_DT0 people_NN2 fish_VVB in_PRP these_DT0 areas_NN2 ._PUN\\nThis yields the following counts and probabilities:\\nsequence count bigram probability\\nAV0 1\\nAV0 DT0 1 1\\nCJC 1\\nCJC AV0 1 1\\nDT0 3\\nDT0 NN2 3 1\\nNN2 4\\nNN2 PRP 1 0.25\\nNN2 PUN 2 0.5\\nNN2 VVB 1 0.25\\nPNP 1\\nPNP VVD 1 1\\nPRP 1\\nPRP DT0 2 1\\nPUN 1\\nPUN CJC 1 1\\nTO0 1\\n12It is possible to build POS taggers that work without a hand-tagged corpus, but they don't perform as well as a system trained on even a 1,000\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 21}, page_content=\"PNP VVD 1 1\\nPRP 1\\nPRP DT0 2 1\\nPUN 1\\nPUN CJC 1 1\\nTO0 1\\n12It is possible to build POS taggers that work without a hand-tagged corpus, but they don't perform as well as a system trained on even a 1,000\\nword corpus which can be tagged in a few hours. Furthermore, these algorithms still require a lexicon which associates possible tags with words.\\n22\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 22}, page_content=\"TO0 VVI 1 1\\nVVB 1\\nVVB PRP 1 1\\nVVD 1\\nVVD TO0 1 1\\nVVI 1\\nVVI NN2 1 1\\nWe can also obtain a lexicon from the tagged data:\\nword tag count\\nthey PNP 1\\nused VVD 1\\nto TO0 1\\ncan VVI 1\\nfish NN2 1\\nVVB 1\\nin PRP 2\\nthose DT0 1\\ntowns NN2 1\\n. PUN 1\\nbut CJC 1\\nnow AV0 1\\nfew DT0 1\\npeople NN2 1\\nthese DT0 1\\nareas NN2 1\\nThe idea of stochastic POS tagging is that the tag can be assigned based on consideration of the lexical probability\\n(how likely it is that the word has that tag), plus the sequence of prior tags. For a bigram model, we only look at\\na single previous tag. This is slightly more complicated than the word prediction case because we have to take into\\naccount both words and tags.\\nWe are trying to estimate the probability of a sequence of tags given a sequence of words: P(TjW). By Bayes\\ntheorem:\\nP(TjW) = P(T)P(WjT)\\nP(W)\\nSince we're looking at assigning tags to a particular sequence of words, P(W) is constant, so for a relative measure\\nof probability we can use:\\nP(TjW) = P(T)P(WjT)\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 22}, page_content=\"theorem:\\nP(TjW) = P(T)P(WjT)\\nP(W)\\nSince we're looking at assigning tags to a particular sequence of words, P(W) is constant, so for a relative measure\\nof probability we can use:\\nP(TjW) = P(T)P(WjT)\\nWe now have to estimate P(T) and P(WjT). If we make the bigram assumption, P(T) is approximated by P(tijti\\x001)\\n\\x97 i.e., the probability of some tag given the immediately preceding tag. We approximate P(WjT) as P(wijti). These\\nvalues can be estimated from the corpus frequencies.\\nNote that we end up multiplying P(tijti\\x001) with P(wijti) (the probability of the word given the tag) rather than\\nP(tijwi) (the probability of the tag given the word). For instance, if we're trying to choose between the tags NN2\\nand VVB for \\x02sh in the sentence they \\x02sh, we calculate P(NN2ijPNPi\\x001), P(\\x02shijNN2i), P(VVBijPNPi\\x001) and\\nP(\\x02shijVVBi).\\nIn fact, POS taggers generally use trigrams rather than bigrams \\x97 the relevant equations are given in J&M, page 306.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 22}, page_content='P(\\x02shijVVBi).\\nIn fact, POS taggers generally use trigrams rather than bigrams \\x97 the relevant equations are given in J&M, page 306.\\nAs with word prediction, backoff and smoothing are crucial for reasonable performance.\\nWhen a POS tagger sees a word which was not in its training data, we need some way of assigning possible tags to the\\nword. One approach is simply to use all possible open class tags, with probabilities based on the unigram probabilities\\n23'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 23}, page_content=\"of those tags. Open class words are ones for which we can never give a complete list for a living language, since words\\nare always being added: i.e., verbs, nouns, adjectives and adverbs. The rest are considered closed class. A better\\napproach is to use a morphological analyser to restrict this set: e.g., words ending in -ed are likely to be VVD (simple\\npast) or VVN (past participle), but can't be VVG (-ing form).\\n3.6 Evaluation of POS tagging\\nPOS tagging algorithms are evaluated in terms of percentage of correct tags. The standard assumption is that every\\nword should be tagged with exactly one tag, which is scored as correct or incorrect: there are no marks for near\\nmisses. Generally there are some words which can be tagged in only one way, so are automatically counted as correct.\\nPunctuation is generally given an unambiguous tag. Therefore the success rates of over 95% which are generally\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 23}, page_content='Punctuation is generally given an unambiguous tag. Therefore the success rates of over 95% which are generally\\nquoted for POS tagging are a little misleading: the baseline of choosing the most common tag based on the training\\nset often gives 90% accuracy. Some POS taggers returns multiple tags in cases where more than one tag has a similar\\nprobability.\\nIt is worth noting that increasing the size of the tagset does not necessarily result in decreased performance: this\\ndepends on whether the tags that are added can generally be assigned unambiguously or not. Potentially, adding more\\n\\x02ne-grained tags could increase performance. For instance, suppose we wanted to distinguish between present tense\\nverbs according to whether they were 1st, 2nd or 3rd person. With the C5 tagset, and the stochastic tagger described,\\nthis would be impossible to do with high accuracy, because all pronouns are tagged PRP, hence they provide no'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 23}, page_content='this would be impossible to do with high accuracy, because all pronouns are tagged PRP, hence they provide no\\ndiscriminating power. On the other hand, if we tagged I and we as PRP1, you as PRP2 and so on, the N-gram approach\\nwould allow some discrimination. In general, predicting on the basis of classes means we have less of a sparse data\\nproblem than when predicting on the basis of words, but we also lose discriminating power. There is also something\\nof a tradeoff between the utility of a set of tags and their usefulness in POS tagging. For instance, C5 assigns separate\\ntags for the different forms of be, which is redundant for many purposes, but helps make distinctions between other\\ntags in tagging models where the context is given by a tag sequence alone (i.e., rather than considering words prior to\\nthe current one).\\nPOS tagging exempli\\x02es some general issues in NLP evaluation:'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 23}, page_content='tags in tagging models where the context is given by a tag sequence alone (i.e., rather than considering words prior to\\nthe current one).\\nPOS tagging exempli\\x02es some general issues in NLP evaluation:\\nTraining data and test data The assumption in NLP is always that a system should work on novel data, therefore\\ntest data must be kept unseen.\\nFor machine learning approaches, such as stochastic POS tagging, the usual technique is to spilt a data set into\\n90% training and 10% test data. Care needs to be taken that the test data is representative.\\nFor an approach that relies on signi\\x02cant hand-coding, the test data should be literally unseen by the researchers.\\nDevelopment cycles involve looking at some initial data, developing the algorithm, testing on unseen data,\\nrevising the algorithm and testing on a new batch of data. The seen data is kept for regression testing.\\nBaselines Evaluation should be reported with respect to a baseline, which is normally what could be achieved with a'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 23}, page_content='Baselines Evaluation should be reported with respect to a baseline, which is normally what could be achieved with a\\nvery basic approach, given the same training data. For instance, the baseline for POS tagging with training data\\nis to choose the most common tag for a particular word on the basis of the training data (and to simply choose\\nthe most frequent tag of all for unseen words).\\nCeiling It is often useful to try and compute some sort of ceiling for the performance of an application. This is usually\\ntaken to be human performance on that task, where the ceiling is the percentage agreement found between two\\nannotators (interannotator agreement). For POS tagging, this has been reported as 96% (which makes existing\\nPOS taggers look impressive). However this raises lots of questions: relatively untrained human annotators\\nworking independently often have quite low agreement, but trained annotators discussing results can achieve'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 23}, page_content=\"working independently often have quite low agreement, but trained annotators discussing results can achieve\\nmuch higher performance (approaching 100% for POS tagging). Human performance varies considerably be-\\ntween individuals. In any case, human performance may not be a realistic ceiling on relatively unnatural tasks,\\nsuch as POS tagging.\\nError analysis The error rate on a particular problem will be distributed very unevenly. For instance, a POS tagger\\nwill never confuse the tag PUN with the tag VVN (past participle), but might confuse VVN with AJ0 (adjective)\\nbecause there's a systematic ambiguity for many forms (e.g., given). For a particular application, some errors\\n24\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 24}, page_content='may be more important than others. For instance, if one is looking for relatively low frequency cases of de-\\nnominal verbs (that is verbs derived from nouns \\x97 e.g., canoe, tango, fork used as verbs), then POS tagging is\\nnot directly useful in general, because a verbal use without a characteristic af\\x02x is likely to be mistagged. This\\nmakes POS-tagging less useful for lexicographers, who are often speci\\x02cally interested in \\x02nding examples of\\nunusual word uses. Similarly, in text categorisation, some errors are more important than others: e.g. treating\\nan incoming order for an expensive product as junk email is a much worse error than the converse.\\nReproducibility If at all possible, evaluation should be done on a generally available corpus so that other researchers\\ncan replicate the experiments.\\n3.7 Further reading\\nThis lecture has skimmed over material that is covered in several chapters of J&M. See 5.9 for the Viterbi algorithm,'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 24}, page_content='can replicate the experiments.\\n3.7 Further reading\\nThis lecture has skimmed over material that is covered in several chapters of J&M. See 5.9 for the Viterbi algorithm,\\nChapter 6 for N-grams (especially 6.3, 6.4 and 6.7), 7.1-7.3 for speech recognition and Chapter 8 on POS tagging.\\n25'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 25}, page_content=\"4 Lecture 4: Parsing and generation\\nIn this lecture, we'll discuss syntax in a way which is much closer to the standard notions in formal linguistics than\\nPOS-tagging is. To start with, we'll brie\\x03y motivate the idea of a generative grammar in linguistics, review the notion\\nof a context-free grammar and then show a context-free grammar for a tiny fragment of English. We'll then show\\nhow context free grammars can be used to implement generators and parsers, and discuss chart parsing, which allows\\nef\\x02cient processing of strings containing a high degree of ambiguity. Finally we'll brie\\x03y touch on probabilistic\\ncontext-free approaches.\\n4.1 Generative grammar\\nSince Chomsky's work in the 1950s, much work in formal linguistics has been concerned with the notion of a genera-\\ntive grammar \\x97 i.e., a formally speci\\x02ed grammar that can generate all and only the acceptable sentences of a natural\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 25}, page_content=\"tive grammar \\x97 i.e., a formally speci\\x02ed grammar that can generate all and only the acceptable sentences of a natural\\nlanguage. It's important to realise that nobody has actually written such a grammar for any natural language or even\\ncome close to doing so: what most linguists are really interested in is the principles that underly such grammars, espe-\\ncially to the extent that they apply to all natural languages. NLP researchers, on the other hand, are at least sometimes\\ninterested in actually building and using large-scale detailed grammars.\\nThe formalisms which are of interest to us for modelling syntax assign internal structure to the strings of a language,\\nwhich can be represented by bracketing. We already saw some evidence of this in derivational morphology (the\\nunionised example), but here we are concerned with the structure of phrases. For instance, the sentence:\\nthe dog slept\\ncan be bracketed\\n((the (big dog)) slept)\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 25}, page_content=\"unionised example), but here we are concerned with the structure of phrases. For instance, the sentence:\\nthe dog slept\\ncan be bracketed\\n((the (big dog)) slept)\\nThe phrase, big dog, is an example of a constituent (i.e. something that is enclosed in a pair of brackets): the big dog\\nis also a constituent, but the big is not. Constituent structure is generally justi\\x02ed by arguments about substitution\\nwhich I won't go into here: J&M discuss this brie\\x03y, but see an introductory syntax book for a full discussion. In this\\ncourse, I will simply give bracketed structures and hope that the constituents make sense intuitively, rather than trying\\nto justify them.\\nTwo grammars are said to be weakly-equivalent if they generate the same strings. Two grammars are strongly-\\nequivalent if they assign the same bracketings to all strings they generate.\\nIn most, but not all, approaches, the internal structures are given labels. For instance, the big dog is a noun phrase\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 25}, page_content=\"equivalent if they assign the same bracketings to all strings they generate.\\nIn most, but not all, approaches, the internal structures are given labels. For instance, the big dog is a noun phrase\\n(abbreviated NP), slept, slept in the park and licked Sandy are verb phrases (VPs). The labels such as NP and VP cor-\\nrespond to non-terminal symbols in a grammar. In this lecture, we'll discuss the use of simple context-free grammars\\nfor language description, moving onto a more expressive formalism in lecture 5.\\n4.2 Context free grammars\\nThe idea of a context-free grammar (CFG) should be familiar from formal language theory. A CFG has four compo-\\nnents, described here as they apply to grammars of natural languages:\\n1. a set of non-terminal symbols (e.g., S, VP), conventionally written in uppercase;\\n2. a set of terminal symbols (i.e., the words), conventionally written in lowercase;\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 25}, page_content='1. a set of non-terminal symbols (e.g., S, VP), conventionally written in uppercase;\\n2. a set of terminal symbols (i.e., the words), conventionally written in lowercase;\\n3. a set of rules (productions), where the left hand side (the mother) is a single non-terminal and the right hand\\nside is a sequence of one or more non-terminal or terminal symbols (the daughters);\\n4. a start symbol, conventionally S, which is a member of the set of non-terminal symbols.\\n26'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 26}, page_content='The formal description of a CFG generally allows productions with an empty righthandside (e.g., Det !\"). It is\\nconvenient to exclude these however, since they complicate parsing algorithms, and a weakly-equivalent grammar can\\nalways be constructed that disallows such empty productions.\\nA grammar in which all nonterminal daughters are the leftmost daughter in a rule (i.e., where all rules are of the form\\nX !Ya\\x03), is said to be left-associative. A grammar where all the nonterminals are rightmost is right-associative.\\nSuch grammars are weakly-equivalent to regular grammars (i.e., grammars that can be implemented by FSAs), but\\nnatural languages seem to require more expressive power than this (see x4.12).\\n4.3 A simple CFG for a fragment of English\\nThe following tiny fragment is intended to illustrate some of the properties of CFGs so that we can discuss parsing\\nand generation. It has some serious de\\x02ciencies as a representation of even this fragment, which we\\'ll ignore for now,'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 26}, page_content=\"and generation. It has some serious de\\x02ciencies as a representation of even this fragment, which we'll ignore for now,\\nthough we'll discuss some of them in lecture 5.\\nS -> NP VP\\nVP -> VP PP\\nVP -> V\\nVP -> V NP\\nVP -> V VP\\nNP -> NP PP\\nPP -> P NP\\n;;; lexicon\\nV -> can\\nV -> fish\\nNP -> fish\\nNP -> rivers\\nNP -> pools\\nNP -> December\\nNP -> Scotland\\nNP -> it\\nNP -> they\\nP -> in\\nThe rules with terminal symbols on the RHS correspond to the lexicon. Here and below, comments are preceded by\\n;;;\\nHereare some strings which this grammar generates, along with their bracketings:\\nthey \\x02sh\\n(S (NP they) (VP (V \\x02sh)))\\nthey can \\x02sh\\n(S (NP they) (VP (V can) (VP (V \\x02sh))))\\n;;; the modal verb `are able to' reading\\n(S (NP they) (VP (V can) (NP \\x02sh)))\\n;;; the less plausible, put \\x02sh in cans, reading\\nthey \\x02sh in rivers\\n(S (NP they) (VP (VP (V \\x02sh)) (PP (P in) (NP rivers))))\\nthey \\x02sh in rivers in December\\n(S (NP they) (VP (VP (V \\x02sh)) (PP (P in) (NP (NP rivers) (PP (P in) (NP December))))))\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 26}, page_content='they \\x02sh in rivers\\n(S (NP they) (VP (VP (V \\x02sh)) (PP (P in) (NP rivers))))\\nthey \\x02sh in rivers in December\\n(S (NP they) (VP (VP (V \\x02sh)) (PP (P in) (NP (NP rivers) (PP (P in) (NP December))))))\\n;;; i.e. the implausible reading where the rivers are in December\\n;;; (cf rivers in Scotland)\\n(S (NP they) (VP (VP (VP (V \\x02sh)) (PP (P in) (NP (NP rivers)))) (PP (P in) (NP December))))\\n;;; i.e. the \\x02shing is done in December\\n27'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 27}, page_content=\"One important thing to notice about these examples is that there's lots of potential for ambiguity. In the they can \\x02sh\\nexample, this is due to lexical ambiguity (it arises from the dual lexical entries of can and \\x02sh), but the last example\\ndemonstrates purely structural ambiguity. In this case, the ambiguity arises from the two possible attachments of the\\nprepositional phrase (PP) in December: it can attach to the NP (rivers) or to the VP. These attachments correspond\\nto different semantics, as indicated by the glosses. PP attachment ambiguities are a major headache in parsing, since\\nsequences of four or more PPs are common in real texts and the number of readings increases as the Catalan series,\\nwhich is exponential. Other phenomena have similar properties: for instance, compound nouns (e.g. long-stay car\\npark shuttle bus).\\nNotice that \\x02sh could have been entered in the lexicon directly as a VP, but that this would cause problems if we were\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 27}, page_content=\"park shuttle bus).\\nNotice that \\x02sh could have been entered in the lexicon directly as a VP, but that this would cause problems if we were\\ndoing derivational morphology, because we want to say that suf\\x02xes like -ed apply to Vs. Making rivers etc NPs rather\\nthan nouns is a simpli\\x02cation I've adopted here to keep the grammar smaller.\\n4.4 Parse trees\\nParse trees are equivalent to bracketed structures, but are easier to read for complex cases. A parse tree and bracketed\\nstructure for one reading of they can \\x02sh in December is shown below. The correspondence should be obvious.\\nS\\nNP VP\\nthey V VP\\ncan VP PP\\nV\\n\\x02sh\\nP NP\\nin December\\n(S (NP they) (VP (V can) (VP (VP (V \\x02sh)) (PP (P in) (NP December)))))\\n4.5 Using a grammar as a random generator\\nThe following simple algorithm illustrates how a grammar can be used to generate sentences.\\nExpand cat category sentence-record:\\nLet possibilities be a set containing all lexical items which match category and all rules with left-hand side category\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 27}, page_content='Expand cat category sentence-record:\\nLet possibilities be a set containing all lexical items which match category and all rules with left-hand side category\\nIf possibilities is empty,\\nthen fail\\nelse\\nRandomly select a possibility chosen from possibilities\\nIf chosen is lexical,\\nthen append it to sentence-record\\nelse expand cat on each rhs category in chosen (left to right) with the updated sentence-record\\nreturn sentence-record\\nFor instance:\\nExpand cat S ()\\n28'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 28}, page_content=\"possibilities = S -> NP VP\\nchosen = S -> NP VP\\nExpand cat NP ()\\npossibilities = it, they, \\x02sh\\nchosen = \\x02sh\\nsentence-record = (\\x02sh)\\nExpand cat VP (\\x02sh)\\npossibilities = VP -> V, VP -> V VP, VP -> V NP\\nchosen = VP -> V\\nExpand cat V (\\x02sh)\\npossibilities = \\x02sh, can\\nchosen = \\x02sh\\nsentence-record = (\\x02sh \\x02sh)\\nObviously, the strings generated could be arbitrarily long. If in this naive generation algorithm, we explored all the\\nsearch space rather than randomly selecting a possible expansion, the algorithm wouldn't terminate.\\nReal generation operates from semantic representations, which aren't encoded in this grammar, so in what follows\\nwe'll concentrate on describing parsing algorithms instead. However, it's important to realise that CFGs are, in prin-\\nciple, bidirectional.\\n4.6 Chart parsing\\nIn order to parse with reasonable ef\\x02ciency, we need to keep a record of the rules that we have applied so that we don't\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 28}, page_content=\"ciple, bidirectional.\\n4.6 Chart parsing\\nIn order to parse with reasonable ef\\x02ciency, we need to keep a record of the rules that we have applied so that we don't\\nhave to backtrack and redo work that we've done before. This works for parsing with CFGs because the rules are\\nindependent of their context: a VP can always expand as a V and an NP regardless of whether or not it was preceded\\nby an NP or a V, for instance. (In some cases we may be able to apply techniques that look at the context to cut down\\nthe search space, because we can tell that a particular rule application is never going to be part of a sentence, but this is\\nstrictly a \\x02lter: we're never going to get incorrect results by reusing partial structures.) This record keeping strategy is\\nan application of dynamic programming which is used in processing formal languages too. In NLP the data structure\\nused for recording partial results is generally known as a chart and algorithms for parsing using such structures are\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 28}, page_content='used for recording partial results is generally known as a chart and algorithms for parsing using such structures are\\nreferred to as chart parsers.\\nA chart is a list of edges. In the simplest version of chart parsing, each edge records a rule application and has the\\nfollowing structure:\\n[id,left vertex, right vertex,mother category, daughters]\\nA vertex is an integer representing a point in the input string, as illustrated below:\\n. they . can . fish .\\n0 1 2 3\\nmother category refers to the rule that has been applied to create the edge. daughters is a list of the edges that acted\\nas the daughters for this particular rule application: it is there purely for record keeping so that the output of parsing\\ncan be a labelled bracketing.\\nFor instance, the following edges would be among those found on the chart after a complete parse of they can \\x02sh\\naccording to the grammar given above (id numbering is arbitrary):\\nid left right mother daughters\\n3 1 2 V (can)\\n4 2 3 NP (fish)\\n5 2 3 V (fish)'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 28}, page_content='according to the grammar given above (id numbering is arbitrary):\\nid left right mother daughters\\n3 1 2 V (can)\\n4 2 3 NP (fish)\\n5 2 3 V (fish)\\n6 2 3 VP (5)\\n7 1 3 VP (3 5)\\n8 1 3 VP (3 4)\\n29'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 29}, page_content=\"The daughters for the terminal rule applications are simply the input word strings. Note that local ambiguities cor-\\nrespond to situations where a particular span has more than one associated edge. We'll see below that we can pack\\nstructures so that we never have two edges with the same category and the same span, but we'll ignore this for the\\nmoment (see x4.9). Also, in this chart we're only recording complete rule applications: this is passive chart parsing.\\nThe more ef\\x02cient active chart is discussed below, in x4.10.\\n4.7 A bottom-up passive chart parser\\nThe following pseudo-code sketch is for a very simple chart parser. The main function is Add new edge which is\\ncalled for each word in the input going left to right. Add new edge recursively scans backwards looking for other\\ndaughters.\\nParse:\\nInitialise the chart (i.e., clear previous results)\\nFor each word word in the input sentence, let from be the left vertex, to be the right vertex and daughters be (word)\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 29}, page_content='daughters.\\nParse:\\nInitialise the chart (i.e., clear previous results)\\nFor each word word in the input sentence, let from be the left vertex, to be the right vertex and daughters be (word)\\nFor each category category that is lexically associated with word\\nAdd new edge from, to, category, daughters\\nOutput results for all spanning edges\\n(i.e., ones that cover the entire input and which have a mother corresponding to the root category)\\nAdd new edge from, to, category, daughters:\\nPut edge in chart: [id,from,to, category,daughters]\\nFor each rule in the grammar of form lhs ->cat1 . . . catn\\x001,category\\nFind set of lists of contiguous edges [id1,from1,to1, cat1,daughters1] . . . [idn\\x001,fromn\\x001,from, catn\\x001,daughtersn\\x001]\\n(such that to1 = from2 etc)\\nFor each list of edges, Add new edge from1, to, lhs, (id1 . . . id)\\nNotice that this means that the grammar rules are indexed by their rightmost category, and that the edges in the chart'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 29}, page_content='For each list of edges, Add new edge from1, to, lhs, (id1 . . . id)\\nNotice that this means that the grammar rules are indexed by their rightmost category, and that the edges in the chart\\nmust be indexed by their to vertex (because we scan backward from the rightmost category). Consider:\\n. they . can . fish .\\n0 1 2 3\\nThe following diagram shows the chart edges as they are constructed in order (when there is a choice, taking rules in\\na priority order according to the order they appear in the grammar):\\nid left right mother daughters\\n1 0 1 NP (they)\\n2 1 2 V (can)\\n3 1 2 VP (2)\\n4 0 2 S (1 3)\\n5 2 3 V (fish)\\n6 2 3 VP (5)\\n7 1 3 VP (2 6)\\n8 0 3 S (1 7)\\n9 2 3 NP (fish)\\n10 1 3 VP (2 9)\\n11 0 3 S (1 10)\\nThe spanning edges are 11 and 8: the output routine to give bracketed parses simply outputs a left bracket, outputs\\nthe category, recurses through each of the daughters and then outputs a right bracket. So, for instance, the output from\\nedge 11 is:\\n(S (NP they) (VP (V can) (NP fish)))\\n30'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 30}, page_content='4.8 A detailed trace of the simple chart parser\\nParse\\nword = they\\ncategories = NP\\nAdd new edge 0, 1, NP, (they)\\nthey can \\x02sh\\n1\\nMatching grammar rules are:\\nVP -> V NP\\nPP -> P NP\\nNo matching edges corresponding to V or P\\nword = can\\ncategories = V\\nAdd new edge 1, 2, V, (can)\\nthey can \\x02sh\\n1 2\\nMatching grammar rules are:\\nVP -> V\\nset of edge lists = f(2)g\\nAdd new edge 1, 2, VP, (2)\\nthey can \\x02sh\\n1 2\\n3\\nMatching grammar rules are:\\nS -> NP VP\\nVP -> V VP\\nset of edge lists corresponding to NP VP = f(1;3)g\\nAdd new edge 0, 2, S, (1, 3)\\nthey can \\x02sh\\n1 2\\n3\\n4\\nNo matching grammar rules for S\\nNo edges matching V VP\\n31'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 31}, page_content='word = \\x02sh\\ncategories = V, NP\\nAdd new edge 2, 3, V, (\\x02sh)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\nMatching grammar rules are:\\nVP -> V\\nset of edge lists = f(5)g\\nAdd new edge 2, 3, VP, (5)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\n6\\nMatching grammar rules are:\\nS -> NP VP\\nVP -> V VP\\nNo edges match NP\\nset of edge lists for V VP = f(2;6)g\\nAdd new edge 1, 3, VP, (2, 6)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\n6\\n7\\nMatching grammar rules are:\\nS -> NP VP\\nVP -> V VP\\nset of edge lists for NP VP = f(1;7)g\\nAdd new edge 0, 3, S, (1, 7)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\n6\\n7\\n8\\nNo matching grammar rules for S\\nNo edges matching V\\n32'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 32}, page_content='Add new edge 2, 3, NP, (\\x02sh)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\n6\\n7\\n8 9\\nMatching grammar rules are:\\nVP -> V NP\\nPP -> P NP\\nset of edge lists corresponding to V NP = f(2;9)g\\nAdd new edge 1, 3, VP, (2, 9)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\n6\\n7\\n8 9\\n10\\nMatching grammar rules are:\\nS -> NP VP\\nVP -> V VP\\nset of edge lists corresponding to NP VP = f(1;10)g\\nAdd new edge 0, 3, S, (1, 10)\\nthey can \\x02sh\\n1 2\\n3\\n4\\n5\\n6\\n7\\n8 9\\n10\\n11\\nNo matching grammar rules for S\\nNo edges corresponding to V VP\\nNo edges corresponding to P NP\\nNo further words in input\\nSpanning edges are 8 and 11: Output results for 8\\n33'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 33}, page_content=\"(S (NP they) (VP (V can) (VP (V fish))))\\nOutput results for 11\\n(S (NP they) (VP (V can) (NP fish)))\\n4.9 Packing\\nThe algorithm given above is exponential in the case where there are an exponential number of parses. The body\\nof the algorithm can be modi\\x02ed so that it runs in cubic time, though producing the output is still exponential. The\\nmodi\\x02cation is simply to change the daughters value on an edge to be a set of lists of daughters and to make an equality\\ncheck before adding an edge so we don't add one that's equivalent to an existing one. That is, if we are about to add\\nan edge:\\n[id,left vertex, right vertex,mother category, daughters]\\nand there is an existing edge:\\n[id-old,left vertex, right vertex,mother category, daughters-old]\\nwe simply modify the old edge to record the new daughters:\\n[id-old,left vertex, right vertex,mother category, daughters-old tdaughters]\\nThere is no need to recurse with this edge, because we couldn't get any new results.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 33}, page_content=\"[id-old,left vertex, right vertex,mother category, daughters-old tdaughters]\\nThere is no need to recurse with this edge, because we couldn't get any new results.\\nFor the example above, everything proceeds as before up to edge 9:\\nid left right mother daughters\\n1 0 1 NP {(they)}\\n2 1 2 V {(can)}\\n3 1 2 VP {(2)}\\n4 0 2 S {(1 3)}\\n5 2 3 V {(fish)}\\n6 2 3 VP {(5)}\\n7 1 3 VP {(2 6)}\\n8 0 3 S {(1 7)}\\n9 2 3 NP {(fish)}\\nHowever, rather than add edge 10, which would be:\\n10 1 3 VP (2 9)\\nwe match this with edge 7, and simply add the new daughters to that.\\n7 1 3 VP {(2 6), (2 9)}\\nThe algorithm then terminates. We only have one spanning edge (edge 8) but the display routine is more complex\\nbecause we have to consider the alternative sets of daughters for edge 7. (You should go through this to convince\\nyourself that the same results are obtained as before.) Although in this case, the amount of processing saved is small,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 33}, page_content=\"yourself that the same results are obtained as before.) Although in this case, the amount of processing saved is small,\\nthe effects are much more important with longer sentences (consider he believes they can \\x02sh, for instance).\\n4.10 Active chart parsing\\nA more minor ef\\x02ciency improvement is obtained by storing the results of partial rule applications. This is active\\nchart parsing, so called because the partial edges are considered to be active: i.e. they `want' more input to make them\\ncomplete. An active edge records the input it expects as well as the daughters it has already seen. For instance, with\\nan active chart parser, we might have the following edges after seeing they:\\n34\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 34}, page_content=\"id left right mother expected daughters\\n1 0 1 NP {(they)}\\n2 0 1 S VP {(1 ?)}\\nThe daughter marked as ? will be instantiated by the edge corresponding to the VP when it is found.\\n4.11 Ordering the search space\\nIn the pseudo-code above, the order of addition of edges to the chart was determined by the recursion. In general,\\nchart parsers make use of an agenda of edges, so that the next edges to be operated on are the ones that are \\x02rst on the\\nagenda. Different parsing algorithms can be implemented by making this agenda a stack or a queue, for instance.\\nSo far, we've considered bottom up parsing: an alternative is top down parsing, where the initial edges are given by\\nthe rules whose mother corresponds to the start symbol.\\nSome ef\\x02ciency improvements can be obtained by ordering the search space appropriately, though which version is\\nmost ef\\x02cient depends on properties of the individual grammar. However, the most important reason to use an explicit\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 34}, page_content='most ef\\x02cient depends on properties of the individual grammar. However, the most important reason to use an explicit\\nagenda is when we are returning parses in some sort of priority order, corresponding to weights on different grammar\\nrules or lexical entries.\\nWeights can be manually assigned to rules and lexical entries in a manually constructed grammar. However, in\\nthe last decade, a lot of work has been done on automatically acquiring probabilities from a corpus annotated with\\ntrees (a treebank), either as part of a general process of automatic grammar acquisition, or as automatically acquired\\nadditions to a manually constructed grammar. Probabilistic CFGs (PCFGs) can be de\\x02ned quite straightforwardly, if\\nthe assumption is made that the probabilities of rules and lexical entries are independent of one another (of course\\nthis assumption is not correct, but the orderings given seem to work quite well in practice). The importance of this is'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 34}, page_content=\"this assumption is not correct, but the orderings given seem to work quite well in practice). The importance of this is\\nthat we rarely want to return all parses in a real application, but instead we want to return those which are top-ranked:\\ni.e., the most likely parses. This is especially true when we consider that realistic grammars can easily return many\\nthousands of parses for sentences of quite moderate length (20 words or so). If edges are prioritised by probability, very\\nlow priority edges can be completely excluded from consideration if there is a cut-off such that we can be reasonably\\ncertain that no edges with a lower priority than the cut-off will contribute to the highest-ranked parse. Limiting the\\nnumber of analyses under consideration is known as beam search (the analogy is that we're looking within a beam of\\nlight, corresponding to the highest probability edges). Beam search is linear rather than exponential or cubic. Just as\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 34}, page_content=\"light, corresponding to the highest probability edges). Beam search is linear rather than exponential or cubic. Just as\\nimportantly, a good priority ordering from a parser reduces the amount of work that has to be done to \\x02lter the results\\nby whatever system is processing the parser's output.\\n4.12 Why can't we use FSAs to model the syntax of natural languages?\\nIn this lecture, we started using CFGs. This raises the question of why we need this more expressive (and hence\\ncomputationally expensive) formalism, rather than modelling syntax with FSAs. One reason is that the syntax of\\nnatural languages cannot be described by an FSA, even in principle, due to the presence of centre-embedding, i.e.\\nstructures which map to:\\nA!\\x0bA\\x0c\\nand which generate grammars of the form anbn. For instance:\\nthe students the police arrested complained\\nhas a centre-embedded structure. However, humans have dif\\x02culty processing more than two levels of embedding:\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 34}, page_content=\"the students the police arrested complained\\nhas a centre-embedded structure. However, humans have dif\\x02culty processing more than two levels of embedding:\\n? the students the police the journalists criticised arrested complained\\nIf the recursion is \\x02nite (no matter how deep), then the strings of the language can be generated by an FSA. So it's not\\nentirely clear whether formally an FSA might not suf\\x02ce.\\nThere's a fairly extensive discussion of these issues in J&M , but there are two essential points for our purposes:\\n35\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 35}, page_content=\"1. Grammars written using \\x02nite state techniques alone are very highly redundant, which makes them very dif\\x02cult\\nto build and maintain.\\n2. Without internal structure, we can't build up good semantic representations.\\nHence the use of more powerful formalisms: in the next lecture, we'll discuss the inadequacies of simple CFGs from\\na similar perspective.\\nHowever, FSAs are very useful for partial grammars which don't require full recursion. In particular, for information\\nextraction, we need to recognise named entities: e.g. Professor Smith, IBM, 101 Dalmatians, the White House, the\\nAlps and so on. Although NPs are in general recursive (the man who likes the dog which bites postmen), relative\\nclauses are not generally part of named entities. Also the internal structure of the names is unimportant for IE. Hence\\nFSAs can be used, with sequences such as `title surname', `DT0 PNP' etc\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 35}, page_content=\"clauses are not generally part of named entities. Also the internal structure of the names is unimportant for IE. Hence\\nFSAs can be used, with sequences such as `title surname', `DT0 PNP' etc\\nCFGs can be automatically compiled into approximately equivalent FSAs by putting bounds on the recursion. This is\\nparticularly important in speech recognition engines.\\n4.13 Further reading\\nThis lecture has covered material which J&M discuss in chapters 9 and 10, though we also touched on PCFGs (covered\\nin their chapter 12) and issues of language complexity which they discuss in chapter 13. J&M's discussion covers the\\nEarley algorithm, which can be thought of as a form of active top-down chart parsing. I chose to concentrate on\\nbottom-up parsing in this lecture, mainly because I \\x02nd it easier to describe, but also because it is easier to see how to\\nextend this to PCFGs. Bottom-up parsing also seems to have better practical performance with the sort of grammars\\nwe'll look at in lecture 5.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 35}, page_content=\"extend this to PCFGs. Bottom-up parsing also seems to have better practical performance with the sort of grammars\\nwe'll look at in lecture 5.\\nThere are a large number of introductory linguistics textbooks which cover elementary syntax and discuss concepts\\nsuch as constituency. For instance, students could usefully look at the \\x02rst \\x02ve chapters of Tallerman (1998):\\nTallerman, Maggie, Understanding Syntax, Arnold, London, 1998\\nAn alternative would be the \\x02rst two chapters of Sag and Wasow (1999) \\x97 copies should be in the Computer Lab-\\noratory library. This has a narrower focus than most other syntax books, but covers a much more detailed grammar\\nfragment. The later chapters (particularly 3 and 4) are relevant for lecture 5.\\nSag, Ivan A. and Thomas Wasow, Syntactic Theory \\x97 a formal introduction, CSLI Publications, Stanford, CA, USA,\\n1999\\n36\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 36}, page_content=\"5 Lecture 5: Parsing with constraint-based grammars\\nThe CFG approach which we've looked at so far has some serious de\\x02ciencies as a model of natural language. In this\\nlecture, I'll discuss some of these and give an introduction to a more expressive formalism which is widely used in\\nNLP, again with the help of a sample grammar. In the \\x02rst part of the next lecture, I will also sketch how we can use\\nthis approach to do compositional semantics.\\n5.1 De\\x02ciencies in atomic category CFGs\\nIf we consider the grammar we saw in the last lecture, several problems are apparent. One is that there is no account\\nof agreement, so, for instance, *it \\x02sh is allowed by the grammar as well as they \\x02sh.13\\nWe could, of course, allow for agreement by increasing the number of atomic symbols in the CFG, introducing NP-sg,\\nNP-pl, VP-sg and VP-pl, for instance. But this approach would soon become very tedious:\\nS -> NP-sg VP-sg\\nS -> NP-pl VP-pl\\nVP-sg -> V-sg NP-sg\\nVP-sg -> V-sg NP-pl\\nVP-pl -> V-pl NP-sg\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 36}, page_content=\"NP-pl, VP-sg and VP-pl, for instance. But this approach would soon become very tedious:\\nS -> NP-sg VP-sg\\nS -> NP-pl VP-pl\\nVP-sg -> V-sg NP-sg\\nVP-sg -> V-sg NP-pl\\nVP-pl -> V-pl NP-sg\\nVP-pl -> V-pl NP-pl\\nNote that we have to expand out the symbols even when there's no constraint on agreement, since we have no way of\\nsaying that we don't care about the value of number for a category.\\nAnother linguistic phenomenon that we are failing to deal with is subcategorization. This is the lexical property that\\ntells us how many arguments a verb can have (among other things). A verb such as adore, for instance, is transitive: a\\nsentence such as *Kim adored is strange, while Kim adored Sandy is usual. A verb such as give is ditransitive: Kim\\ngave Sandy an apple (or Kim gave an apple to Sandy). Without going into details of exactly how subcategorization is\\nde\\x02ned, or what an argument is, it should be intuitively obvious that we're not encoding this property with our CFG.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 36}, page_content=\"de\\x02ned, or what an argument is, it should be intuitively obvious that we're not encoding this property with our CFG.\\nThe grammar allows the following, for instance:\\nthey \\x02sh \\x02sh it\\n(S (NP they) (VP (V \\x02sh) (VP (V \\x02sh) (NP it))))\\nAgain this could be dealt with by multiplying out symbols (V-intrans, V-ditrans etc), but the grammar becomes ex-\\ntremely cumbersome.\\nFinally, consider the phenomenon of long-distance dependencies, exempli\\x02ed, for instance, by:\\nwhich problem did you say you don't understand?\\nwho do you think Kim asked Sandy to hit?\\nwhich kids did you say were making all that noise?\\nTraditionally, these sentences are said to contain `gap's, corresponding to the place where the noun phrase would\\nnormally appear: the gaps are marked by underscores below:\\nwhich problem did you say you don't understand ?\\nwho do you think Kim asked Sandy to hit?\\nwhich kids did you say were making all that noise?\\nNotice that, in the third example, the verb were shows plural agreement.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 36}, page_content='who do you think Kim asked Sandy to hit?\\nwhich kids did you say were making all that noise?\\nNotice that, in the third example, the verb were shows plural agreement.\\nDoing this in standard CFGs is possible, but extremely verbose, potentially leading to millions of rules. Instead of\\nhaving simple atomic categories in the CFG, we want to allow for features on the categories, which can have values\\nindicating things like plurality. As the long-distance dependency examples should indicate, the features need to be\\ncomplex-valued. For instance,\\n13There was also no account of case: this is only re\\x03ected in a few places in modern English, but *they can they is clearly ungrammatical (as\\nopposed to they can them, which is grammatical with the transitive verb use of can).\\n37'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 37}, page_content=\"* what kid did you say were making all that noise?\\nis not grammatical. The analysis needs to be able to represent the information that the gap corresponds to a plural\\nnoun phrase.\\nIn what follows, I will illustrate a simple constraint-based grammar formalism, using feature structures. A constraint-\\nbased grammar describes a language using a set of independently stated constraints, without imposing any conditions\\non processing or processing order. A CFG can be taken as an example of a constraint-based grammar, but usually the\\nterm is reserved for richer formalisms. The simplest way to think of feature structures (FSs) is that we're replacing the\\natomic categories of a CFG with more complex data structures. I'll \\x02rst illustrate this idea intuitively, using a grammar\\nfragment like the one in lecture 4 but enforcing agreement. I'll then go through the feature structure formalism in more\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 37}, page_content=\"fragment like the one in lecture 4 but enforcing agreement. I'll then go through the feature structure formalism in more\\ndetail. This is followed by an example of a more complex grammar, which allows for subcategorization (I won't show\\nhow case and long-distance dependencies are dealt with).\\n5.2 A very simple FS grammar encoding agreement\\nIn a FS grammar, rules are described as relating FSs: i.e., lexical entries and phrases are FSs. In these formalisms,\\nthe term sign is often used to refer to lexical entries and phrases collectively. In fact, rules themselves can be treated\\nas FSs. Feature structures are singly-rooted directed acyclic graphs, with arcs labelled by features and terminal nodes\\nassociated with values. A particular feature in a structure may be atomic-valued, meaning it points to a terminal node\\nin the graph, or complex-valued, meaning it points to a non-terminal node. A sequence of features is known as a path.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 37}, page_content='in the graph, or complex-valued, meaning it points to a non-terminal node. A sequence of features is known as a path.\\nFor instance, in the structure below, there are two arcs, labelled with CAT and AGR, and three nodes, with the two\\nterminal nodes having values noun and sg. Each of the features is thus atomic-valued.\\nCAT -noun\\nAGR\\njsg\\nIn the graph below, the feature HEAD is complex-valued, and the value of AGR (i.e., the value of the path HEAD AGR)\\nis unspeci\\x02ed:\\nHEAD - CAT -noun\\nAGR\\nj\\nFSs are usually drawn as attribute-value matrices or AVMs. The AVMs corresponding to the two FSs above are as\\nfollows:\\n\\x14\\nCAT noun\\nAGR sg\\n\\x15\\n\"\\nHEAD\\n\\x14\\nCAT noun\\nAGR\\n\\x02 \\x03\\n\\x15#\\nSince FSs are graphs, rather than trees, a particular node may be accessed from the root by more than one path: this is\\nknown as reentrancy. In AVMs, reentrancy is conventionally indicated by boxed integers, with node identity indicated'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 37}, page_content='known as reentrancy. In AVMs, reentrancy is conventionally indicated by boxed integers, with node identity indicated\\nby integer identity. The actual integers used are arbitrary. This is illustrated with an abstract example using features F\\nand G below:\\n38'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 38}, page_content=\"Graph AVM\\nNon-reentrant\\naF :\\nG - a\\n\\x14\\nF a\\nG a\\n\\x15\\nReentrant\\nF\\nzG - a\\n\\x14\\nF 0 a\\nG 0\\n\\x15\\nWhen using FSs in grammars, structures are combined by uni\\x02cation. This means that all the information in the two\\nstructures is combined. The empty square brackets (\\x02 \\x03) in an AVM indicate that a value is unspeci\\x02ed: i.e. this is a\\nnode which can be uni\\x02ed with a terminal node (i.e., an atomic value) or a complex value. More details of uni\\x02cation\\nare given below.\\nWhen FSs are used in a particular grammar, all signs will have a similar set of features (although sometimes there\\nare differences between lexical and phrasal signs). Feature structure grammars can be used to implement a variety of\\nlinguistic frameworks. For the \\x02rst example of a FS grammar, we'll just consider how agreement could be encoded.\\nSuppose we are trying to model a grammar which is weakly equivalent to the CFG fragment below:\\nS -> NP-sg VP-sg\\nS -> NP-pl VP-pl\\nVP-sg -> V-sg NP-sg\\nVP-sg -> V-sg NP-pl\\nVP-pl -> V-pl NP-sg\\nVP-pl -> V-pl NP-pl\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 38}, page_content='S -> NP-sg VP-sg\\nS -> NP-pl VP-pl\\nVP-sg -> V-sg NP-sg\\nVP-sg -> V-sg NP-pl\\nVP-pl -> V-pl NP-sg\\nVP-pl -> V-pl NP-pl\\nV-pl -> like\\nV-sg -> likes\\nNP-sg -> it\\nNP-pl -> they\\nNP-sg -> fish\\nNP-pl -> fish\\nThe FS equivalent shown below splits up the categories so that the main category and the agreement values are distinct.\\nIn the grammar below, I have used the arrow notation for rules as an abbreviation: I will describe the actual FS encoding\\nof rules shortly. The FS grammar just needs two rules. There is a single rule corresponding to the S-> NP VPrule,\\nwhich enforces identity of agreement values between the NP and the VP by means of reentrancy (indicated by the tag\\n1 ). The rule corresponding to VP-> V NP simply makes the agreement values of the V and the VP the same but\\nignores the agreement value on the NP.14 The lexicon speci\\x02es agreement values for it, they, like and likes, but leaves'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 38}, page_content='ignores the agreement value on the NP.14 The lexicon speci\\x02es agreement values for it, they, like and likes, but leaves\\nthe agreement value for \\x02sh uninstantiated (i.e., underspeci\\x02ed). Note that the grammar also has a root FS: a structure\\nonly counts as a valid parse if it is uni\\x02able with the root.\\nFS grammar fragment encoding agreement\\nGrammar rules\\nRule1\\n\\x14\\nCAT S\\nAGR 1\\n\\x15\\n!\\n\\x14\\nCAT NP\\nAGR 1\\n\\x15\\n,\\n\\x14\\nCAT VP\\nAGR 1\\n\\x15\\nRule2\\n\\x14\\nCAT VP\\nAGR 1\\n\\x15\\n!\\n\\x14\\nCAT V\\nAGR 1\\n\\x15\\n,\\n\\x14\\nCAT NP\\nAGR\\n\\x02 \\x03\\n\\x15\\nLexicon:\\n;;; noun phrases\\nthey\\n\\x14\\nCAT noun\\nAGR pl\\n\\x15\\n14Note that the reentrancy indicators are local to each rule: the 1 in rule 1 is not the same structure as the 1 in rule 2.\\n39'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 39}, page_content='\\x02sh\\n\"\\x14\\nCAT noun\\nAGR\\n\\x02 \\x03\\n\\x15#\\nit\\n\\x14\\nCAT noun\\nAGR sg\\n\\x15\\n;;; verbs\\nlike\\n\\x14\\nCAT verb\\nAGR pl\\n\\x15\\nlikes\\n\\x14\\nCAT verb\\nAGR sg\\n\\x15\\nRoot structure:\\x02\\nCAT S\\n\\x03\\nConsider parsing they like it with this grammar. The lexical structures for like and it are uni\\x02ed with the corresponding\\nstructure to the right hand side of rule 2. Both uni\\x02cations succeed, and the structure corresponding to the mother of\\nthe rule is:\\n\\x14\\nCAT VP\\nAGR pl\\n\\x15\\nThe agreement value is pl because of the coindexation with the agreement value of like. This structure can unify\\nwith the rightmost daughter of rule 1. The structure for they is uni\\x02ed with the leftmost daughter. Rule 1 says that\\nboth daughters have to have the same agreement value, which is the case in this example. Rule application therefore\\nsucceeds and since the result uni\\x02es with the rule structure, there is a valid parse.\\nTo see what is going on a bit more precisely, we need to show the rules as FSs. There are several ways of encoding'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 39}, page_content='To see what is going on a bit more precisely, we need to show the rules as FSs. There are several ways of encoding\\nthis, but for current purposes I will assume that rules have features MOTHER, DTR1, DTR2 . . . DTRN. So Rule2, which\\nI informally wrote as:\\n\\x14\\nCAT VP\\nAGR 1\\n\\x15\\n!\\n\\x14\\nCAT V\\nAGR 1\\n\\x15\\n,\\n\\x14\\nCAT NP\\nAGR\\n\\x02 \\x03\\n\\x15\\nis actually:\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT VP\\nAGR\\n1\\n\\x15\\nDTR1\\n\\x14\\nCAT V\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT NP\\nAGR\\n\\x02 \\x03\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nThe structure for like can be uni\\x02ed with the value of DTR1 in the rule. Uni\\x02cation means all information is retained,\\nso the result includes the agreement value from like:\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT VP\\nAGR\\n1 pl\\n\\x15\\nDTR1\\n\\x14\\nCAT V\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT NP\\nAGR\\n\\x02 \\x03\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nThe structure for it is uni\\x02ed with the value for DTR2:\\n40'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 40}, page_content='2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT VP\\nAGR\\n1 pl\\n\\x15\\nDTR1\\n\\x14\\nCAT V\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT NP\\nAGR sg\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nThe rule application thus succeeds. The MOTHER value acts as the DTR2 of Rule 1. That is:\\n\\x14\\nCAT VP\\nAGR pl\\n\\x15\\nis uni\\x02ed with the DTR2 value of:\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT S\\nAGR\\n1\\n\\x15\\nDTR1\\n\\x14\\nCAT NP\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT VP\\nAGR 1\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nThis gives:\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT S\\nAGR\\n1 pl\\n\\x15\\nDTR1\\n\\x14\\nCAT NP\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT VP\\nAGR 1\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nThe FS for they is:\\n\\x14\\nCAT NP\\nAGR pl\\n\\x15\\nThe uni\\x02cation of this with the value of DTR1 succeeds but adds no new information:\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT S\\nAGR\\n1 pl\\n\\x15\\nDTR1\\n\\x14\\nCAT NP\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT VP\\nAGR 1\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nSimilarly, this structure uni\\x02es with the root structure, so this is a valid parse.\\nNote however, that if we had tried to parse it like it, a uni\\x02cation failure would have occurred, since the AGR on the\\nlexical entry for it has the value sg which clashes with the value pl.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 40}, page_content='Note however, that if we had tried to parse it like it, a uni\\x02cation failure would have occurred, since the AGR on the\\nlexical entry for it has the value sg which clashes with the value pl.\\nI have described these uni\\x02cations as occurring in a particular order, but it is very important to note that order is not\\nsigni\\x02cant and that the same overall result would have been obtained if another order had been used. This means that\\ndifferent parsing algorithms are guaranteed to give the same result. The one proviso is that with some FS grammars,\\njust like CFGs, some algorithms may terminate while others do not.\\n5.3 Feature structures in detail\\nSo far, I have been using a rather informal description of FSs. The following section gives more formal de\\x02nitions.\\n41'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 41}, page_content='FSs can be thought of as graphs which have labelled arcs connecting nodes (except for the case of the simplest FSs,\\nwhich consist of a single node with no arcs) The labels on the arcs are the features. Arcs are regarded as having a\\ndirection, conventionally regarded as pointing into the structure, away from the single root node. The set of features\\nand the set of atomic values are assumed to be \\x02nite.\\nProperties of FSs\\nConnectedness and unique root A FS must have a unique root node: apart from the root node, all nodes have one or\\nmore parent nodes.\\nUnique features Any node may have zero or more arcs leading out of it, but the label on each (that is, the feature)\\nmust be unique.\\nNo cycles No node may have an arc that points back to the root node or to a node that intervenes between it and the\\nroot node. (Some variants of FS formalisms allow cycles.)\\nValues A node which does not have any arcs leading out of it may have an associated atomic value.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 41}, page_content='root node. (Some variants of FS formalisms allow cycles.)\\nValues A node which does not have any arcs leading out of it may have an associated atomic value.\\nFiniteness An FS must have a \\x02nite number of nodes.\\nSequences of features are known as paths.\\nFeature structures can be regarded as being ordered by information content \\x97 an FS is said to subsume another if the\\nlatter carries extra information. This is important because we de\\x02ne uni\\x02cation in terms of subsumption.\\nProperties of subsumption FS1 subsumes FS2 if and only if the following conditions hold:\\nPath values For every path P in FS1 there is a path P in FS2. If P has a value t in FS1, then P also has value t in FS2.\\nPath equivalences Every pair of paths P and Q which are reentrant in FS1 (i.e., which lead to the same node in the\\ngraph) are also reentrant in FS2.\\nUni\\x02cation corresponds to conjunction of information, and thus can be de\\x02ned in terms of subsumption, which is a'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 41}, page_content=\"graph) are also reentrant in FS2.\\nUni\\x02cation corresponds to conjunction of information, and thus can be de\\x02ned in terms of subsumption, which is a\\nrelation of information containment. The uni\\x02cation of two FSs is de\\x02ned to be the most general FS which contains\\nall the information in both of the FSs. Uni\\x02cation will fail if the two FSs contain con\\x03icting information. As we saw\\nwith the simple grammar above, this prevented it like it getting an analysis, because the AGR values con\\x03icted.\\nProperties of uni\\x02cation The uni\\x02cation of two FSs, FS1 and FS2, is the most general FS which is subsumed by both\\nFS1 and FS2, if it exists.\\n5.4 A grammar enforcing subcategorization\\nAlthough the grammar shown above improves on the simple CFG, it still doesn't encode subcategorization. The\\ngrammar shown overleaf does this. It moves further away from the CFG. In particular, in the previous grammar the cat\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 41}, page_content='grammar shown overleaf does this. It moves further away from the CFG. In particular, in the previous grammar the cat\\nfeature encoded both the part-of-speech (i.e., noun or verb) and the distinction between the lexical sign and the phrase\\n(i.e., N vs NP and V vs VP). In the grammar below, the CAT feature just encodes the major category (noun vs verb) and\\nthe phrasal distinction is encoded in terms of whether the subcategorization requirements have been satis\\x02ed. The CAT\\nand AGR features are now inside another feature head. Signs have three features at the top-level: HEAD, COMP and\\nSPR. This re\\x03ects a portion of a linguistic framework which is described in great detail in Sag and Wasow (1999).15\\nBrie\\x03y, HEAD contains information which is shared between the lexical entries and phrases of the same category:\\ne.g., nouns share this information with the noun phrase which dominates them in the tree, while verbs share head'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 41}, page_content=\"e.g., nouns share this information with the noun phrase which dominates them in the tree, while verbs share head\\ninformation with verb phrases and sentences. So HEAD is used for agreement information and for category information\\n(i.e. noun, verb etc). In contrast, COMP and SPR are about subcategorization: they contain information about what\\ncan combine with this sign. In the grammar below, the speci\\x02er is the subject of a verb, but in a larger grammar a\\ndeterminer would be the speci\\x02er of a noun. For instance, an intransitive verb will have a SPR corresponding to its\\nsubject `slot' and a value of \\x02lled for its COMP.16\\n15You aren't expected to know any details of the linguistic framework for the exam, and you do not have to remember the feature structure\\narchitecture described here. The point of giving this more complicated grammar is that it starts to demonstrate the power of the feature structure\\nframework, in a way that the simple grammar using agreement does not.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 41}, page_content=\"framework, in a way that the simple grammar using agreement does not.\\n16There's a more elegant way of doing this using lists, but since this complicates the grammar quite a lot, I won't show this here.\\n42\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 42}, page_content=\"The grammar below has just two rules, one for combining a sign with its complement, another for combining a sign\\nwith its speci\\x02er. Rule 1 says that, when building the phrase, the COMP value of the \\x02rst daughter is to be equated\\n(uni\\x02ed) with the whole structure of the second daughter (indicated by 2 ). The head of the mother is equated with\\nthe head of the \\x02rst daughter ( 1 ). The SPR of the mother is also equated with the SPR of the \\x02rst daughter ( 3 ). The\\nCOMP value of the mother is stipulated as being \\x02lled: this means the mother can't act as the \\x02rst daughter in another\\napplication of the rule, since \\x02lled won't unify with a complex feature structure. The speci\\x02er rule is fairly similar, in\\nthat a SPR `slot' is being instantiated, although in this case it's the second daughter that contains the slot and is sharing\\nits head information with the mother. The rule also stipulates that the AGR values of the two daughters have to be\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 42}, page_content='its head information with the mother. The rule also stipulates that the AGR values of the two daughters have to be\\nuni\\x02ed and that the speci\\x02er daughter has to have a \\x02lled complement. These rules are controlled by the lexical entries\\nin the sense that it\\'s the lexical entries which determine the required complements and speci\\x02er of a word.\\nAs an example, consider analysing they \\x02sh. The verb entry for \\x02sh can be uni\\x02ed with the second daughter position\\nof rule 2, giving the following partially instantiated rule:2\\n6\\n4\\nHEAD\\n1\\n\\x14\\nCAT verb\\nAGR 3 pl\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5!\\n2\\n2\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR\\n3\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5,\\n\"\\nHEAD\\n1\\nCOMP \\x02lled\\nSPR 2\\n#\\nThe \\x02rst daughter of this result can be uni\\x02ed with the structure for they, which in this case returns the same structure,\\nsince it adds no new information. The result can be uni\\x02ed with the root structure, so this is a valid parse.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 42}, page_content='since it adds no new information. The result can be uni\\x02ed with the root structure, so this is a valid parse.\\nOn the other hand, the lexical entry for the noun \\x02sh does not unify with the second daughter position of rule2. The\\nentry for they does not unify with the \\x02rst daughter position of rule1. Hence there is no other parse.\\nSimple FS grammar fragment encoding subcategorization\\nRule1 ;;; comp \\x02lling\"\\nHEAD 1\\nCOMP \\x02lled\\nSPR 3\\n#\\n!\\n\"\\nHEAD 1\\nCOMP 2\\nSPR 3\\n#\\n, 2\\n\\x02\\nCOMP \\x02lled\\n\\x03\\nRule2 ;;; spr \\x02lling:\"\\nHEAD 1\\nCOMP \\x02lled\\nSPR \\x02lled\\n#\\n! 2\\n2\\n4\\nHEAD\\n\\x02\\nAGR\\n3\\n\\x03\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n5,\\n2\\n4\\nHEAD\\n1\\n\\x02\\nAGR 3\\n\\x03\\nCOMP \\x02lled\\nSPR 2\\n3\\n5\\nLexicon:\\n;;; noun phrases\\nthey\\n2\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR pl\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5\\n\\x02sh\\n2\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR\\n\\x02 \\x03\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5\\nit\\n2\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR sg\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5\\n;;; verbs\\n\\x02sh\\n2\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT verb\\nAGR pl\\n\\x15\\nCOMP \\x02lled\\nSPR\\nh\\nHEAD\\n\\x02\\nCAT noun\\n\\x03i\\n3\\n7\\n7\\n7\\n5\\n43'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 43}, page_content=\"can\\n2\\n6\\n6\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT verb\\nAGR\\n\\x02 \\x03\\n\\x15\\nCOMP\\nh\\nHEAD\\n\\x02\\nCAT verb\\n\\x03i\\nSPR\\nh\\nHEAD\\n\\x02\\nCAT noun\\n\\x03i\\n3\\n7\\n7\\n7\\n7\\n7\\n5\\n;;; auxiliary verb\\ncan\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT verb\\nAGR pl\\n\\x15\\nCOMP\\n\\x14\\nHEAD\\n\\x02\\nCAT noun\\n\\x03\\nCOMP \\x02lled\\n\\x15\\nSPR\\nh\\nHEAD\\n\\x02\\nCAT noun\\n\\x03i\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\n;;; transitive verb\\nRoot structure:2\\n4\\nHEAD\\n\\x02\\nCAT verb\\n\\x03\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n5\\n5.5 Parsing with feature structure grammars\\nFormally we can treat feature structure grammars in terms of subsumption. I won't give details here, but the intuition\\nis that the rule FSs, the lexical entry FSs and the root FS all act as constraints on the parse, which have to be sat-\\nis\\x02ed simultaneously. This means the system has to build a parse structure which is subsumed by all the applicable\\nconstraints. However, this description of what it means for something to be a valid parse doesn't give any hint of a\\nsensible algorithm.\\nThe standard approach to implementation is to use chart parsing, as described in the previous lecture, but the notion\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 43}, page_content=\"sensible algorithm.\\nThe standard approach to implementation is to use chart parsing, as described in the previous lecture, but the notion\\nof a grammar rule matching an edge in the chart is more complex. In a naive implementation, when application of a\\ngrammar rule is checked, all the feature structures in the edges in the chart that correspond to the possible daughters\\nhave to be copied, and the grammar rule feature structure itself is also copied. The copied daughter structures are\\nuni\\x02ed with the daughter positions in the copy of the rule, and if uni\\x02cation succeeds, the copied structure is associated\\nwith a new edge on the chart.\\nThe need for copying is often discussed in terms of the destructive nature of the standard algorithm for uni\\x02cation\\n(which I won't describe here), but this is perhaps a little misleading. Uni\\x02cation, however implemented, involves\\nsharing information between structures. Assume, for instance, that the FS representing the lexical entry of the noun\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 43}, page_content=\"sharing information between structures. Assume, for instance, that the FS representing the lexical entry of the noun\\nfor \\x02sh is underspeci\\x02ed for number agreement. When we parse a sentence like:\\nthe \\x02sh swims\\nthe part of the FS in the result that corresponds to the original lexical entry will have its AGR value instantiated. This\\nmeans that the structure corresponding to a particular edge cannot be reused in another analysis, because it will contain\\n`extra' information. Consider, for instance, parsing:\\nthe \\x02sh in the lake which is near the town swim\\nA possible analysis of:\\n\\x02sh in the lake which is near the town\\nis:\\n(\\x02sh (in the lake) (which is near the town))\\ni.e., the \\x02sh (sg) is near the town. If we instantiate the AGR value in the FS for \\x02sh as sg while constructing this parse,\\nand then try to reuse that same FS for \\x02sh in the other parses, analysis will fail. Hence the need for copying, so we can\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 43}, page_content='and then try to reuse that same FS for \\x02sh in the other parses, analysis will fail. Hence the need for copying, so we can\\nuse a fresh structure each time. Copying is potentially extremely expensive, because realistic grammars involve FSs\\nwith many hundreds of nodes.\\n44'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 44}, page_content=\"So, although uni\\x02cation is very near to linear in complexity, naive implementations of FS formalisms are very in-\\nef\\x02cient. Furthermore, packing is not straightforward, because two structures are rarely identical in real grammars\\n(especially ones that encode semantics).\\nReasonably ef\\x02cient implementations of FS formalisms can nevertheless be developed. Copying can be greatly re-\\nduced:\\n1. by doing an ef\\x02cient pretest before uni\\x02cation, so that copies are only made when uni\\x02cation is likely to succeed\\n2. by sharing parts of FSs that aren't changed\\n3. by taking advantage of locality principles in linguistic formalisms which limit the need to percolate information\\nthrough structures\\nPacking can also be implemented: the test to see if a new edge can be packed involves subsumption rather than equality.\\nAs with CFGs, for real ef\\x02ciency we need to control the search space so we only get the most likely analyses. De\\x02ning\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 44}, page_content='As with CFGs, for real ef\\x02ciency we need to control the search space so we only get the most likely analyses. De\\x02ning\\nprobabilistic FS grammars in a way which is theoretically well-motivated is much more dif\\x02cult than de\\x02ning a PCFG.\\nPractically it seems to turn out that treating a FS grammar much as though it were a CFG works fairly well, but this is\\nan active research issue.\\n5.6 Templates\\nThe lexicon outlined above has the potential to be very redundant. For instance, as well as the intransitive verb \\x02sh,\\na full lexicon would have entries for sleep, snore and so on, which would be essentially identical. We avoid this\\nredundancy by associating names with particular feature structures and using those names in lexical entries. For\\ninstance:\\n\\x02shINTRANS\\nVERB\\nsleep INTRANS VERB\\nsnore INTRANS VERB\\nwhere the template is speci\\x02ed as:\\nINTRANS VERB\\n2\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT verb\\nAGR pl\\n\\x15\\nCOMP \\x02lled\\nSPR\\nh\\nHEAD\\n\\x02\\nCAT noun\\n\\x03i\\n3\\n7\\n7\\n7\\n5'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 44}, page_content='instance:\\n\\x02shINTRANS\\nVERB\\nsleep INTRANS VERB\\nsnore INTRANS VERB\\nwhere the template is speci\\x02ed as:\\nINTRANS VERB\\n2\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT verb\\nAGR pl\\n\\x15\\nCOMP \\x02lled\\nSPR\\nh\\nHEAD\\n\\x02\\nCAT noun\\n\\x03i\\n3\\n7\\n7\\n7\\n5\\nThe lexical entry may have some speci\\x02c information associated with it (e.g., semantic information, see next lecture)\\nwhich will be expressed as a FS: in this case, the template and the lexical feature structure are combined by uni\\x02cation.\\n5.7 Interface to morphology\\nSo far we have assumed a full-form lexicon, but we can now return to the approach to morphology that we saw in\\nlecture 2, and show how this relates to feature structures. Recall that we have spelling rules which can be used to\\nanalyse a word form to return a stem and list of af\\x02xes and that each af\\x02x is associated with an encoding of the\\ninformation it contributes. For instance, the af\\x02x s is associated with the template PLURAL_NOUN, which would\\ncorrespond to the following information in our grammar fragment:\\n\"\\nHEAD\\n\\x14\\nCAT noun'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 44}, page_content='information it contributes. For instance, the af\\x02x s is associated with the template PLURAL_NOUN, which would\\ncorrespond to the following information in our grammar fragment:\\n\"\\nHEAD\\n\\x14\\nCAT noun\\nAGR pl\\n\\x15#\\nA stem for a noun is generally assumed to be uninstantiated for number (i.e., neutral between sg and pl). So the lexical\\nentry for the noun dog in our fragment would be:\\n2\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR\\n\\x02 \\x03\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5\\n45'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 45}, page_content=\"One simple way of implementing in\\x03ectional morphology in FSs is simply to unify the contribution of the af\\x02x with\\nthat of the stem. If we unify the FS corresponding to the stem for dog to the FS for PLURAL_NOUN, we get:\\n2\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR pl\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\n3\\n7\\n5\\nThis approach assumes that we also have a template SINGULAR_NOUN, where this is associated with a `null' af\\x02x.\\nIn the case of an example such as feed incorrectly analysed as fee -ed, discussed in x2.5, the af\\x02x information will fail\\nto unify with the stem, ruling out that analysis.\\nThere are other ways of encoding in\\x03ectional morphology with FS, which I won't discuss here. Note that this simple\\napproach is not, in general, adequate for derivational morphology. For instance, the af\\x02x -ize, which combines with\\na noun to form a verb (e.g., lemmatization), cannot be represented simply by uni\\x02cation, because it has to change a\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 45}, page_content=\"a noun to form a verb (e.g., lemmatization), cannot be represented simply by uni\\x02cation, because it has to change a\\nnominal form into a verbal one. This can be implemented by some form of lexical rule (which are essentially grammar\\nrules with single daughters), but I won't discuss this in this course. Note, however, that this re\\x03ects the distinction\\nbetween in\\x03ectional and derivational morphology that we saw in x2.2:while in\\x03ectional morphology can be seen as\\nsimple addition of information, derivational morphology converts feature structures into new structures. However,\\nderivational morphology is often not treated as productive, especially in limited domain systems.\\n5.8 Further reading\\nJ&M describe feature structures as augmenting a CFG rather than replacing it, but most of their discussion applies\\nequally to the FS formalism I've outlined here.\\nLinGO (Linguistic Grammars Online: http://lingo.stanford.edu) distributes Open Source FS grammars for a variety\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 45}, page_content=\"equally to the FS formalism I've outlined here.\\nLinGO (Linguistic Grammars Online: http://lingo.stanford.edu) distributes Open Source FS grammars for a variety\\nof languages. The LinGO English Resource Grammar (ERG) is probably the largest freely available bidirectional\\ngrammar.\\n46\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 46}, page_content='6 Lecture 6: Compositional and lexical semantics\\nThis lecture will give a rather super\\x02cial account of semantics and some of its computational aspects:\\n1. Compositional semantics in feature structure grammars\\n2. Meaning postulates\\n3. Classical lexical relations: hyponymy, meronymy, synonymy, antonymy\\n4. Taxonomies and WordNet\\n5. Classes of polysemy: homonymy, regular polysemy, vagueness\\n6. Word sense disambiguation\\n6.1 Simple semantics in feature structures\\nThe grammar fragment below is based on the one in the previous lecture. It is intended as a rough indication of how\\nit is possible to build up semantic representations using feature structures. The lexical entries have been augmented\\nwith pieces of feature structure re\\x03ecting predicate-argument structure. With this grammar, the FS for they can \\x02sh\\nwill have a SEM value of:2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nPRED and\\nARG1\\n\\x14\\nPRED pron\\nARG1\\n1\\n\\x15\\nARG2\\n2\\n6\\n6\\n6\\n6\\n6\\n4\\nPRED and\\nARG1\\n\"\\nPRED can\\nv\\nARG1 1\\nARG2 2\\n#\\nARG2\\n\\x14\\nPRED \\x02sh n\\nARG1 2\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n5'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 46}, page_content='will have a SEM value of:2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nPRED and\\nARG1\\n\\x14\\nPRED pron\\nARG1\\n1\\n\\x15\\nARG2\\n2\\n6\\n6\\n6\\n6\\n6\\n4\\nPRED and\\nARG1\\n\"\\nPRED can\\nv\\nARG1 1\\nARG2 2\\n#\\nARG2\\n\\x14\\nPRED \\x02sh n\\nARG1 2\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n5\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nThis can be taken to be equivalent to the logical expression pron(x) ^(can\\nv(x;y) ^\\x02sh n(y)) by translating the\\nreentrancy between argument positions into variable equivalence.\\nThe most important thing to notice is how the syntactic argument positions in the lexical entries are linked to their\\nsemantic argument positions. This means, for instance, that for the transitive verb can, the syntactic subject will always\\ncorrespond to the \\x02rst argument position, while the syntactic object will correspond to the second position.\\nSimple FS grammar with crude semantic composition\\nRule1 ;;; comp \\x02lling2\\n6\\n6\\n6\\n4\\nHEAD\\n1\\nCOMP \\x02lled\\nSPR 3\\nSEM\\n\"\\nPRED and\\nARG1 4\\nARG2 5\\n#\\n3\\n7\\n7\\n7\\n5\\n!\\n2\\n4\\nHEAD\\n1\\nCOMP 2\\nSPR 3\\nSEM 4\\n3\\n5,\\n2\\n\\x14\\nCOMP \\x02lled\\nSEM 5\\n\\x15\\nRule2 ;;; spr \\x02lling:2\\n6\\n6\\n6\\n4\\nHEAD\\n1\\nCOMP \\x02lled'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 46}, page_content='6\\n6\\n6\\n4\\nHEAD\\n1\\nCOMP \\x02lled\\nSPR 3\\nSEM\\n\"\\nPRED and\\nARG1 4\\nARG2 5\\n#\\n3\\n7\\n7\\n7\\n5\\n!\\n2\\n4\\nHEAD\\n1\\nCOMP 2\\nSPR 3\\nSEM 4\\n3\\n5,\\n2\\n\\x14\\nCOMP \\x02lled\\nSEM 5\\n\\x15\\nRule2 ;;; spr \\x02lling:2\\n6\\n6\\n6\\n4\\nHEAD\\n1\\nCOMP \\x02lled\\nSPR \\x02lled\\nSEM\\n\"\\nPRED and\\nARG1 4\\nARG2 5\\n#\\n3\\n7\\n7\\n7\\n5\\n!\\n2\\n2\\n6\\n4\\nHEAD\\n\\x02\\nAGR\\n3\\n\\x03\\nCOMP \\x02lled\\nSPR \\x02lled\\nSEM 4\\n3\\n7\\n5,\\n2\\n6\\n4\\nHEAD\\n1\\n\\x02\\nAGR 3\\n\\x03\\nCOMP \\x02lled\\nSPR 2\\nSEM 5\\n3\\n7\\n5\\nLexicon:\\n47'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 47}, page_content='can\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT verb\\nAGR pl\\n\\x15\\nCOMP\\n2\\n4\\nHEAD\\n\\x02\\nCAT noun\\n\\x03\\nCOMP \\x02lled\\nSEM\\n\\x02\\nINDEX\\n2\\n\\x03\\n3\\n5\\nSPR\\n\"\\nHEAD\\n\\x02\\nCAT noun\\n\\x03\\nSEM\\n\\x02\\nINDEX\\n1\\n\\x03\\n#\\nSEM\\n\"\\nPRED can v\\nARG1 1\\nARG2 2\\n#\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\n;;; transitive verb\\n\\x02sh\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\nSEM\\n\"\\nINDEX\\n1\\nPRED \\x02sh n\\nARG1 1\\n#\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\n;;; noun phrase\\nthey\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nHEAD\\n\\x14\\nCAT noun\\nAGR pl\\n\\x15\\nCOMP \\x02lled\\nSPR \\x02lled\\nSEM\\n\"\\nINDEX\\n1\\nPRED pron\\nARG1 1\\n#\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\n;;; noun phrase\\nAn alternative approach to encoding semantics is to write the semantic composition rules in a separate formalism\\nsuch as typed lambda calculus. This corresponds more closely to the approach most commonly assumed in formal\\nlinguistics: variants of lambda calculus are sometimes used in NLP, but I won\\'t discuss this further here.\\nIn general, a semantic representation constructed for a sentence is called the logical form of the sentence. The se-'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 47}, page_content=\"In general, a semantic representation constructed for a sentence is called the logical form of the sentence. The se-\\nmantics shown above can be taken to be equivalent to a form of predicate calculus without variables or quanti\\x02ers:\\ni.e. the `variables' in the representation actually correspond to constants. It turns out that this very impoverished\\nform of semantic representation is adequate for many NLP applications: template representations, used in information\\nextraction or simple dialogue systems can be thought of as equivalent to this. But for a fully adequate representation\\nwe need something richer \\x97 for instance, to do negation properly. Minimally we need full \\x02rst-order predicate cal-\\nculus (FOPC). FOPC logical forms can be passed to theorem-provers in order to do inference about the meaning of a\\nsentence. However, although this approach has been extensively explored in research work, especially in the 1980s, it\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 47}, page_content=\"sentence. However, although this approach has been extensively explored in research work, especially in the 1980s, it\\nhasn't so far led to practical systems. There are many reasons for this, but perhaps the most important is the dif\\x02culty\\nof acquiring detailed domain knowledge expressed in FOPC. There is also a theoretical AI problem, because we seem\\nto need some form of probabilistic reasoning for many applications. So, although most researchers who are working in\\ncomputational compositional semantics take support for inference as a desideratum, many systems actually use some\\nform of shallow inference (e.g., semantic transfer in MT, mentioned in lecture 8).\\nFOPC also has the disadvantage that it forces quanti\\x02ers to be in a particular scopal relationship, and this information\\nis not (generally) overt in NL sentences. One classic example is:\\nEvery man loves a woman\\nwhich is ambiguous between:\\n8x[man0(x) )9y[woman0(y) ^love0(x;y)]]\\nand the less-likely, `one speci\\x02c woman' reading:\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 47}, page_content=\"Every man loves a woman\\nwhich is ambiguous between:\\n8x[man0(x) )9y[woman0(y) ^love0(x;y)]]\\nand the less-likely, `one speci\\x02c woman' reading:\\n9y[woman0(y) ^8x[man0(x) )love0(x;y)]]\\nMost current systems construct an underspeci\\x02ed representation which is neutral between these readings, if they\\nrepresent quanti\\x02er scope at all. There are several different alternative formalisms for underspeci\\x02cation.\\n48\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 48}, page_content=\"6.2 Generation\\nWe can generate from a semantic representation with a suitable FS grammar. Producing an output string given an input\\nlogical form is generally referred to as tactical generation or realization, as opposed to strategic generation or text\\nplanning, which concerns how you might build the logical form in the \\x02rst place. Strategic generation is an open-ended\\nproblem: it depends very much on the application and I won't have much to say about it here. Tactical generation is\\nmore tractable, and is useful without a strategic component in some contexts, such as the semantic transfer approach\\nto MT, which I'll brie\\x03y discuss in lecture 8.\\nTactical generation can use similar techniques to parsing: for instance one approach is chart generation which uses\\nmany of the same techniques as chart parsing. There has been much less work on generation than on parsing in general,\\nand building bidirectional grammars is hard: most grammars for parsing allow through many ungrammatical strings.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 48}, page_content=\"and building bidirectional grammars is hard: most grammars for parsing allow through many ungrammatical strings.\\nRecently there has been some work on statistical generation, where n-grams are used to choose between realisations\\nconstructed by a grammar that overgenerates. But even relatively `tight' bidirectional grammars may need to use\\nstatistical techniques in order to generate natural sounding utterances.\\n6.3 Meaning postulates\\nInference rules can be used to relate open class predicates: i.e., predicates that correspond to open class words. This is\\nthe classic way of representing lexical meaning in formal semantics within linguistics:17\\n8x[bachelor(x) $man(x) ^unmarried(x)]\\nLinguistically and philosophically, this gets pretty dubious. Is the current Pope a bachelor? Technically presumably\\nyes, but bachelor seems to imply someone who could be married: it's a strange word to apply to the Pope under\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 48}, page_content=\"yes, but bachelor seems to imply someone who could be married: it's a strange word to apply to the Pope under\\ncurrent assumptions about celibacy. Meaning postulates are also too unconstrained: I could construct a predicate\\n`bachelor-weds-thurs' to correspond to someone who was unmarried on Wednesday and married on Thursday, but this\\nisn't going to correspond to a word in any natural language. In any case, very few words are as simple to de\\x02ne as\\nbachelor: consider how you might start to de\\x02ne table, tomato or thought, for instance.18\\nFor computational semantics, perhaps the best way of regarding meaning postulates is simply as one reasonable way of\\nlinking compositionally constructed semantic representations to a speci\\x02c domain. In NLP, we're normally concerned\\nwith implication rather than de\\x02nition and this is less problematic philosophically:\\n8x[bachelor(x) !man(x) ^unmarried(x)]\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 48}, page_content='with implication rather than de\\x02nition and this is less problematic philosophically:\\n8x[bachelor(x) !man(x) ^unmarried(x)]\\nHowever, the big computational problems with meaning postulates are their acquisition and the control of inference\\nonce they have been obtained. Building meaning postulates for anything other than a small, bounded domain is an\\nAI-complete problem.\\nThe more general, shallower, relationships that are classically discussed in lexical semantics are currently more useful\\nin NLP, especially for broad-coverage processing.\\n6.4 Hyponymy: IS-A\\nHyponymy is the classical IS-A relation: e.g. dog is a hyponym of animal. That is, the relevant sense of dog is the\\nhyponym of animal: as nearly everything said in this lecture is about word senses rather than words, I will avoid\\nexplicitly qualifying all statements in this way, but this should be globally understood.\\nanimal is the hypernym of dog. Hyponyms can be arranged into taxonomies: classically these are tree-structured: i.e.,'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 48}, page_content=\"animal is the hypernym of dog. Hyponyms can be arranged into taxonomies: classically these are tree-structured: i.e.,\\neach term has only one hypernym.\\nDespite the fact that hyponymy is by far the most important meaning relationship assumed in NLP, many questions\\narise which don't currently have very good answers:\\n17Generally, linguists don't actually write meaning postulates for open-class words, but this is the standard assumption about how meaning would\\nbe represented if anyone could be bothered to do it!\\n18There has been a court case that hinged on the precise meaning of table and also one that depended on whether tomatoes were fruits or\\nvegetables.\\n49\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 49}, page_content=\"1. What classes of words can be categorised by hyponymy? Some nouns, classically biological taxonomies, but\\nalso human artifacts, professions etc work reasonably well. Abstract nouns, such as truth, don't really work very\\nwell (they are either not in hyponymic relationships at all, or very shallow ones). Some verbs can be treated as\\nbeing hyponyms of one another \\x97 e.g. murder is a hyponym of kill, but this is not nearly as clear as it is for\\nconcrete nouns. Event-denoting nouns are similar to verbs in this respect. Hyponymy is essentially useless for\\nadjectives.\\n2. Do differences in quantisation and individuation matter? For instance, is chair a hyponym of furniture? is beer\\na hyponym of drink? is coin a hyponym of money?\\n3. Is multiple inheritance allowed? Intuitively, multiple parents might be possible: e.g. coin might be metal (or\\nobject?) and also money. Artifacts in general can often be described either in terms of their form or their\\nfunction.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 49}, page_content=\"object?) and also money. Artifacts in general can often be described either in terms of their form or their\\nfunction.\\n4. What should the top of the hierarchy look like? The best answer seems to be to say that there is no single top\\nbut that there are a series of hierarchies.\\n6.5 Other lexical semantic relations\\nMeronymy i.e., PART-OF\\nThe standard examples of meronymy apply to physical relationships: e.g., arm is part of a body (arm is a\\nmeronym of body); steering wheel is a meronym of car. Note the distinction between `part' and `piece': if I\\nattack a car with a chainsaw, I get pieces rather than parts!\\nSynonymy i.e., two words with the same meaning (or nearly the same meaning)\\nTrue synonyms are relatively uncommon: most cases of true synonymy are correlated with dialect differences\\n(e.g., eggplant / aubergine, boot / trunk). Often synonymy involves register distinctions, slang or jargons: e.g.,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 49}, page_content=\"(e.g., eggplant / aubergine, boot / trunk). Often synonymy involves register distinctions, slang or jargons: e.g.,\\npoliceman, cop, rozzer . . . Near-synonyms convey nuances of meaning: thin, slim, slender, skinny.\\nAntonymy i.e., opposite meaning\\nAntonymy is mostly discussed with respect to adjectives: e.g., big/little, though it's only relevant for some\\nclasses of adjectives.\\n6.6 WordNet\\nWordNet is the main resource for lexical semantics for English that is used in NLP \\x97 primarily because of its very\\nlarge coverage and the fact that it's freely available. WordNets are under development for many other languages,\\nthough so far none are as extensive as the original.\\nThe primary organisation of WordNet is into synsets: synonym sets (near-synonyms). To illustrate this, the following\\nis part of what WordNet returns as an `overview' of red:\\nwn red -over\\nOverview of adj red\\nThe adj red has 6 senses (first 5 from tagged texts)\\n1. (43) red, reddish, ruddy, blood-red, carmine,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 49}, page_content='is part of what WordNet returns as an `overview\\' of red:\\nwn red -over\\nOverview of adj red\\nThe adj red has 6 senses (first 5 from tagged texts)\\n1. (43) red, reddish, ruddy, blood-red, carmine,\\ncerise, cherry, cherry-red, crimson, ruby, ruby-red,\\nscarlet -- (having any of numerous bright or strong\\ncolors reminiscent of the color of blood or cherries\\nor tomatoes or rubies)\\n2. (8) red, reddish -- ((used of hair or fur) of a\\nreddish brown color; \"red deer\"; reddish hair\")\\n50'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 50}, page_content='Nouns in WordNet are organised by hyponymy, as illustrated by the fragment below:\\nSense 6\\nbig cat, cat\\n=> leopard, Panthera pardus\\n=> leopardess\\n=> panther\\n=> snow leopard, ounce, Panthera uncia\\n=> jaguar, panther, Panthera onca, Felis onca\\n=> lion, king of beasts, Panthera leo\\n=> lioness\\n=> lionet\\n=> tiger, Panthera tigris\\n=> Bengal tiger\\n=> tigress\\n=> liger\\n=> tiglon, tigon\\n=> cheetah, chetah, Acinonyx jubatus\\n=> saber-toothed tiger, sabertooth\\n=> Smiledon californicus\\n=> false saber-toothed tiger\\nThe following is an overview of the information available in WordNet for the various POS classes:\\n\\x0f all classes\\n1. synonyms (ordered by frequency)\\n2. familiarity / polysemy count\\n3. compound words (done by spelling)\\n\\x0f nouns\\n1. hyponyms / hypernyms (also sisters)\\n2. holonyms / meronyms\\n\\x0f adjectives\\n1. antonyms\\n\\x0f verbs\\n1. antonyms\\n2. hyponyms / hypernyms (also sisters)\\n3. syntax (very simple)\\n\\x0f adverbs'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 50}, page_content=\"\\x0f nouns\\n1. hyponyms / hypernyms (also sisters)\\n2. holonyms / meronyms\\n\\x0f adjectives\\n1. antonyms\\n\\x0f verbs\\n1. antonyms\\n2. hyponyms / hypernyms (also sisters)\\n3. syntax (very simple)\\n\\x0f adverbs\\nTaxonomies have also been automatically or semi-automatically extracted from machine-readable dictionaries, but\\nthese are not distributed. Microsoft's MindNet is the best known example (it has many more relationships than just\\nhyponymy). There are other collections of terms, generally hierarchically ordered, especially medical ontologies.\\nThere have been a number of attempts to build an ontology for world knowledge: none of the more elaborate ones are\\ngenerally available. There is an ongoing attempt at standardisation of ontologies. Ontology support is an important\\ncomponent of the semantic web.\\n51\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 51}, page_content=\"6.7 Using lexical semantics\\nBy far the most commonly used lexical relation is hyponymy. Hyponymy relations can be used in many ways:\\n\\x0f Semantic classi\\x02cation: e.g., for selectional restrictions (e.g., the object of eat has to be something edible) and\\nfor named entity recognition\\n\\x0f Shallow inference: `X murdered Y' implies `X killed Y' etc\\n\\x0f Back-off to semantic classes in some statistical approaches\\n\\x0f Word-sense disambiguation\\n\\x0f MT: if you can't translate a term, substitute a hypernym\\n\\x0f Query expansion for information retrieval: if a search doesn't return enough results, one option is to replace an\\nover-speci\\x02c term with a hypernym\\nSynonymy or near-synonymy is relevant for some of these reasons and also for generation. (However dialect and reg-\\nister haven't been investigated much in NLP, so the possible relevance of different classes of synonym for customising\\ntext hasn't really been looked at.)\\n6.8 Polysemy\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 51}, page_content=\"ister haven't been investigated much in NLP, so the possible relevance of different classes of synonym for customising\\ntext hasn't really been looked at.)\\n6.8 Polysemy\\nPolysemy refers to the state of a word having more than one sense: the standard example is bank (river bank) vs bank\\n(\\x02nancial institution).\\nThis is homonymy \\x97 the two senses are unrelated (not entirely true for bank, actually, but historical relatedness isn't\\nactually important \\x97 it's whether ordinary speakers of the language feel there's a relationship). Homonymy is the\\nmost obvious case of polysemy, but is actually relatively infrequent compared to uses which have different but related\\nmeanings, such as bank (\\x02nancial institution) vs bank (in a casino).\\nIf polysemy were always homonymy, word senses would be discrete: two senses would be no more likely to share\\ncharacteristics than would morphologically unrelated words. But most senses are actually related. Regular or sys-\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 51}, page_content='characteristics than would morphologically unrelated words. But most senses are actually related. Regular or sys-\\ntematic polysemy concerns related but distinct usages of words, often with associated syntactic effects. For instance,\\nstrawberry, cherry (fruit / plant), rabbit, turkey, halibut (meat / animal), tango, waltz (dance (noun) / dance (verb)).\\nThere are a lot of complicated issues in deciding whether a word is polysemous or simply general/vague. For instance,\\nteacher is intuitively general between male and female teachers rather than ambiguous, but giving good criteria as a\\nbasis of this distinction is dif\\x02cult. Dictionaries are not much help, since their decisions as to whether to split a sense\\nor to provide a general de\\x02nition are very often contingent on external factors such as the size of the dictionary or the\\nintended audience, and even when these factors are relatively constant, lexicographers often make different decisions\\nabout whether and how to split up senses.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 51}, page_content=\"intended audience, and even when these factors are relatively constant, lexicographers often make different decisions\\nabout whether and how to split up senses.\\n6.9 Word sense disambiguation\\nWord sense disambiguation (WSD) is needed for most NL applications that involve semantics (explicitly or implicitly).\\nIn limited domains, WSD is not too big a problem, but for large coverage text processing it's a serious bottleneck.\\nWSD needs depend on the application \\x97 there is no objective notion of word sense (dictionaries differ extensively) and\\nit's very hard to come up with good criteria to judge whether or not to distinguish senses. But in order to experiment\\nwith WSD as a standalone module, there has to be a standard: most commonly WordNet, because it is the only\\nextensive modern resource for English with no problematic IPR issues. This is controversial, because WordNet has a\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 51}, page_content=\"extensive modern resource for English with no problematic IPR issues. This is controversial, because WordNet has a\\nvery \\x02ne granularity of senses \\x97 it's also obvious that its senses often overlap. However, the only current alternative\\nis a pre-1920 version of Webster's. Recently WSD `competitions' have been organised: SENSEVAL and SENSEVAL\\n2.\\nWSDup to the early 1990s was mostly done by hand-constructed rules (still used in some MT systems). Dahlgren\\ninvestigated WSD in a fairly broad domain in the 1980s. Reasonably broad-coverage WSD generally depends on:\\n52\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 52}, page_content=\"\\x0f frequency\\n\\x0f collocations\\n\\x0f selectional restrictions/preferences\\nWhat's changed since the 1980s is that various statistical or machine-learning techniques have been used to avoid\\nhand-crafting rules.\\n\\x0f supervised learning. Requires a sense-tagged corpus, which is extremely time-consuming to construct systemat-\\nically (examples are the Semcor and SENSEVAL corpora, but both are really too small). Most experimentation\\nhas been done with a small set of words which can be sense-tagged by the experimenter (e.g., plant). Supervised\\nlearning techniques do not carry over well from one corpus to another.\\n\\x0f unsupervised learning (see below)\\n\\x0f Machine readable dictionaries (MRDs). Disambiguating dictionary de\\x02nitions according to the internal data in\\ndictionaries is necessary to build taxonomies from MRDs. MRDs have also been used as a source of selectional\\npreference and collocation information for general WSD (quite successfully).\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 52}, page_content=\"dictionaries is necessary to build taxonomies from MRDs. MRDs have also been used as a source of selectional\\npreference and collocation information for general WSD (quite successfully).\\nUntil recently, most of the statistical or machine-learning techniques have been evaluated on homonyms: these are\\nrelatively easy to disambiguate. So 95% disambiguation in e.g., Yarowsky's experiments sounds good (see below),\\nbut doesn't translate into high precision on all words when target is WordNet senses (in SENSEVAL 2 the best system\\nwas around 70%).\\nThere have also been some attempts at automatic sense induction, where an attempt is made to determine the clusters\\nof usages in texts that correspond to senses. In principle, this is a very good idea, since the whole notion of a word\\nsense is fuzzy: word senses can be argued to be artifacts of dictionary publishing. However, so far sense induction\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 52}, page_content=\"sense is fuzzy: word senses can be argued to be artifacts of dictionary publishing. However, so far sense induction\\nhas not been much explored in monolingual contexts, though it could be considered as an inherent part of statistical\\napproaches to MT.\\n6.10 Collocations\\nInformally, a collocation is a group of two or more words that occur together more often than would be expected by\\nchance (there are other de\\x02nitions \\x97 this is not really a precise notion). Collocations have always been the most useful\\nsource of information for WSD, even in Dahlgren's early experiments. For instance:\\n(23) Striped bass are common.\\n(24) Bass guitars are common.\\nstriped is a good indication that we're talking about the \\x02sh (because it's a particular sort of bass), similarly with guitar\\nand music. In both bass guitar and striped bass, we've arguably got a multiword expression (i.e., a conventional phrase\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 52}, page_content=\"and music. In both bass guitar and striped bass, we've arguably got a multiword expression (i.e., a conventional phrase\\nthat might be listed in a dictionary), but the principle holds for any sort of collocation. The best collocates for WSD\\ntend to be syntactically related in the sentence to the word to be disambiguated, but many techniques simply use a\\nwindow of words.\\nJ&M make a useful (though non-standard) distinction between collocation and co-occurrence: co-occurrence refers to\\nthe appearance of another word in a larger window of text than a collocation. For instance, trout might co-occur with\\nthe \\x02sh sense of bass.\\n6.11 Yarowsky's unsupervised learning approach to WSD\\nYarowsky (1995) describes a technique for unsupervised learning using collocates (collocates and co-occurrences in\\nJ&M's terms). A few seed collocates are chosen for each sense (manually or via an MRD), then these are used\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 52}, page_content=\"J&M's terms). A few seed collocates are chosen for each sense (manually or via an MRD), then these are used\\nto accurately identify distinct senses. The sentences in which the disambiguated senses occur can then be used to\\nlearn other discriminating collocates automatically, producing a decision list. The process can then be iterated. The\\n53\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 53}, page_content=\"algorithm allows bad collocates to be overridden. This works because of the general principle of `one sense per\\ncollocation' (experimentally demonstrated by Yarowsky \\x97 it's not absolute, but there are very strong preferences).\\nIn a bit more detail, using Yarowsky's example of disambiguating plant (which is homonymous between factory vs\\nvegetation senses):\\n1. Identify all examples of the word to be disambiguated in the training corpus and store their contexts.\\nsense training example\\n? company said that the plant is still operating\\n? although thousands of plant and animal species\\n? zonal distribution of plant life\\n? company manufacturing plant is in Orlando\\netc\\n2. Identify some seeds which reliably disambiguate a few of these uses. Tag the disambiguated senses and count\\nthe rest as residual. For instance, choosing `plant life' as a seed for the vegetation sense of plant (sense A) and\\n`manufacturing plant' as the seed for the factory sense (sense B):\\nsense training example\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 53}, page_content=\"`manufacturing plant' as the seed for the factory sense (sense B):\\nsense training example\\n? company said that the plant is still operating\\n? although thousands of plant and animal species\\nA zonal distribution of plant life\\nB company manufacturing plant is in Orlando\\netc\\nThis disambiguated 2% of uses in Yarowsky's corpus, leaving 98% residual.\\n3. Train a decision list classi\\x02er on the Sense A/Sense B examples. A decision list approach gives a list of criteria\\nwhich are tried in order until an applicable test is found: this is then applied. The tests are each associated with\\na reliability metric. The original seeds are likely to be at the top of the initial decision list, followed by other\\ndiscriminating terms. e.g. the decision list might include:\\nreliability criterion sense\\n8.10 plant life A\\n7.58 manufacturing plant B\\n6.27 animal within 10 words of plant A\\netc\\n4. Apply the decision list classi\\x02er to the training set and add all examples which are tagged with greater than a\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 53}, page_content=\"7.58 manufacturing plant B\\n6.27 animal within 10 words of plant A\\netc\\n4. Apply the decision list classi\\x02er to the training set and add all examples which are tagged with greater than a\\nthreshold reliability to the Sense A and Sense B sets.\\nsense training example\\n? company said that the plant is still operating\\nA although thousands of plant and animal species\\nA zonal distribution of plant life\\nB company manufacturing plant is in Orlando\\netc\\n5. Iterate the previous steps 3 and 4 until convergence\\n6. Apply the classi\\x02er to the unseen test data\\nYarowsky also demonstrated the principle of `one sense per discourse' (again, a very strong, but not absolute effect).\\nThis can be used as an additional re\\x02nement for the algorithm above.\\nYarowsky argues that decision lists work better than many other statistical frameworks because no attempt is made to\\ncombine probabilities. This would be complex, because the criteria are not independent of each other.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 53}, page_content=\"combine probabilities. This would be complex, because the criteria are not independent of each other.\\nYarowsky's experiments were nearly all on homonyms: these principles probably don't hold as well for sense exten-\\nsion.\\n54\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 54}, page_content=\"6.12 Evaluation of WSD\\nThe baseline for WSD is generally `pick the most frequent' sense: this is hard to beat! However, in many applications,\\nwe don't know the frequency of senses.\\nSENSEVAL and SENSEVAL-2 evaluated WSD in multiple languages, with various criteria, but generally using Word-\\nNet senses for English. The human ceiling for this task varies considerably between words: probably partly because of\\ninherent differences in semantic distance between groups of uses and partly because of WordNet itself, which some-\\ntimes makes very \\x02ne-grained distinctions. An interesting variant in SENSEVAL-2 was to do one experiment on WSD\\nwhere the disambiguation was with respect to uses requiring different translations into Japanese. This has the advan-\\ntage that it is useful and relatively objective, but sometimes this task requires splitting terms which aren't polysemous\\nin English (e.g., water \\x97 hot vs cold). Performance of WSD on this task seems a bit better than the general WSD\\ntask.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 54}, page_content=\"in English (e.g., water \\x97 hot vs cold). Performance of WSD on this task seems a bit better than the general WSD\\ntask.\\n6.13Further reading\\nJ&M go into quite a lot of detail about compositional semantics including underspeci\\x02cation.\\nWordNet is freely downloadable: the website has pointers to several papers which provide a good introduction.\\nFor a lot more detail of WSD than provided by J&M, see Manning and Schutze who have a very detailed account of\\nWSD and word-sense induction:\\nManning, Christopher and Hinrich Schutze (1999), Foundations of Statistical Natural Language Processing, MIT\\nPress\\nYarowsky's paper is well-written and should be understandable:\\nYarowsky, David (1995)\\nUnsupervised word sense disambiguation rivalling supervised methods,\\nProceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-95) MIT, 189\\x96196\\nLike many other recent NLP papers, this can be downloaded via www.citeseer.com\\n55\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 55}, page_content=\"7 Lecture 7: Discourse\\nUtterances are always understood in a particular context. Context-dependent situations include:\\n1. Referring expressions: pronouns, de\\x02nite expressions etc.\\n2. Universe of discourse: every dog barked, doesn't mean every dog in the world but only every dog in some\\nexplicit or implicit contextual set.\\n3. Responses to questions, etc: only make sense in a context: Who came to the party? Not Sandy.\\n4. Implicit relationships between events: Max fell. John pushed him \\x97 the second sentence is (usually) understood\\nas providing a causal explanation.\\nIn the \\x02rst part of this lecture, I give a brief overview of rhetorical relations which can be seen as structuring text\\nat a level above the sentence. I'll then go on to talk about one particular case of context-dependent interpretation \\x97\\nanaphor resolution. I will describe an algorithm for anaphor resolution which uses a relatively broad-coverage shallow\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 55}, page_content='anaphor resolution. I will describe an algorithm for anaphor resolution which uses a relatively broad-coverage shallow\\nparser and then discuss a variant of it that relies on POS-tagging and regular expression matching rather than parsing.\\n7.1 Rhetorical relations and coherence\\nConsider the following discourse:\\nMax fell. John pushed him.\\nThis discourse can be interpreted in at least two ways:\\n1. Max fell because John pushed him.\\n2. Max fell and then John pushed him.\\nThere seems to be an implicit relationship between the two original sentences: a discourse relation or rhetorical\\nrelation. (I will use the terms interchangeably here, though different theories use different terminology, and rhetorical\\nrelation tends to refer to a more surfacy concept than discourse relation.) In 1 the link is a form of explanation, but 2 is\\nan example of narration. Theories of discourse/rhetorical relations reify link types such as Explanation and Narration.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 55}, page_content=\"an example of narration. Theories of discourse/rhetorical relations reify link types such as Explanation and Narration.\\nThe relationship is made more explicit in 1 and 2 than it was in the original sentence: because and and then are said\\nto be cue phrases.\\n7.2 Coherence\\nDiscourses have to have connectivity to be coherent:\\nKim got into her car. Sandy likes apples.\\nBoth of these sentences make perfect sense in isolation, but taken together they are incoherent. Adding context can\\nrestore coherence:\\nKim got into her car. Sandy likes apples, so Kim thought she'd go to the farm shop and see if she could\\nget some.\\nThe second sentence can be interpreted as an explanation of the \\x02rst. In many cases, this will also work if the context\\nis known, even if it isn't expressed.\\nStrategic generation requires a way of implementing coherence. For example, consider a system that reports share\\nprices. This might generate:\\nIn trading yesterday: Dell was up 4.2%, Safeway was down 3.2%, Compaq was up 3.1%.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 55}, page_content='prices. This might generate:\\nIn trading yesterday: Dell was up 4.2%, Safeway was down 3.2%, Compaq was up 3.1%.\\n56'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 56}, page_content=\"This is much less acceptable than a connected discourse:\\nComputer manufacturers gained in trading yesterday: Dell was up 4.2% and Compaq was up 3.1%. But\\nretail stocks suffered: Safeway was down 3.2%.\\nHere but indicates a Contrast. Not much actual information has been added (assuming we know what sort of company\\nDell, Compaq and Safeway are), but the discourse is easier to follow.\\nDiscourse coherence assumptions can affect interpretation:\\nJohn likes Bill. He gave him an expensive Christmas present.\\nIf we interpret this as Explanation, then `he' is most likely Bill. But if it is Justi\\x02cation (i.e., the speaker is justifying\\nthe \\x02rst sentence), then `he' is John.\\n7.3 Factors in\\x03uencing discourse interpretation\\n1. Cue phrases. These are sometimes unambiguous, but not usually. e.g. and is a cue phrase when used in sentential\\nor VP conjunction.\\n2. Punctuation (also prosody) and text structure. For instance, parenthetical information cannot be related to a\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 56}, page_content='or VP conjunction.\\n2. Punctuation (also prosody) and text structure. For instance, parenthetical information cannot be related to a\\nmain clause by Narration, but a list is often interpreted as Narration:\\nMax fell (John pushed him) and Kim laughed.\\nMax fell, John pushed him and Kim laughed.\\nSimilarly, enumerated lists can indicate a form of narration.\\n3. Real world content:\\nMax fell. John pushed him as he lay on the ground.\\n4. Tense and aspect.\\nMax fell. John had pushed him.\\nMax was falling. John pushed him.\\nIt should be clear that it is potentially very hard to identify rhetorical relations. In fact, recent research that simply\\nuses cue phrases and punctuation is proving quite promising. This can be done by hand-coding a series of \\x02nite-state\\npatterns, or by a form of supervised learning.\\n7.4 Discourse structure and summarization\\nIf we consider a discourse relation as a relationship between two phrases, we get a binary branching tree structure for'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 56}, page_content=\"7.4 Discourse structure and summarization\\nIf we consider a discourse relation as a relationship between two phrases, we get a binary branching tree structure for\\nthe discourse. In many relationships, such as Explanation, one phrase depends on the other: e.g., the phrase being\\nexplained is the main one and the other is subsidiary. In fact we can get rid of the subsidiary phrases and still have\\na reasonably coherent discourse. (The main phrase is sometimes called the nucleus and the subsidiary one is the\\nsatellite.) This can be exploited in summarization.\\nFor instance:\\nWe get a binary branching tree structure for the discourse. In many relationships one phrase depends on\\nthe other. In fact we can get rid of the subsidiary phrases and still have a reasonably coherent discourse.\\nOther relationships, such as Narration, give equal weight to both elements, so don't give any clues for summarization.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 56}, page_content=\"Other relationships, such as Narration, give equal weight to both elements, so don't give any clues for summarization.\\nRather than trying to \\x02nd rhetorical relations for arbitrary text, genre-speci\\x02c cues can be exploited, for instance for\\nscienti\\x02c texts. This allows more detailed summaries to be constructed.\\n57\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 57}, page_content=\"7.5 Referring expressions\\nI'll now move on to talking about another form of discourse structure, speci\\x02cally the link between referring expres-\\nsions. The following example will be used to illustrate referring expressions and anaphora resolution:\\nNiall Ferguson is proli\\x02c, well-paid and a snappy dresser. Stephen Moss hated him \\x97 at least until he\\nspent an hour being charmed in the historian's Oxford study. (quote taken from the Guardian)\\nSome terminology:\\nreferent a real world entity that some piece of text (or speech) refers to. e.g., the two people who are mentioned in\\nthis quote.\\nreferring expressions bits of language used to perform reference by a speaker. In, the paragraph above, Niall Fergu-\\nson, him and the historian are all being used to refer to the same person (they corefer).\\nantecedent the text evoking a referent. Niall Ferguson is the antecedent of him and the historian\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 57}, page_content='son, him and the historian are all being used to refer to the same person (they corefer).\\nantecedent the text evoking a referent. Niall Ferguson is the antecedent of him and the historian\\nanaphora the phenomenon of referring to an antecedent: him and the historian are anaphoric because they refer to a\\npreviously introduced entity.\\nWhat about a snappy dresser? Traditionally, this would be described as predicative: that is, it is a predicate, like an\\nadjective, rather than being a referring expression itself.\\nGenerally, entities are introduced in a discourse (technically, evoked) by inde\\x02nite noun phrases or proper names.\\nDemonstratives and pronouns are generally anaphoric. De\\x02nite noun phrases are often anaphoric (as above), but often\\nused to bring a mutually known and uniquely identi\\x02able entity into the current discourse. e.g., the president of the\\nUS.\\nSometimes, pronouns appear before their referents are introduced: this is cataphora. E.g., at the start of a discourse:'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 57}, page_content=\"US.\\nSometimes, pronouns appear before their referents are introduced: this is cataphora. E.g., at the start of a discourse:\\nAlthough she couldn't see any dogs, Kim was sure she'd heard barking.\\nboth cases of she refer to Kim - the \\x02rst is a cataphor.\\n7.6 Pronoun agreement\\nPronouns generally have to agree in number and gender with their antecedents. In cases where there's a choice of\\npronoun, such as he/she or it for an animal (or a baby, in some dialects), then the choice has to be consistent.\\n(25) A little girl is at the door \\x97 see what she wants, please?\\n(26) My dog has hurt his foot \\x97 he is in a lot of pain.\\n(27) * My dog has hurt his foot \\x97 it is in a lot of pain.\\nComplications include the gender neutral they (some dialects), use of they with everybody, group nouns, conjunctions\\nand discontinuous sets:\\n(28) Somebody's at the door \\x97 see what they want, will you?\\n(29) I don't know who the new teacher will be, but I'm sure they'll make changes to the course.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 57}, page_content=\"and discontinuous sets:\\n(28) Somebody's at the door \\x97 see what they want, will you?\\n(29) I don't know who the new teacher will be, but I'm sure they'll make changes to the course.\\n(30) Everybody's coming to the party, aren't they?\\n(31) The team played really well, but now they are all very tired.\\n(32) Kim and Sandy are asleep: they are very tired.\\n(33) Kim is snoring and Sandy can't keep her eyes open: they are both exhausted.\\n58\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 58}, page_content=\"7.7 Re\\x03exives\\n(34) Johni cut himselfi shaving. (himself = John, subscript notation used to indicate this)\\n(35) # Johni cut himj shaving. (i 6= j \\x97 a very odd sentence)\\nThe informal and not fully adequate generalisation is that re\\x03exive pronouns must be co-referential with a preceding\\nargument of the same verb (i.e., something it subcategorizes for), while non-re\\x03exive pronouns cannot be. In linguis-\\ntics, the study of inter-sentential anaphora is known as binding theory: I won't discuss this further, since the constraints\\non reference involved are quite different from those with intra-sentential anaphora.\\n7.8 Pleonastic pronouns\\nPleonastic pronouns are semantically empty, and don't refer:\\n(36) It is snowing\\n(37) It is not easy to think of good examples.\\n(38) It is obvious that Kim snores.\\n(39) It bothers Sandy that Kim snores.\\nNote also:\\n(40) They are digging up the street again\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 58}, page_content=\"(36) It is snowing\\n(37) It is not easy to think of good examples.\\n(38) It is obvious that Kim snores.\\n(39) It bothers Sandy that Kim snores.\\nNote also:\\n(40) They are digging up the street again\\nThis is an (informal) use of they which, though probably not technically pleonastic, doesn't apparently refer to a\\ndiscourse referent in the standard way (they = `the authorities'??).\\n7.9 Salience\\nThere are a number of effects which cause particular pronoun referents to be preferred, after all the hard constraints\\ndiscussed above are taken into consideration.\\nRecency More recent referents are preferred. Only relatively recently referred to entities are accessible.\\n(41) Kim has a fast car. Sandy has an even faster one. Lee likes to drive it.\\nit preferentially refers to Sandy's car, rather than Kim's.\\nGrammatical role Subjects >objects >everything else:\\n(42) Fred went to the Grafton Centre with Bill. He bought a CD.\\nhe is more likely to be interpreted as Fred than as Bill.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 58}, page_content='Grammatical role Subjects >objects >everything else:\\n(42) Fred went to the Grafton Centre with Bill. He bought a CD.\\nhe is more likely to be interpreted as Fred than as Bill.\\nRepeated mention Entities that have been mentioned more frequently are preferred:\\n(43) Fred was getting bored. He decided to go shopping. Bill went to the Grafton Centre with Fred. He\\nbought a CD.\\nHe=Fred (maybe) despite the general preference for subjects.\\nParallelism Entities which share the same role as the pronoun in the same sort of sentence are preferred:\\n(44) Bill went with Fred to the Grafton Centre. Kim went with him to Lion Yard.\\nHim=Fred, because the parallel interpretation is preferred.\\n59'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 59}, page_content='Coherence effects The pronoun resolution may depend on the rhetorical/discourse relation that is inferred.\\n(45) Bill likes Fred. He has a great sense of humour.\\nHe = Fred preferentially, possibly because the second sentence is interpreted as an explanation of the \\x02rst, and\\nhaving a sense of humour is seen as a reason to like someone.\\n7.10 Algorithms for resolving anaphora\\nMost work has gone into the problem of resolving pronoun referents. As well as discourse understanding, this is often\\nimportant in MT. For instance, English it has to be resolved to translate into German because German has grammatical\\ngender (though note, if there are two possible antecedents, but both have the same gender, we probably do not need\\nto resolve between the two for MT). I will describe one approach to anaphora resolution and a modi\\x02cation of it that\\nrequires fewer resources.\\n7.11 Lappin and Leass (1994)'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 59}, page_content=\"to resolve between the two for MT). I will describe one approach to anaphora resolution and a modi\\x02cation of it that\\nrequires fewer resources.\\n7.11 Lappin and Leass (1994)\\nThe algorithm relies on parsed text (from a fairly shallow, very broad-coverage parser, which unfortunately isn't\\ngenerally available). The text the system was developed and tested on was all from online computer manuals. The\\nfollowing description is a little simpli\\x02ed:\\nThe discourse model consists of a set of referring NPs arranged into equivalence classes, each class having a global\\nsalience value.\\nFor each sentence:\\n1. Divide by two the global salience factors for each existing equivalence class.\\n2. Identify referring NPs (i.e., exclude pleonastic it etc)\\n3. Calculate global salience factors for each NP (see below)\\n4. Update the discourse model with the referents and their global salience scores.\\n5. For each pronoun:\\n(a) Collect potential referents (cut off is four sentences back).\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 59}, page_content='4. Update the discourse model with the referents and their global salience scores.\\n5. For each pronoun:\\n(a) Collect potential referents (cut off is four sentences back).\\n(b) Filter referents according to binding theory and agreement constraints.\\n(c) Calculate the per pronoun adjustments for each referent (see below).\\n(d) Select the referent with the highest salience value for its equivalence class plus its per-pronoun adjustment.\\nIn case of a tie, prefer the closest referent in the string.\\n(e) Add the pronoun in to the equivalence class for that referent, and increment the salience factor by the\\nnon-duplicate salience factors pertaining to the pronoun.\\nThe salience factors were determined experimentally. Global salience factors mostly take account of grammatical\\nfunction \\x97 they encode the hierarchy mentioned previously. They give lowest weight to an NP in an adverbial'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 59}, page_content=\"function \\x97 they encode the hierarchy mentioned previously. They give lowest weight to an NP in an adverbial\\nposition, such as inside an adjunct PP. This is achieved by giving every non-adverbial an extra positive score, because\\nwe want all global salience scores to be positive integers. Embedded NPs are also downweighted by giving a positive\\nscore to non-embedded NPs. Recency weights mean that intra-sentential binding is preferred.\\nGlobal salience factors.\\nrecency 100\\nsubject 80\\nobjects of existential sentences 70\\ndirect object 50\\nindirect object 40\\noblique complement 40\\nnon-embedded noun 80\\nother non-adverbial 50\\n`Existential objects' refers to NPs which are in syntactic object position in sentences such as:\\n60\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 60}, page_content=\"There is a cat in the garden.\\nHere a cat is syntactically an object, but functions more like a subject, while there, which is syntactically the the\\nsubject, does not refer. An oblique complement is a complement other than a noun phrase, such as a PP.\\nThe per-pronoun modi\\x02cations have to be calculated each time a candidate pronoun is being evaluated. The modi\\x02-\\ncations strongly disprefer cataphora and slightly prefer referents which are `parallel', where parallel here just means\\nhaving the same syntactic role.\\nPer pronoun salience factors:\\ncataphora -175\\nsame role 35\\nApplying this to the sample discourse:\\nNiall Ferguson is proli\\x02c, well-paid and a snappy dresser.\\nStephen Moss hated him \\x97 at least until he spent an hour being charmed in the historian's Oxford study.\\nAssume we have processed up to `\\x97' and are resolving he. Discourse referents:\\nN Niall Ferguson, him 435\\nS Stephen Moss 310\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 60}, page_content=\"Assume we have processed up to `\\x97' and are resolving he. Discourse referents:\\nN Niall Ferguson, him 435\\nS Stephen Moss 310\\nI am assuming that a snappy dresser is ignored, although it might actually be treated as another potential referent,\\ndepending on the parser.\\nN has score 155 + 280 ((subject + non-embedded + non-adverbial + recency)/2 + (direct object + head + non-adverbial\\n+ recency))\\nS has score 310 (subject + non-embedded + non-adverbial + recency) + same role per-pronoun 35\\nSo in this case, the wrong candidate wins.\\nWe now add he to the discourse referent equivalence class. The additional weight is only 80, for subject, because we\\ndon't add weights for a given factor more than once in a sentence.\\nN Niall Ferguson, him, he 515\\nNote that the wrong result is quite plausible:\\nNiall Ferguson is proli\\x02c, well-paid and a snappy dresser.\\nStephen Moss hated him \\x97 at least until he spent an afternoon being interviewed at very short notice.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 60}, page_content=\"Niall Ferguson is proli\\x02c, well-paid and a snappy dresser.\\nStephen Moss hated him \\x97 at least until he spent an afternoon being interviewed at very short notice.\\nThe overall performance of the algorithm reported by Lappin and Leass was 86% but this was on computer manuals\\nalone. Their results can't be directly replicated, due to their use of a proprietary parser, but other experiments suggest\\nthat the accuracy on other types of text could be lower.\\n7.12 Anaphora for everyone\\nIt is potentially important to resolve anaphoric expressions, even for `shallow' NLP tasks, such as Web search, where\\nfull parsing is impractical. An article which mentions the name `Niall Ferguson' once, but then has multiple uses of\\n`he', `the historian' etc referring to the same person is more relevant to a search for `Niall Ferguson' than one which\\njust mentions the name once. It is therefore interesting to see whether an algorithm can be developed which does not\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 60}, page_content='just mentions the name once. It is therefore interesting to see whether an algorithm can be developed which does not\\nrequire parsed text. Kennedy and Boguraev (1996) describe a variant of Lappin and Leass which was developed for\\ntext which had just been tagged for part-of-speech.\\nThe input text was tagged with the Lingsoft tagger (a very high precision and recall tagger that uses manually developed\\nrules: see http://www.lingsoft.fi/demos.html). Besides POS tags, this gives some grammatical function\\ninformation: e.g., it notates subjects (for English, this is quite easy to do on the basis of POS-tagged text with some\\nsimple regular expressions). The text was then run through a series of regular expression \\x02lters to identify NPs and\\nmark expletive it. Heuristics de\\x02ned as regular expressions are also used to identify the NPs grammatical role. Global\\nsalience factors are as in Lappin and Leass, but Kennedy and Boguraev add a factor for context (as determined by a'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 60}, page_content=\"salience factors are as in Lappin and Leass, but Kennedy and Boguraev add a factor for context (as determined by a\\ntext segmentation algorithm). They also use a distinct factor for possessive NPs.\\nBecause this algorithm doesn't have access to a parser, the implementation of binding theory has to rely on heuristics\\nbased on the role relationships identi\\x02ed. Otherwise, the algorithm is much the same as for Lappin and Leass.\\n61\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 61}, page_content=\"Overall accuracy is quoted as 75%, measured on a mixture of genres (so it isn't possible to directly compare with\\nLappin and Leass, since that was only tested on computer manual information). Few errors were caused by the lack\\nof detailed syntactic information. 35% of errors were caused by failure to identify gender correctly, 14% were caused\\nbecause quoted contexts weren't handled.\\n7.13 Another note on evaluation\\nThe situation with respect to evaluation of anaphora resolution is less satisfactory than POS tagging or WSD. This is\\npartly because of lack of evaluation materials such as independently marked-up corpora. Another factor is the dif\\x02culty\\nin replication: e.g., Lappin and Leass's algorithm can't be fully replicated because of lack of availability of the parser.\\nThis can be partially circumvented by evaluating algorithms on treebanks, but existing treebanks are relatively limited\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 61}, page_content=\"This can be partially circumvented by evaluating algorithms on treebanks, but existing treebanks are relatively limited\\nin the sort of text they contain. Alternatively, different parsers can be compared according to the accuracy with which\\nthey supply the necessary information, but again this requires a suitable testing environment.\\n7.14 Further reading\\nJ&M discuss the most popular approach to rhetorical relations, rhetorical structure theory or RST. I haven't discussed\\nit in detail here, partly because I \\x02nd the theory very unclear: attempts to annotate text using RST approaches tend\\nnot to yield good interannotator agreement (see comments on evaluation in lecture 3), although to be fair, this is a\\nproblem with all approaches to rhetorical relations. The discussion of the factors in\\x03uencing anaphora resolution and\\nthe description of the Lappin and Leass algorithm that I've given here are partly based on J&M's account.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 61}, page_content=\"the description of the Lappin and Leass algorithm that I've given here are partly based on J&M's account.\\nThe references below are for completeness rather than suggested reading:\\nLappin, Shalom and Herb Leass (1994)\\nAn algorithm for pronominal anaphora resolution,\\nComputational Linguistics 20(4), 535\\x96561\\nKennedy, Christopher and Branimir Boguraev (1996)\\nAnaphora for everyone: pronominal anaphora resolution without a parser,\\nProceedings of the 16th International Conference on Computational Linguistics (COLING 96), Copenhagen, Den-\\nmark, 113\\x96118\\n62\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 62}, page_content=\"8 Lecture 8: Applications\\nThis lecture considers three applications of NLP: machine translation, spoken dialogue systems and email response.\\nThis isn't intended as a complete overview of these areas, but just as a way of describing how some of the techniques\\nwe've seen in the previous lectures are being used in current systems or how they might be used in the future.\\nMachine translation\\n8.1 Methodology for MT\\nThere are four main classical approaches to MT:\\n\\x0f Direct transfer: map between morphologically analysed structures.\\n\\x0f Syntactic transfer: map between syntactically analysed structures.\\n\\x0f Semantic transfer: map between semantics structures.\\n\\x0f Interlingua: construct a language-neutral representation from parsing and use this for generation.\\nThe standard illustration of the different classical approaches to MT is the Vauquois triangle. This is supposed to\\nillustrate the amount of effort required for analysis and generation as opposed to transfer in the different approaches.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 62}, page_content='illustrate the amount of effort required for analysis and generation as opposed to transfer in the different approaches.\\ne.g., direct transfer requires very little effort for analysis or generation, since it simply involves morphological analysis,\\nbut it requires more effort on transfer than syntactic or semantic transfer do.\\n-direct\\n-syntactic transfer\\n-semantic transfer\\n\\x1e\\nanalysis\\n^\\ngeneration\\nSource Language Target Language\\nInterlingua\\nThe Vauquois triangle is potentially misleading, because it suggests a simple trade-off in effort. It is at least as plausi-\\nble that the correct geometry is as below (the Vauquois inverted funnel with very long spout):\\n63'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 63}, page_content='-direct\\n-syntactic transfer\\n-\\nsemantic\\ntransfer\\nunderspeci\\x02ed semantics\\nresolved logical form\\nSource Language Target Language\\nLanguage Neutral Utterance Representation\\nThis diagram is intended to indicate that the goal of producing a language-neutral representation may be extremely\\ndif\\x02cult!\\nStatistical MT involves learning translations from a parallel corpus: i.e. a corpus consisting of multiple versions of\\na single text in different languages. The classic work was done on the proceedings of the Canadian parliament (the\\nCanadian Hansard). It is necessary to align the texts, so that sentences which are translations of each other are paired:\\nthis is non-trivial (the mapping may not be one-to-one). The original statistical MT approach can be thought of as\\ninvolving direct transfer, with some more recent work being closer to syntactic (or even semantic) transfer.\\nExample-based MT involves using a database of existing translation pairs and trying to \\x02nd the closest matching'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 63}, page_content='Example-based MT involves using a database of existing translation pairs and trying to \\x02nd the closest matching\\nphrase. It is very useful as part of machine-aided translation.\\n8.2 MT using semantic transfer\\nSemantic transfer is an approach to MT which involves:\\n1. Parsing a source language string to produce a meaning representation\\n2. Transforming that representation to one appropriate for the target language\\n3. Generating from the transformed representation\\nConstraint-based grammars are potentially well suited to semantic transfer.\\nFor instance:\\nInput: Kim singt\\nSource LF: named(x;\\x93Kim\\x94);singen(e;x)\\nTarget LF: named(x;\\x93Kim\\x94);sing(e;x)\\nOutput: Kim sings\\nTransfer rules:\\nsingen(e;x) $sing(e;x)\\n$indicates transfer equivalence or translation equivalence: the double arrow indicates reversibility:\\nInput: Kim sings\\nSource LF: named(x;\\x93Kim\\x94);sing(e;x)\\nTarget LF: named(x;\\x93Kim\\x94);singen(e;x)\\nOutput: Kim singt\\n64'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 64}, page_content=\"`named' can be regarded as a language-neutral predicate, so no transfer is necessary. We also assume we don't change\\nstrings like \\x93Kim\\x94.\\nSEMANTIC TRANSFER\\n*\\nj\\nPARSING\\n6\\nMORPHOLOGY\\n6\\nINPUT PROCESSING\\n6\\nsource language input\\nTACTICAL GENERATION\\n?\\nMORPHOLOGY GENERATION\\n?\\nOUTPUT PROCESSING\\n?\\ntarget language output\\nSemantic transfer rules are a form of quasi-inference: they map between meaning representations. Obviously the\\nexample above was trivial: more generally some form of mismatch is likely to be involved, although the idea of\\nsemantic transfer is that there is less mismatch at the semantic level than at a syntactic level. Semantic transfer\\ndoes not require that quanti\\x02er scope be resolved. Semantic transfer requires detailed bidirectional grammars for the\\nlanguages involved, which currently makes it more suitable for high-precision, limited domain systems.\\nAn anaphora resolution module is potentially needed when translating between languages like English and German,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 64}, page_content=\"An anaphora resolution module is potentially needed when translating between languages like English and German,\\nsince English it can correspond to German er, sie or es, for instance. But the resolution should be done on an `as-\\nneeded' basis, triggered by transfer, since it some contexts there is no ambiguity.\\nSome deployed MT systems use a form of semantic transfer, but syntactic transfer is more common. In these systems,\\ngeneration is usually a form of text reconstruction, rather than `proper' tactical generation. Direct transfer is used as a\\nfallback if syntactic analysis fails. Systran uses a mixture of direct transfer and syntactic transfer: it works reasonably\\nwell because it has an enormous lexicon of phrases. Handling multiword expressions is a major problem in MT.\\nStatistical MT is the commonest approach in the research community, followed by semantic transfer.\\nAll MT systems require some form of WSD: potentially big improvements could be made in this area. One dif\\x02culty,\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 64}, page_content='All MT systems require some form of WSD: potentially big improvements could be made in this area. One dif\\x02culty,\\nhowever, is that MT systems often have to operate with rather small amounts of text, which limits the availability of\\ncues.\\nDialoguesystems\\n8.3 Human dialogue basics\\nTurn-taking: generally there are points where a speaker invites someone else to take a turn (possibly choosing a\\nspeci\\x02c person), explicitly (e.g., by asking a question) or otherwise.\\nPauses: pauses between turns are generally very short (a few hundred milliseconds, but highly culture speci\\x02c).\\nLonger pauses are assumed to be meaningful: example from Levinson (1983: 300)\\nA: Is there something bothering you or not? (1.0 sec pause)\\nA: Yes or no? (1.5 sec pause)\\n65'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 65}, page_content=\"A: Eh?\\nB: No.\\nTurn-taking disruption is very dif\\x02cult to adjust to. This is evident in situations such as delays on phone lines\\nand people using speech prostheses, as well as slow automatic systems.\\nOverlap: Utterances can overlap (the acceptability of this is dialect/culture speci\\x02c but unfortunately humans tend to\\ninterrupt automated systems \\x97 this is known as barge in).\\nBackchannel: Utterances like Uh-huh, OK can occur during other speaker's utterance as a sign that the hearer is\\npaying attention.\\nAttention: The speaker needs reassurance that the hearer is understanding/paying attention. Often eye contact is\\nenough, but this is problematic with telephone conversations, dark sunglasses, etc. Dialogue systems should\\ngive explicit feedback.\\nCooperativity: Because participants assume the others are cooperative, we get effects such as indirect answers to\\nquestions.\\nWhen do you want to leave?\\nMy meeting starts at 3pm.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 65}, page_content=\"Cooperativity: Because participants assume the others are cooperative, we get effects such as indirect answers to\\nquestions.\\nWhen do you want to leave?\\nMy meeting starts at 3pm.\\nAll of these phenomena mean that the problem of spoken dialogue understanding is very complex. This together with\\nthe unreliability of speech recognition means that spoken dialogue systems are currently only usable for very limited\\ninteractions.\\n8.4Spoken dialogue systems\\n1. Single initiative systems (also known as system initiative systems): system controls what happens when.\\nSystem: Which station do you want to leave from?\\nUser: King's Cross\\nGenerally very limited: for instance, in the example above the system won't accept anything that's not a station\\nname. So it wouldn't accept either King's Cross or Liverpool Street, depending on when the next train to\\nCambridge is. Designing such systems tends to involve HCI issues (persuading the user not to complicate\\nthings), rather than language related ones.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 65}, page_content=\"Cambridge is. Designing such systems tends to involve HCI issues (persuading the user not to complicate\\nthings), rather than language related ones.\\n2. Mixed initiative dialogue. Both participants can control the dialogue to some extent.\\nSystem: Which station do you want to leave from?\\nUser: I don't know, tell me which station I need for Cambridge.\\nThe user has responded to a question with a question of their own, thereby taking control of the dialogue.\\nUnfortunately, getting systems like this to work properly is very dif\\x02cult and although research systems and a\\nsmall number of practical systems have been built, performance is often better if you don't allow this sort of\\ninteraction. The term `mixed-initiative' is often used (somewhat misleadingly) for systems which simply allow\\nusers to optionally specify more than one piece of information at once:\\nSystem: Which day do you want to leave?\\nUser: the twenty-third\\nOR\\nUser:the twenty-third of February\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 65}, page_content=\"users to optionally specify more than one piece of information at once:\\nSystem: Which day do you want to leave?\\nUser: the twenty-third\\nOR\\nUser:the twenty-third of February\\n3. Dialogue tracking. Explicit dialogue models may improve performance in other tasks such as spoken language\\nmachine translation or summarising a human-to-human dialogue. Generally it's less critical to get everything\\nright in such cases, which means broader domains are potentially realistic.\\n66\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 66}, page_content='The use of FSAs in controlling dialogues was mentioned in lecture 2. Initial versions of simple SDSs can now be\\nbuilt in a few weeks using toolkits developed by Nuance and other companies: CFGs are generally hand-built for each\\ndialogue state. This is time-consuming, but testing the SDS with real users and re\\x02ning it to improve performance is\\nprobably a more serious bottleneck in deploying systems.\\nEmail response using deep grammars\\n8.5 A large coverage grammar\\nThe email response application that I mentioned in lecture 1 might be addressed using domain-speci\\x02c grammars, but\\nunlike in dialogue systems, it is much more dif\\x02cult to make the limitations in the grammar obvious to the user (and\\nif the coverage is very limited a menu-driven system might well work better). It is too expensive to manually build a\\nnew broad-coverage grammar for each new application and grammar induction is generally not feasible because the'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 66}, page_content=\"new broad-coverage grammar for each new application and grammar induction is generally not feasible because the\\ndata that is available is too limited. The LinGO ERG constraint-based grammar mentioned in lecture 5 has been used\\nfor parsing in commercially-deployed email response systems. The grammar was slightly tailored for the different\\ndomains, but this mostly involved adding lexical entries. The ERG had previously been used on the Verbmobil spoken\\nlanguage MT task: the examples below are taken from this.\\nIndication of coverage of the ERG:\\n1. The week of the twenty second, I have two hour blocks available.\\n2. If you give me your name and your address we will send you the ticket.\\n3. Okay, actually I forgot to say that what we need is a two hour meeting.\\n4. The morning is good, but nine o'clock might be a little too late, as I\\nhave a seminar at ten o'clock.\\n5. Well, I am going on vacation for the next two weeks, so the \\x02rst day\\nthat I would be able to meet would be the eighteenth\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 66}, page_content=\"have a seminar at ten o'clock.\\n5. Well, I am going on vacation for the next two weeks, so the \\x02rst day\\nthat I would be able to meet would be the eighteenth\\n6. Did you say that you were free from three to \\x02ve p.m. on Wednesday,\\nthe third, because if so that would be a perfect time for me.\\nCoverage was around 80% on Verbmobil.\\nEf\\x02ciency (with the PET system on an 850MHz CPU):\\nItem Word Lexical Readings First All Passive\\nLength Entries Reading Readings Edges\\n1 12 33 15 150 ms 270 ms 1738\\n2 15 41 2 70 ms 110 ms 632\\n3 15 63 8 70 ms 140 ms 779\\n4 21 76 240 90 ms 910 ms 5387\\n5 26 87 300 1460 ms 8990 ms 41873\\n6 27 100 648 1080 ms 1450 ms 7850\\nThe ERG and other similar systems have demonstrated that it is possible to use a general purpose grammar in multiple\\napplications. However, it is crucial that there is a fallback strategy when a parse fails. For email response, the fallback\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 66}, page_content='applications. However, it is crucial that there is a fallback strategy when a parse fails. For email response, the fallback\\nis to send the email to a human. Reliability of the automated system is extremely important: sending an inappropriate\\nresponse can be very costly.\\nA big dif\\x02culty for email response is connecting the semantics produced by the general purpose grammar to the\\nunderlying knowledge base or database. This is expensive in terms of manpower, but does not require much linguistic\\nexpertise. Hence, this sort of approach is potentially commercially viable for organisations that have to deal with\\na lot of fairly routine email. Although tailoring the grammar by adding lexical entries is not too hard, it is much\\nmore dif\\x02cult to manually adjust the weights on grammar rules and lexical entries so that the best parse is preferred:\\nautomatic methods are de\\x02nitely required here. Much less training data is required to tune a grammar than to induce\\none.\\n8.6Further reading'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 66}, page_content='automatic methods are de\\x02nitely required here. Much less training data is required to tune a grammar than to induce\\none.\\n8.6Further reading\\nJ&M discuss MT and spoken dialogue systems.\\n67'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 67}, page_content='A glossary/index of some of the terms used in the lectures\\nThis is primarily intended to cover concepts which are mentioned in more than one lecture. The lecture where the term\\nis explained in most detail is generally indicated. In some cases, I have just given a pointer to the section in the lectures\\nwhere the term is de\\x02ned. Note that IGE stands for The Internet Grammar of English, http://www.ucl.ac.uk/internet-\\ngrammar/home.htm There are a few cases where this uses a term in a slightly different way from these course notes: I\\nhave tried to indicate these.\\nactive chart See x4.10.\\nadjective See IGE or notes for prelecture exercises in lecture 3.\\nadjunct See argument and also IGE.\\nadverb See IGE or notes for prelecture exercises in lecture 3.\\naf\\x02x A morpheme which can only occur in conjunction with other morphemes (lecture 2).\\nAI-complete A half-joking term, applied to problems that would require a solution to the problem of representing the'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 67}, page_content='AI-complete A half-joking term, applied to problems that would require a solution to the problem of representing the\\nworld and acquiring world knowledge (lecture 1).\\nagreement The requirement for two phrases to have compatible values for grammatical features such as number and\\ngender. For instance, in English, dogs bark is grammatical but dog bark and dogs barks are not. See IGE.\\n(lecture 5)\\nambiguity The same string (or sequence of sounds) meaning different things. Contrasted with vagueness.\\nanaphora The phenomenon of referring to something that was mentioned previously in a text. An anaphor is an\\nexpression which does this, such as a pronoun (see x7.5).\\nantonymy Opposite meaning: such as clean and dirty (x6.5).\\nargument In syntax, the phrases which are lexically required to be present by a particular word (prototypically a\\nverb). This is as opposed to adjuncts, which modify a word or phrase but are not required. For instance, in:\\nKim saw Sandy on Tuesday'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 67}, page_content='verb). This is as opposed to adjuncts, which modify a word or phrase but are not required. For instance, in:\\nKim saw Sandy on Tuesday\\nSandy is an argument but on Tuesday is an adjunct. Arguments are speci\\x02ed by the subcategorization of a verb\\netc. Also see the IGE. (lecture 5)\\naspect A term used to cover distinctions such as whether a verb suggests an event has been completed or not (as\\nopposed to tense, which refers to the time of an event). For instance, she was writing a book vs she wrote a\\nbook.\\nbackoff Usually used to refer to techniques for dealing with data sparseness in probabilistic systems: using a more\\ngeneral classi\\x02cation rather than a more speci\\x02c one. For instance, using unigram probabilities instead of\\nbigrams; using word classes instead of individual words (lecture 3).\\nbaseline In evaluation, the performance produced by a simple system against which the experimental technique is\\ncompared (x3.6).\\nbidirectional Usable for both analysis and generation (lecture 2).'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 67}, page_content=\"baseline In evaluation, the performance produced by a simple system against which the experimental technique is\\ncompared (x3.6).\\nbidirectional Usable for both analysis and generation (lecture 2).\\ncase Distinctions between nominals indicating their syntactic role in a sentence. In English, some pronouns show a\\ndistinction: e.g., she is used for subjects, while her is used for objects. e.g., she likes her vs *her likes she.\\nLanguages such as German and Latin mark case much more extensively.\\nceiling In evaluation, the performance produced by a `perfect' system (such as human annotation) against which the\\nexperimental technique is compared (x3.6).\\nchart parsing See x4.6.\\n68\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 68}, page_content='Chomsky Noam Chomsky, professor at MIT. The founder of modern theories of syntax in linguistics.\\nclosed class Refers to parts of speech, such as conjunction, for which all the members could potentially be enumerated\\n(lecture 3).\\ncoherence See x7.2\\ncollocation See x6.10\\ncomplement For the purposes of this course, an argument other than the subject.\\ncompositionality The idea that the meaning of a phrase is a function of the meaning of its parts. compositional\\nsemantics is the study of how meaning can be built up by semantic rules which mirror syntactic structure\\n(lecture 6).\\nconstituent A sequence of words which is considered as a unit in a particular grammar (lecture 4).\\nconstraint-based grammar A formalism which describes a language using a set of independently stated constraints,\\nwithout imposing any conditions on processing or processing order (lecture 5).\\ncontext The situation in which an utterance occurs: includes prior utterances, the physical environment, background'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 68}, page_content='without imposing any conditions on processing or processing order (lecture 5).\\ncontext The situation in which an utterance occurs: includes prior utterances, the physical environment, background\\nknowledge of the speaker and hearer(s), etc etc.\\ncorpus A body of text used in experiments (plural corpora). See x3.1.\\ncue phrases Phrases which indicates particular rhetorical relations.\\ndenominal Something derived from a noun: e.g., the verb tango is a denominal verb.\\nderivational morphology See x2.2\\ndeterminer See IGE or notes for prelecture exercises in lecture 3.\\ndeverbal Something derived from a verb: e.g., the adjective surprised.\\ndirect object See IGE. Contrast indirect object.\\ndiscourse In NLP, a piece of connected text.\\ndiscourse relations See rhetorical relations.\\ndomain Not a precise term, but I use it to mean some restricted set of knowledge appropriate for an application.\\nerror analysis In evaluation, working out what sort of errors are found for a given approach (x3.6).'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 68}, page_content='error analysis In evaluation, working out what sort of errors are found for a given approach (x3.6).\\nfeature structure See Lecture 5.\\nfull-form lexicon A lexicon where all morphological variants are explicitly listed (lecture 2).\\ngeneration The process of constructing strings from some input representation. With bidirectional grammars using\\ncompositional semantics, generation can be split into strategic generation, which is the process of deciding on\\nthe logical form (also known as text planning), and tactical generation which is the process of going from the\\nlogical form to the string (also known as realization). x6.2.\\ngenerative grammar The family of approaches to linguistics where a natural language is treated as governed by rules\\nwhich can produce all and only the well-formed utterances. Lecture 4.\\ngrammar Formally, in the generative tradition, the set of rules and the lexicon. Lecture 4.\\nhead In syntax, the most important element of a phrase.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 68}, page_content='grammar Formally, in the generative tradition, the set of rules and the lexicon. Lecture 4.\\nhead In syntax, the most important element of a phrase.\\nhearer Anyone on the receiving end of an utterance (spoken, written or signed). x1.3.\\nhomonymy Instances of polysemy where the two senses are unrelated (x6.8).\\n69'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 69}, page_content=\"hyponymy An `IS-A' relationship (x6.4) More general terms are hypernyms, more speci\\x02c hyponyms.\\nindirect object The bene\\x02ciary in verb phrases like give a present to Sandy or give Sandy a present. In this case the\\nindirect object is Sandy and the direct object is a present.\\ninterannotator agreement The degree of agreement between the decisions of two or more humans with respect to\\nsome categorisation (x3.6).\\nlanguage model A term generally used in speech recognition, for a statistical model of a natural language (lecture 3).\\nlemmatization Finding the stem and af\\x02xes for words (lecture 2).\\nlexical ambiguity Ambiguity caused because of multiple senses for a word.\\nlexicon The part of an NLP system that contains information about individual words (lecture 1).\\nlinking Relating syntax and semantics in lexical entries (x6.1).\\nlocal ambiguity Ambiguity that arises during analysis etc, but which will be resolved when the utterance is com-\\npletely processed.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 69}, page_content=\"linking Relating syntax and semantics in lexical entries (x6.1).\\nlocal ambiguity Ambiguity that arises during analysis etc, but which will be resolved when the utterance is com-\\npletely processed.\\nlogical form The semantic representation constructed for an utterance (x6.1).\\nmeaning postulates Inference rules that capture some aspects of the meaning of a word.\\nmeronymy The `part-of' lexical semantic relation (x6.5).\\nmorpheme Minimal information carrying units within a word (x2.1).\\nmorphology See x1.2\\nMT Machine translation\\nmultiword expression A conventional phrase that has something idiosyncratic about it and therefore might be listed\\nin a dictionary.\\nmumble input Any unrecognised input in a spoken dialogue system (lecture 2).\\nn-gram A sequence of nwords (x3.2).\\nnamed entity recognition Recognition and categorisation of person names, names of places, dates etc (lecture 4).\\nnoun See IGE or notes for prelecture exercises in lecture 3.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 69}, page_content='named entity recognition Recognition and categorisation of person names, names of places, dates etc (lecture 4).\\nnoun See IGE or notes for prelecture exercises in lecture 3.\\nnoun phrase (NP) A phrase which has a noun as syntactic head. See IGE.\\nontology In NLP and AI, a speci\\x02cation of the entities in a particular domain and (sometimes) the relationships\\nbetween them. Often hierarchically structured.\\nopen class Opposite of closed class.\\northographic rules spelling rules (x2.3)\\novergenerate Of a grammar, to produce strings which are invalid, e.g., because they are not grammatical according\\nto human judgements.\\npacking See x4.9\\npassive chart parsing See x4.7\\nparse tree See x4.4\\npart of speech The main syntactic categories: noun, verb, adjective, adverb, preposition, conjunction etc.\\n70'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 70}, page_content=\"part of speech tagging Automatic assignment of syntactic categories to the words in a text. The set of categories\\nused is actually generally more \\x02ne-grained than traditional parts of speech.\\npolysemy The phenomenon of words having different senses (x6.8).\\npragmatics See x1.2\\npredicate In logic, something that takes zero or more arguments and returns a truth value. (Used in IGE for the verb\\nphrase following the subject in a sentence, but I don't use that terminology.)\\npre\\x02x An af\\x02x that precedes the stem.\\nprobabilistic context free grammars (PCFGs) CFGs with probabilities associated with rules (lecture 4).\\nrealization Another term for tactical generation \\x97 see generation.\\nreferring expression See x7.5\\nrelative clause See IGE.\\nA restrictive relative clause is one which limits the interpretation of a noun to a subset: e.g. the students who\\nsleep in lectures are obviously overworking refers to a subset of students. Contrast non-restrictive, which is a\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 70}, page_content=\"sleep in lectures are obviously overworking refers to a subset of students. Contrast non-restrictive, which is a\\nform of parenthetical comment: e.g. the students, who sleep in lectures, are obviously overworking means all\\n(or nearly all) are sleeping.\\nselectional restrictions Constraints on the semantic classes of arguments to verbs etc (e.g., the subject of think is\\nrestricted to being sentient). The term selectional preference is used for non-absolute restrictions.\\nsemantics See x1.2\\nsign As used in lecture 5, the bundle of properties representing a word or phrase.\\nsmoothing Redistributing observed probabilities to allow for sparse data, especially to give a non-zero probability\\nto unseen events (lecture 2).\\nsparse data Especially in statistical techniques, data concerning rare events which isn't adequate to give good proba-\\nbility estimates (lecture 2).\\nspeaker Someone who makes an utterance (x1.3).\\nspelling rules x2.3\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 70}, page_content='bility estimates (lecture 2).\\nspeaker Someone who makes an utterance (x1.3).\\nspelling rules x2.3\\nstem A morpheme which is a central component of a word (contrast af\\x02x). x2.1.\\nstemming Stripping af\\x02xes (see x2.4).\\nstrong equivalence Of grammars, accepting/rejecting exactly the same strings and assigning the same bracketings\\n(contrast weak equivalence). Lecture 4.\\nstructural ambiguity The situation where the same string corresponds to multiple bracketings.\\nsubcategorization The lexical property that tells us how many arguments a verb etc can have.\\nsuf\\x02x An af\\x02x that follows the stem.\\nsummarization Producing a shorter piece of text (or speech) that captures the essential information in the original.\\nsynonymy Having the same meaning (x6.5).\\nsyntax See x1.2\\ntaxonomy Traditionally, the scheme of classi\\x02cation of biological organisms. Extended in NLP to mean a hierarchical\\nclassi\\x02cation of word senses. The term ontology is sometimes used in a rather similar way, but ontologies tend'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 70}, page_content='classi\\x02cation of word senses. The term ontology is sometimes used in a rather similar way, but ontologies tend\\nto be classi\\x02cations of domain-knowledge, without necessarily having a direct link to words, and may have a\\nricher structure than a taxonomy.\\n71'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 71}, page_content=\"template In feature structure grammars, see 5.6\\ntense Past, present, future etc.\\ntext planning Another term for strategic generation: see generation.\\ntraining data Data used to train any sort of machine-learning system. Must be separated from test data which is kept\\nunseen. Manually-constructed systems should ideally also use strictly unseen data for evaluation.\\ntransfer In MT, the process of going from a representation appropriate to the original (source) language to one\\nappropriate for the target language.\\ntreebank a corpus annotated with trees (lecture 4).\\nuni\\x02cation See Lecture 5, especially x5.3.\\nweak equivalence Of grammars, accepting/rejecting exactly the same strings (contrast strong equivalence). Lecture\\n4.\\nWizard of Oz experiment An experiment where data is collected, generally for a dialogue system, by asking users\\nto interact with a mock-up of a real system, where some or all of the `processing' is actually being done by a\\nhuman rather than automatically.\\nWordNet See x6.6\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 71}, page_content=\"to interact with a mock-up of a real system, where some or all of the `processing' is actually being done by a\\nhuman rather than automatically.\\nWordNet See x6.6\\nword-sense disambiguation See x6.9\\nutterance A piece of speech or text (sentence or fragment) generated by a speaker in a particular context.\\nverb See IGE or notes for prelecture exercises in lecture 3.\\nverb phrase (VP) A phrase headed by a verb.\\n72\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 72}, page_content=\"Exercises for NLP course, 2004\\nNotes on exercises\\nThese exercises are organised by lecture. They are divided into two classes: prelecture and postlecture. The prelecture\\nexercises are intended to review the basic concepts that you'll need to fully understand the lecture. Depending on your\\nbackground, you may \\x02nd these trivial or you may need to read the notes, but in either case they shouldn't take more\\nthan a few minutes. The \\x02rst one or two examples generally come with answers, other answers are at the end (where\\nappropriate).\\nAnswersto the postlecture exercises are with the supervision notes (where appropriate). These are mostly intended as\\nquick exercises to check understanding of the lecture, though some are more open-ended.\\nA Lecture 1\\nA.1 Postlecture exercises\\nIf you use a word processor with a spelling and grammar checker, try looking at its treatment of agreement and some of\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 72}, page_content='A Lecture 1\\nA.1 Postlecture exercises\\nIf you use a word processor with a spelling and grammar checker, try looking at its treatment of agreement and some of\\nthe other phenomena discussed in the lecture. If possible, try switching settings between British English and American\\nEnglish.\\nBLecture 2\\nB.1 Prelecture exercises\\n1. Split the following words into morphological units, labelling each as stem, suf\\x02x or pre\\x02x. If there is any\\nambiguity, give all possible splits.\\n(a) dries\\nanswer: dry (stem), -s (suf\\x02x)\\n(b) cartwheel\\nanswer: cart (stem), wheel (stem)\\n(c) carries\\n(d) running\\n(e) uncaring\\n(f) intruders\\n(g) bookshelves\\n(h) reattaches\\n(i) anticipated\\n2. List the simple past and past/passive participle forms of the following verbs:\\n(a) sing\\nAnswer: simple past sang, participle sung\\n(b) carry\\n(c) sleep\\n(d) see\\nNote that the simple past is used by itself (e.g., Kim sang well) while the participle form is used with an auxiliary (e.g.,'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 72}, page_content='Answer: simple past sang, participle sung\\n(b) carry\\n(c) sleep\\n(d) see\\nNote that the simple past is used by itself (e.g., Kim sang well) while the participle form is used with an auxiliary (e.g.,\\nKim had sung well). The passive participle is always the same as the past participle in English: (e.g., Kim began the\\nlecture early, Kim had begun the lecture early, The lecture was begun early).\\n73'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 73}, page_content=\"B.2 Post-lecture exercises\\n1. For each of the following surface forms, give a list of the states that the FST given in the lecture notes for\\ne-insertion passes through, and the corresponding underlying forms:\\n(a) c a t s\\n(b) c o r p u s\\n(c) a s s e s\\n(d) a s s e s s\\n(e) a x e s\\n2. Modify the FSA for dates so that it only accepts valid months. Turn your revised FSA into a FST which maps\\nbetween the numerical representation of months and their abbreviations (Jan . . . Dec).\\nC Lecture 3\\nC.1 Pre-lecture\\nLabel each of the words in the following sentences with their part of speech, distinguishing between nouns, proper\\nnouns, verbs, adjectives, adverbs, determiners, prepositions, pronouns and others. (Traditional classi\\x02cations often\\ndistinguish between a large number of additional parts of speech, but the \\x02ner distinctions won't be important here.)\\nThere are notes on part of speech distinctions below, if you have problems.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 73}, page_content=\"distinguish between a large number of additional parts of speech, but the \\x02ner distinctions won't be important here.)\\nThere are notes on part of speech distinctions below, if you have problems.\\n1. The brown fox could jump quickly over the dog, Rover. Answer: The/Det brown/Adj fox/Noun could/Verb(modal)\\njump/Verb quickly/Adverb over/Preposition the/Determiner dog/Noun, Rover/Proper noun.\\n2. The big cat chased the small dog into the barn.\\n3. Those barns have red roofs.\\n4. Dogs often bark loudly.\\n5. Further discussion seems useless.\\n6. Kim did not like him.\\n7. Time \\x03ies.\\nNotes on parts of speech. These notes are English-speci\\x02c and are just intended to help with the lectures and the exer-\\ncises: see a linguistics textbook for de\\x02nitions! Some categories have fuzzy boundaries, but none of the complicated\\ncases will be important for this course.\\nNoun prototypically, nouns refer to physical objects or substances: e.g., aardvark, chainsaw, rice. But they can also\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 73}, page_content=\"cases will be important for this course.\\nNoun prototypically, nouns refer to physical objects or substances: e.g., aardvark, chainsaw, rice. But they can also\\nbe abstract (e.g. truth, beauty) or refer to events, states or processes (e.g., decision). If you can say the X and\\nhave a sensible phrase, that's a good indication that X is a noun.\\nPronoun something that can stand in for a noun: e.g., him, his\\nProper noun / Proper name a name of a person, place etc: e.g., Elizabeth, Paris\\nVerb Verbs refer to events, processes or states but since nouns and adjectives can do this as well, the distinction\\nbetween the categories is based on distribution, not semantics. For instance, nouns can occur with determiners\\nlike the (e.g., the decision) whereas verbs can't (e.g., * the decide). In English, verbs are often found with\\nauxiliaries (be, have or do) indicating tense and aspect, and sometime occur with modals, like can, could etc.\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 73}, page_content='auxiliaries (be, have or do) indicating tense and aspect, and sometime occur with modals, like can, could etc.\\nAuxiliaries and modals are themselves generally treated as subclasses of verbs.\\n74'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 74}, page_content=\"Adjective a word that modi\\x02es a noun: e.g., big, loud. Most adjectives can also occur after the verb be and a few\\nother verbs: e.g., the students are unhappy. Numbers are sometimes treated as a type of adjective by linguists\\nbut generally given their own category in traditional grammars. Past participle forms of verbs can also often be\\nused as adjectives (e.g., worried in the very worried man). Sometimes it's impossible to tell whether something\\nis a participle or an adjective (e.g., the man was worried).\\nAdverb a word that modi\\x02es a verb: e.g. quickly, probably.\\nDeterminer these precede nouns e.g., the, every, this. It is not always clear whether a word is a determiner or some\\ntype of adjective.\\nPreposition e.g., in, at, with\\nNouns, proper nouns, verbs, adjectives and adverbs are the open classes: new words can occur in any of these cate-\\ngories. Determiners, prepositions and pronouns are closed classes (as are auxiliary and modal verbs).\\nC.2 Post-lecture\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 74}, page_content='gories. Determiners, prepositions and pronouns are closed classes (as are auxiliary and modal verbs).\\nC.2 Post-lecture\\nTry out one or more of the following POS tagging sites:\\nhttp://www.coli.uni-sb.de/\\x98thorsten/tnt/\\nhttp://www.comp.lancs.ac.uk/computing/research/ucrel/claws/trial.html\\nhttp://l2r.cs.uiuc.edu/\\x98cogcomp/eoh/posdemo.html\\nOnlythe \\x02rst site uses an approach comparable to that described in the lecture. Find two short pieces of naturally\\noccurring English text, one of which you think should be relatively easy to tag correctly and one which you predict to\\nbe dif\\x02cult. Look at the tagged output and estimate the percentage of correct tags in each case, concentrating on the\\nopen-class words. You might like to get another student to look at the same output and see if you agree on which tags\\nare correct.\\nD Lecture 4\\nD.1 Pre-lecture\\nPut brackets round the noun phrases and the verb phrases in the following sentences (if there is ambiguity, give two\\nbracketings):'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 74}, page_content='are correct.\\nD Lecture 4\\nD.1 Pre-lecture\\nPut brackets round the noun phrases and the verb phrases in the following sentences (if there is ambiguity, give two\\nbracketings):\\n1. The cat with white fur chased the small dog into the barn.\\nAnswer: ((The cat)np with (white fur)np)np chased (the small dog)np into (the barn)np\\nThe cat with white fur (chased the small dog into the barn)vp\\n2. The big cat with black fur chased the dog which barked.\\n3. Three dogs barked at him.\\n4. Kim saw the birdwatcher with the binoculars.\\nNote that noun phrases consist of the noun, the determiner (if present) and any modi\\x02ers of the noun (adjective,\\nprepositional phrase, relative clause). This means that noun phrases may be nested. Verb phrases include the verb and\\nany auxiliaries, plus the object and indirect object etc (in general, the complements of the verb \\x97 discussed in lecture\\n5) and any adverbial modi\\x02ers. The verb phrase does not include the subject.\\n75'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 75}, page_content='D.2 Post-lecture\\nUsing the CFG given in the lecture notes (section 4.3):\\n1. show the edges generated when parsing they \\x02sh in rivers in December with the simple chart parser in 4.7\\n2. show the edges generated for this sentence if packing is used (as described in 4.9)\\n3. show the edges generated for they \\x02sh in rivers if an active chart parser is used (as in 4.10)\\nE Lecture 5\\nE.1 Pre-lecture\\n1. A very simple form of semantic representation corresponds to making verbs one-, two- or three- place logical\\npredicates. Proper names are assumed to correspond to constants. The \\x02rst argument should always correspond\\nto the subject of the active sentence, the second to the object (if there is one) and the third to the indirect object\\n(i.e., the bene\\x02ciary, if there is one). Give representations for the following examples:\\n(a) Kim likes Sandy\\nAnswer: like(Kim, Sandy)\\n(b) Kim sleeps\\n(c) Sandy adores Kim\\n(d) Kim is adored by Sandy (note, this is passive: the by should not be represented)'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 75}, page_content='(a) Kim likes Sandy\\nAnswer: like(Kim, Sandy)\\n(b) Kim sleeps\\n(c) Sandy adores Kim\\n(d) Kim is adored by Sandy (note, this is passive: the by should not be represented)\\n(e) Kim gave Rover to Sandy (the to is not represented)\\n(f) Kim gave Sandy Rover\\n2. List three verbs that are intransitive only, three which are simple transitive only, three which can be intransitive\\nor transitive and three which are ditransitives.\\nThe distinction between intransitive, transitive and ditransitive verbs can be illustrated by examples such as:\\nsleep \\x97 intransitive. No object is (generally) possible: * Kim slept the evening.\\nadore \\x97 transitive. An object is obligatory: *Kim adored.\\ngive \\x97- ditransitive. These verbs have an object and an indirect object. Kim gave Sandy an apple (or Kim gave\\nan apple to Sandy).\\nE.2 Post-lecture\\n1. Give the uni\\x02cation of the following feature structures:\\n(a)\\n\\x14\\nCAT\\n\\x02 \\x03\\nAGR pl\\n\\x15\\nuni\\x02ed with\\n\\x14\\nCAT VP\\nAGR\\n\\x02 \\x03\\n\\x15\\n(b)\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT VP\\nAGR\\n1\\n\\x15\\nDTR1\\n\\x14\\nCAT V\\nAGR 1\\n\\x15'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 75}, page_content='E.2 Post-lecture\\n1. Give the uni\\x02cation of the following feature structures:\\n(a)\\n\\x14\\nCAT\\n\\x02 \\x03\\nAGR pl\\n\\x15\\nuni\\x02ed with\\n\\x14\\nCAT VP\\nAGR\\n\\x02 \\x03\\n\\x15\\n(b)\\n2\\n6\\n6\\n6\\n6\\n6\\n6\\n6\\n4\\nMOTHER\\n\\x14\\nCAT VP\\nAGR\\n1\\n\\x15\\nDTR1\\n\\x14\\nCAT V\\nAGR 1\\n\\x15\\nDTR2\\n\\x14\\nCAT NP\\nAGR\\n\\x02 \\x03\\n\\x15\\n3\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\nuni\\x02ed with\\n\"\\nDTR1\\n\\x14\\nCAT V\\nAGR sg\\n\\x15#\\n(c)\\n\\x14\\nF\\n1\\nG 1\\n\\x15\\nuni\\x02ed with\\n2\\n6\\n4\\nF\\n\\x02\\nJ a\\n\\x03\\nG\\n\\x14\\nJ\\n\\x02 \\x03\\nK b\\n\\x15\\n3\\n7\\n5\\n76'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 76}, page_content=\"(d)\\n\\x14\\nF 1 a\\nG 1\\n\\x15\\nuni\\x02ed with \\x02\\nG b\\n\\x03\\n(e)\\n\\x14\\nF 1\\nG 1\\n\\x15\\nuni\\x02ed with\\n2\\n6\\n4\\nF\\n\\x02\\nJ a\\n\\x03\\nG\\n\\x14\\nJ b\\nK b\\n\\x15\\n3\\n7\\n5\\n(f)\\n\\x14\\nF\\n\\x02\\nG\\n1\\n\\x03\\nH 1\\n\\x15\\nuni\\x02ed with\\n\\x14\\nF 1\\nH 1\\n\\x15\\n(g)\\n2\\n4\\nF\\n1\\nG 1\\nH 2\\nJ 2\\n3\\n5uni\\x02ed with\\n\\x14\\nF\\n1\\nJ 1\\n\\x15\\n(h)\\n\\x14\\nF\\n\\x02\\nG 1\\n\\x03\\nH 1\\n\\x15\\nuni\\x02ed with\\n\\x14\\nF 2\\nH\\n\\x02\\nJ 2\\n\\x03\\n\\x15\\n2. Add case to the initial FS grammar in order to prevent sentences such as they can they from parsing.\\n3. Work though parses of the following strings for the second FS grammar, deciding whether they parse or not:\\n(a) \\x02sh \\x02sh\\n(b) they can \\x02sh\\n(c) it \\x02sh\\n(d) they can\\n(e) they \\x02sh it\\n4. Modify the second FS grammar to allow for verbs which take two complements. Also add a lexical entry for\\ngive (just do the variant which takes two noun phrases).\\nF Lecture 6\\nF.1 Pre-lecture\\nWithout looking at a dictionary, write down brief de\\x02nitions for as many senses as you can think of for the following\\nwords:\\n1. plant\\n2. shower\\n3. bass\\nIf possible, compare your answers with another student's and with a dictionary.\\nF.2 Post-lecture\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 76}, page_content=\"words:\\n1. plant\\n2. shower\\n3. bass\\nIf possible, compare your answers with another student's and with a dictionary.\\nF.2 Post-lecture\\n1. If you did the exercise associated with the previous lecture to add ditransitive verbs to the grammar, amend your\\nmodi\\x02ed grammar so that it produces semantic representations.\\n2. Give hypernyms and (if possible) hyponyms for the nominal senses of the following words:\\n(a) horse\\n(b) rice\\n(c) curtain\\n3. List some possible seeds for Yarowsky's algorithm that would distinguish between the senses of shower and\\nbass that you gave in the prelecture exercise.\\n77\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 77}, page_content='G Lecture 7\\nG.1 Pre-lecture\\nNo suggested exercises.\\nG.2 Post-lecture\\nWork through the Lappin and Leass algorithm with a short piece of naturally occurring text. If there are cases where\\nthe algorithm gets the results wrong, suggest the sorts of knowledge that would be needed to give the correct answer.\\nH Lecture 8\\nH.1 Exercises (pre- or post- lecture)\\nIf you have never used a spoken dialogue system, try one out. One example is British Airways \\x03ight arrivals (0870 551\\n1155 \\x97 charged at standard national rate). Think of a realistic task before you phone: for instance, to \\x02nd arrival times\\nfor a morning \\x03ight from Edinburgh to Heathrow. Another example is the Transco Meter Helpline (0870 608 1524 \\x97\\nstandard national rate) which tells customers who their gas supplier is. This system uses a very simple dialogue model\\nbut allows for interruptions etc.\\nUse Systran (via http://world.altavista.com/) to translate some text and investigate whether the text it'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 77}, page_content=\"but allows for interruptions etc.\\nUse Systran (via http://world.altavista.com/) to translate some text and investigate whether the text it\\noutputs is grammatical and whether it deals well with issues discussed in the course, such as lexical ambiguity and\\npronoun resolution. Ideally you would get the help of someone who speaks a language other than English for this if\\nyou're not fairly \\x03uent in another language yourself: the language pairs that Systran deals with are listed on the site.\\n78\"),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 78}, page_content='I Answers to some of the pre-lecture exercises\\nI.1 Lecture 2\\n1. (a) carries\\ncarry (stem) s (suf\\x02x)\\n(b) running\\nrun (stem) ing (suf\\x02x)\\n(c) uncaring\\nun (pre\\x02x) care (stem) ing (suf\\x02x)\\n(d) intruders\\nintrude (stem) er (suf\\x02x) s (suf\\x02x)\\nNote that in- is not a real pre\\x02x here\\n(e) bookshelves\\nbook (stem) shelf (stem) s (suf\\x02x)\\n(f) reattaches\\nre (pre\\x02x) attach (stem) s (suf\\x02x)\\n(g) anticipated\\nanticipate (stem) ed (suf\\x02x)\\n2. (a) carry\\nAnswer: simple past carried, past participle carried\\n(b) sleep\\nAnswer: simple past slept, past participle slept\\n(c) see\\nAnswer: simple past saw, past participle seen\\nI.2 Lecture 3\\n1. The/Det big/Adj cat/Noun chased/Verb the/Det small/Adj dog/Noun into/Prep the/Det barn/Noun.\\n2. Those/Det barns/Noun have/Verb red/Adj roofs/Noun.\\n3. Dogs/Noun often/Adverb bark/Verb loudly/Adverb.\\n4. Further/Adj discussion/Noun seems/Verb useless/Adj.\\n5. Kim/Proper noun did/Verb(aux) not/Adverb(or Other) like/Verb him/Pronoun.\\n6. Time/Noun \\x03ies/Verb.'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 78}, page_content='4. Further/Adj discussion/Noun seems/Verb useless/Adj.\\n5. Kim/Proper noun did/Verb(aux) not/Adverb(or Other) like/Verb him/Pronoun.\\n6. Time/Noun \\x03ies/Verb.\\nTime/Verb \\x03ies/Noun. (the imperative!)\\nI.3 Lecture 4\\n1. The big cat with black fur chased the dog which barked.\\n((The big cat)np with (black fur)np)np chased (the dog which barked)np\\nThe big cat with black fur (chased the dog which barked)vp\\n2. Three dogs barked at him. (Three dogs)np barked at (him)np Three dogs (barked at him)vp\\n3. Kim saw the birdwatcher with the binoculars.\\nAnalysis 1 (the birdwatcher has the binoculars) (Kim)np saw ((the birdwatcher)np with (the binoculars)np)np\\nKim (saw the birdwatcher with the binoculars)vp\\nAnalysis 2 (the seeing was with the binoculars) (Kim)np saw (the birdwatcher)np with (the binoculars)np\\nKim (saw the birdwatcher with the binoculars)vp\\n79'),\n",
       " Document(metadata={'source': 'revised.pdf', 'page': 79}, page_content='I.4 Lecture 5\\n1. Kim sleeps\\nsleep(Kim)\\n2. Sandy adores Kim\\nadore(Sandy, Kim)\\n3. Kim is adored by Sandy\\nadore(Sandy, Kim)\\n4. Kim gave Rover to Sandy\\ngive(Kim, Rover, Sandy)\\n5. Kim gave Sandy Rover\\ngive(Kim, Rover, Sandy)\\nSome examples of different classes of verb (obviously you have almost certainly come up with different ones!)\\nsleep, snore, sneeze, cough \\x97 intransitive only\\nadore, comb, rub \\x97 simple transitive only eat, wash, shave, dust \\x97 transitive or intransitive\\ngive, hand, lend \\x97 ditransitive\\n80')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a FAISS vector database\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x19777b980e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "prompt = PromptTemplate(template=\"Answer the question based on the following context: {context}\\n\\nQuestion: {question}\\nAnswer:\")\n",
    "\n",
    "#model\n",
    "model = ChatGroq(model=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"summarize this document\"\n",
    "response = qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document discusses the relationship between discourse structure and summarization in Natural Language Processing (NLP). It explains that discourse relations can be represented as a binary branching tree structure, where some relationships, such as Explanation, have a main phrase (nucleus) and a subsidiary phrase (satellite) that can be removed without losing coherence. This can be exploited in summarization. However, other relationships, like Narration, give equal weight to both elements, making summarization more challenging. The document also provides an overview of the NLP field, its subareas, and methodologies, and outlines the structure of a course that covers these topics in more detail. \n",
      "\n",
      "Key points:\n",
      "\n",
      "* Discourse structure can be represented as a binary branching tree\n",
      "* Some relationships (e.g. Explanation) have a main and subsidiary phrase that can be used for summarization\n",
      "* Other relationships (e.g. Narration) give equal weight to both elements, making summarization harder\n",
      "* NLP is a multidisciplinary field with various subareas and methodologies\n",
      "* The course provides an overview of NLP and its applications.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
